# -*- coding: utf-8 -*-
"""without attention

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Ig0P9bo21VzxU4BmzIGJd9KrgxhlOHhh
"""

import torch
import torch.nn as nn
import os
import wandb
from torch.utils.data import Dataset, DataLoader
from collections import defaultdict


try:
    wandb.login(key="999fe4f321204bd8f10135f3e40de296c23050f9")
except:
    print("WandB login failed - results will not be logged. Set WANDB_API_KEY in your environment.")

class Encoder(nn.Module):
    def __init__(self, input_dim, emb_dim, hidden_dim, num_layers, rnn_type, dropout=0.5, bidirectional=False):
        super(Encoder, self).__init__()
        self.embedding = nn.Embedding(input_dim, emb_dim)
        self.rnn_type = rnn_type
        self.num_layers = num_layers
        self.hidden_dim = hidden_dim
        self.bidirectional = bidirectional
        self.dropout = nn.Dropout(dropout)

        self.directions = 2 if bidirectional else 1

        if rnn_type == 'lstm':
            self.rnn = nn.LSTM(emb_dim, hidden_dim, num_layers, batch_first=True,
                              dropout=dropout if num_layers > 1 else 0,
                              bidirectional=bidirectional)
        elif rnn_type == 'gru':
            self.rnn = nn.GRU(emb_dim, hidden_dim, num_layers, batch_first=True,
                             dropout=dropout if num_layers > 1 else 0,
                             bidirectional=bidirectional)
        else:  # rnn
            self.rnn = nn.RNN(emb_dim, hidden_dim, num_layers, batch_first=True,
                             dropout=dropout if num_layers > 1 else 0,
                             bidirectional=bidirectional)


        if bidirectional:
            self.projection = nn.Linear(hidden_dim * 2, hidden_dim)

    def forward(self, src):
        embedded = self.dropout(self.embedding(src))

        if self.rnn_type == 'lstm':
            outputs, (hidden, cell) = self.rnn(embedded)

            # Process bidirectional states if needed
            if self.bidirectional:
                # Process hidden states
                hidden = hidden.view(self.num_layers, self.directions, -1, self.hidden_dim)
                # Concat forward and backward directions
                hidden = torch.cat([hidden[:, 0], hidden[:, 1]], dim=2)
                # Project to the correct dimension
                hidden = self.projection(hidden)

                # Process cell states
                cell = cell.view(self.num_layers, self.directions, -1, self.hidden_dim)
                cell = torch.cat([cell[:, 0], cell[:, 1]], dim=2)
                cell = self.projection(cell)

                # If outputs is bidirectional, we need to process it too
                batch_size = outputs.size(0)
                seq_len = outputs.size(1)
                # Reshape and project encoder outputs
                outputs = outputs.contiguous().view(batch_size, seq_len, self.hidden_dim * 2)
                outputs = self.projection(outputs)

                return outputs, (hidden, cell)
            return outputs, (hidden, cell)
        else:
            outputs, hidden = self.rnn(embedded)

            # Process bidirectional states if needed
            if self.bidirectional:
                hidden = hidden.view(self.num_layers, self.directions, -1, self.hidden_dim)
                hidden = torch.cat([hidden[:, 0], hidden[:, 1]], dim=2)
                hidden = self.projection(hidden)

                # Process outputs if bidirectional
                batch_size = outputs.size(0)
                seq_len = outputs.size(1)
                # Reshape and project encoder outputs
                outputs = outputs.contiguous().view(batch_size, seq_len, self.hidden_dim * 2)
                outputs = self.projection(outputs)

            return outputs, hidden



class Decoder(nn.Module):
    def __init__(self, output_dim, emb_dim, hidden_dim, num_layers, rnn_type, dropout=0.5, attention=False):
        super(Decoder, self).__init__()
        self.embedding = nn.Embedding(output_dim, emb_dim)
        self.rnn_type = rnn_type
        self.num_layers = num_layers
        self.hidden_dim = hidden_dim
        self.output_dim = output_dim
        self.dropout = nn.Dropout(dropout)

        rnn_input_size = emb_dim

        if rnn_type == 'lstm':
            self.rnn = nn.LSTM(rnn_input_size, hidden_dim, num_layers, batch_first=True,
                              dropout=dropout if num_layers > 1 else 0)
        elif rnn_type == 'gru':
            self.rnn = nn.GRU(rnn_input_size, hidden_dim, num_layers, batch_first=True,
                             dropout=dropout if num_layers > 1 else 0)
        else:  # rnn
            self.rnn = nn.RNN(rnn_input_size, hidden_dim, num_layers, batch_first=True,
                             dropout=dropout if num_layers > 1 else 0)

        self.fc_out = nn.Linear(hidden_dim, output_dim)

    def forward(self, input_char, hidden, encoder_outputs=None):
        # Reshape input to [batch_size, 1]
        input_char = input_char.unsqueeze(1)

        # Embed input character
        embedded = self.dropout(self.embedding(input_char))  # [batch_size, 1, emb_dim]

        # Simple forward pass through RNN - no attention
        if self.rnn_type == 'lstm':
            output, (hidden, cell) = self.rnn(embedded, hidden)
            hidden_state = (hidden, cell)
        else:
            output, hidden = self.rnn(embedded, hidden)
            hidden_state = hidden

        # Generate prediction from the output
        prediction = self.fc_out(output.squeeze(1))  # [batch_size, output_dim]

        return prediction, hidden_state
class Seq2Seq(nn.Module):
    def __init__(self, encoder, decoder, rnn_type, device, use_attention=False):
        super().__init__()
        self.encoder = encoder
        self.decoder = decoder
        self.rnn_type = rnn_type
        self.device = device
        # self.use_attention = use_attention

    def forward(self, src, trg, teacher_forcing_ratio=0.0):
        batch_size = src.size(0)
        trg_len = trg.size(1)
        trg_vocab_size = self.decoder.output_dim

        outputs = torch.zeros(batch_size, trg_len, trg_vocab_size).to(self.device)


        encoder_outputs, hidden = self._encode(src)

        # Use the first token as input to start decoding
        input_char = trg[:, 0]  # <sos> token

        for t in range(1, trg_len):
            output, hidden = self.decoder(input_char, hidden)

            outputs[:, t] = output

            # Teacher forcing: use real target or predicted token
            teacher_force = torch.rand(1).item() < teacher_forcing_ratio
            top1 = output.argmax(1)
            input_char = trg[:, t] if teacher_force else top1

        return outputs

    def _encode(self, src):
        # Get encoder outputs and final hidden state
        encoder_outputs, hidden = self.encoder(src)

        # Adjust hidden state dimensions if encoder and decoder have different layers
        encoder_layers = self.encoder.num_layers
        decoder_layers = self.decoder.num_layers

        if self.rnn_type == 'lstm':
            hidden_state, cell_state = hidden

            # If encoder has fewer layers than decoder, pad with zeros
            if encoder_layers < decoder_layers:
                padding = torch.zeros(
                    decoder_layers - encoder_layers,
                    hidden_state.size(1),
                    hidden_state.size(2)
                ).to(self.device)
                hidden_state = torch.cat([hidden_state, padding], dim=0)
                cell_state = torch.cat([cell_state, padding], dim=0)

            # If encoder has more layers than decoder, truncate
            elif encoder_layers > decoder_layers:
                hidden_state = hidden_state[:decoder_layers]
                cell_state = cell_state[:decoder_layers]

            # Make sure hidden dimensions match decoder's expected dimensions
            if hidden_state.size(2) != self.decoder.hidden_dim:
                # Project hidden state to the decoder's dimension using a linear projection
                batch_size = hidden_state.size(1)
                proj_hidden = torch.zeros(
                    hidden_state.size(0),
                    batch_size,
                    self.decoder.hidden_dim
                ).to(self.device)

                for layer in range(hidden_state.size(0)):
                    # Simple linear projection for each layer
                    proj_hidden[layer] = hidden_state[layer].clone().view(batch_size, -1)[:, :self.decoder.hidden_dim]

                # Apply the same projection to cell state
                proj_cell = torch.zeros_like(proj_hidden)
                for layer in range(cell_state.size(0)):
                    proj_cell[layer] = cell_state[layer].clone().view(batch_size, -1)[:, :self.decoder.hidden_dim]

                hidden = (proj_hidden, proj_cell)
            else:
                hidden = (hidden_state, cell_state)
        else:
            # For GRU and RNN
            # If encoder has fewer layers than decoder, pad with zeros
            if encoder_layers < decoder_layers:
                padding = torch.zeros(
                    decoder_layers - encoder_layers,
                    hidden.size(1),
                    hidden.size(2)
                ).to(self.device)
                hidden = torch.cat([hidden, padding], dim=0)
            # If encoder has more layers than decoder, truncate
            elif encoder_layers > decoder_layers:
                hidden = hidden[:decoder_layers]

            # Making sure hidden dimensions match decoder's expected dimensions
            if hidden.size(2) != self.decoder.hidden_dim:
                # Project hidden state to the decoder's dimension
                batch_size = hidden.size(1)
                proj_hidden = torch.zeros(
                    hidden.size(0),
                    batch_size,
                    self.decoder.hidden_dim
                ).to(self.device)

                for layer in range(hidden.size(0)):
                    # Simple linear projection for each layer
                    proj_hidden[layer] = hidden[layer].clone().view(batch_size, -1)[:, :self.decoder.hidden_dim]

                hidden = proj_hidden

        return encoder_outputs, hidden

# Character-level vocabulary builder
def build_vocab(tokens):
    vocab = {'<pad>': 0, '<sos>': 1, '<eos>': 2, '<unk>': 3}
    for token in tokens:
        for char in token:
            if char not in vocab:
                vocab[char] = len(vocab)
    return vocab

def encode_sequence(seq, vocab):
    return [vocab.get(char, vocab['<unk>']) for char in seq]

class DakshinaDataset(Dataset):
    def __init__(self, data_path, latin_vocab=None, Tamil_vocab=None):
        self.latin_words = []
        self.Tamil_words = []

        # Group all transliterations by Tamil word
        candidates = defaultdict(list)

        with open(data_path, encoding='utf-8') as f:
            for line in f:
                parts = line.strip().split('\t')
                if len(parts) != 3:
                    continue
                native, latin, rel = parts[0], parts[1], int(parts[2])
                candidates[native].append((latin, rel))

        # Keep only the transliteration with highest score for each native word
        for native, translits in candidates.items():
            max_rel = max(rel for _, rel in translits)
            for latin, rel in translits:
                if rel == max_rel:
                    self.latin_words.append(latin)
                    self.Tamil_words.append(native)

        print(f"Dataset from {data_path}: {len(self.latin_words)} pairs.")

        self.latin_vocab = latin_vocab or build_vocab(self.latin_words)
        self.Tamil_vocab = Tamil_vocab or build_vocab(self.Tamil_words)

    def __len__(self):
        return len(self.latin_words)

    def __getitem__(self, idx):
        src_seq = encode_sequence(self.latin_words[idx], self.latin_vocab)
        trg_seq = encode_sequence(self.Tamil_words[idx], self.Tamil_vocab)
        # add <sos> and <eos> tokens for target sequences
        trg_seq = [self.Tamil_vocab['<sos>']] + trg_seq + [self.Tamil_vocab['<eos>']]
        return src_seq, trg_seq

def collate_fn(batch):
    # batch is a list of tuples (src_seq, trg_seq)
    src_seqs, trg_seqs = zip(*batch)

    # find max lengths
    max_src_len = max(len(seq) for seq in src_seqs)
    max_trg_len = max(len(seq) for seq in trg_seqs)

    # pad sequences
    src_padded = [seq + [0]*(max_src_len - len(seq)) for seq in src_seqs]
    trg_padded = [seq + [0]*(max_trg_len - len(seq)) for seq in trg_seqs]

    # convert to tensors
    src_tensor = torch.tensor(src_padded, dtype=torch.long)
    trg_tensor = torch.tensor(trg_padded, dtype=torch.long)

    return src_tensor, trg_tensor

def compute_word_accuracy(preds, trg, pad_idx, sos_idx=1, eos_idx=2):
    """
    Compute word-level accuracy: a word is correct only if all tokens match (excluding pad, sos, eos).
    preds, trg: [batch_size, seq_len]
    """
    batch_size = preds.size(0)
    correct = 0

    for i in range(batch_size):
        # Get sequence for this example (exclude pad, sos, eos tokens)
        pred_seq = [idx for idx in preds[i].tolist() if idx not in [pad_idx, sos_idx, eos_idx]]
        trg_seq = [idx for idx in trg[i].tolist() if idx not in [pad_idx, sos_idx, eos_idx]]

        # Compare full sequences (exact match)
        if pred_seq == trg_seq:
            correct += 1

    return correct, batch_size

def beam_search(model, src_seq, src_vocab, tgt_vocab, beam_width=3, max_len=20):
    model.eval()
    index_to_char = {v: k for k, v in tgt_vocab.items()}
    device = model.device

    # Prepare input
    src_indices = encode_sequence(src_seq, src_vocab)
    src_tensor = torch.tensor([src_indices], dtype=torch.long).to(device)

    # Get encoder outputs and hidden state
    encoder_outputs, hidden = model._encode(src_tensor)

    # Start with start-of-sequence token
    beams = [([tgt_vocab['<sos>']], 0.0, hidden)]

    for _ in range(max_len):
        new_beams = []
        for seq, score, hidden in beams:
            last_token = torch.tensor([seq[-1]], dtype=torch.long).to(device)

            output, new_hidden = model.decoder(last_token, hidden)

            log_probs = torch.log_softmax(output, dim=-1)
            topk = torch.topk(log_probs, beam_width)

            for prob, idx in zip(topk.values[0], topk.indices[0]):
                new_seq = seq + [idx.item()]
                new_score = score + prob.item()
                new_beams.append((new_seq, new_score, new_hidden))

        beams = sorted(new_beams, key=lambda x: x[1], reverse=True)[:beam_width]

        # Stop if all beams end with EOS
        if all(seq[-1] == tgt_vocab['<eos>'] for seq, _, _ in beams):
            break

    # Pick the best beam
    best_seq = beams[0][0]
    # Remove special tokens for output
    decoded = [index_to_char[i] for i in best_seq if i not in {tgt_vocab['<sos>'], tgt_vocab['<eos>'], tgt_vocab['<pad>']}]
    return ''.join(decoded)

def train(model, dataloader, optimizer, criterion, clip=1, teacher_forcing_ratio=0.0):
    model.train()
    epoch_loss = 0
    total_words = 0
    correct_words = 0

    pad_idx = 0  # Pad index in vocabulary
    sos_idx = 1  # Start of sequence index
    eos_idx = 2  # End of sequence index

    for src, trg in dataloader:
        src, trg = src.to(model.device), trg.to(model.device)
        optimizer.zero_grad()

        # Generate sequence
        output = model(src, trg, teacher_forcing_ratio=teacher_forcing_ratio)
        output_dim = output.shape[-1]

        # Ignore first token (<sos>) in loss calculation
        output = output[:, 1:].contiguous().view(-1, output_dim)
        trg_flat = trg[:, 1:].contiguous().view(-1)

        loss = criterion(output, trg_flat)
        loss.backward()

        # Use gradient clipping
        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)
        optimizer.step()

        epoch_loss += loss.item()

        # Calculate word accuracy
        pred_tokens = output.argmax(1).view(trg[:, 1:].shape)  # [batch_size, trg_len-1]
        trg_trimmed = trg[:, 1:]                             # [batch_size, trg_len-1]

        correct, total = compute_word_accuracy(pred_tokens, trg_trimmed, pad_idx, sos_idx, eos_idx)
        correct_words += correct
        total_words += total

    avg_loss = epoch_loss / len(dataloader)
    word_acc = correct_words / total_words if total_words > 0 else 0

    return avg_loss, word_acc * 100

def evaluate(model, dataloader, criterion):
    model.eval()
    epoch_loss = 0
    total_words = 0
    correct_words = 0

    pad_idx = 0  # Pad index in vocabulary
    sos_idx = 1  # Start of sequence index
    eos_idx = 2  # End of sequence index

    with torch.no_grad():
        for src, trg in dataloader:
            src, trg = src.to(model.device), trg.to(model.device)

            # Generate full sequence with no teacher forcing
            output = model(src, trg, teacher_forcing_ratio=0.0)
            output_dim = output.shape[-1]

            # Ignore first token (<sos>) in loss calculation
            output = output[:, 1:].contiguous().view(-1, output_dim)
            trg_flat = trg[:, 1:].contiguous().view(-1)

            loss = criterion(output, trg_flat)
            epoch_loss += loss.item()

            # Calculate word accuracy
            pred_tokens = output.argmax(1).view(trg[:, 1:].shape)
            trg_trimmed = trg[:, 1:]

            correct, total = compute_word_accuracy(pred_tokens, trg_trimmed, pad_idx, sos_idx, eos_idx)
            correct_words += correct
            total_words += total

    avg_loss = epoch_loss / len(dataloader)
    word_acc = correct_words / total_words if total_words > 0 else 0

    return avg_loss, word_acc * 100

def predict_examples(model, dataloader, latin_index_to_token, Tamil_index_to_token, n=5):
    """Show a few examples of model predictions vs actual targets"""
    model.eval()
    pad_idx = 0
    sos_idx = 1  # Start of sequence
    eos_idx = 2  # End of sequence
    count = 0
    results = []

    print("\nPrediction Examples:")
    print("-" * 60)

    with torch.no_grad():
        for src, trg in dataloader:
            src, trg = src.to(model.device), trg.to(model.device)
            output = model(src, trg, teacher_forcing_ratio=0.0)
            print(output.shape)
            pred_tokens = output.argmax(-1)  # [batch_size, seq_len]

            for i in range(min(src.size(0), n - count)):
                # Decode input
                input_indices = [idx for idx in src[i].tolist() if idx != pad_idx]
                input_tokens = [latin_index_to_token.get(idx, '<unk>') for idx in input_indices]
                input_text = "".join(input_tokens)

                # Decode target
                target_indices = [idx for idx in trg[i].tolist() if idx not in [pad_idx, sos_idx, eos_idx]]
                target_tokens = [Tamil_index_to_token.get(idx, '<unk>') for idx in target_indices]
                target_text = "".join(target_tokens)

                # Decode prediction
                pred_indices = [idx for idx in pred_tokens[i].tolist() if idx not in [pad_idx, sos_idx, eos_idx]]
                pred_tokens_text = [Tamil_index_to_token.get(idx, '<unk>') for idx in pred_indices]
                pred_text = "".join(pred_tokens_text)

                result = {
                    "input": input_text,
                    "target": target_text,
                    "prediction": pred_text,
                    "correct": pred_text == target_text
                }
                results.append(result)

                print(f"Input:     {input_text}")
                print(f"Target:    {target_text}")
                print(f"Predicted: {pred_text}")
                print("-" * 60)

                count += 1
                if count >= n:
                    break
            if count >= n:
                break

    return results

# Define sweep configuration with improved parameters
def get_sweep_config():
    sweep_config = {
        'method': 'bayes',
        'metric': {
            'name': 'val_accuracy',
            'goal': 'maximize'
        },
        'parameters': {
            'embed_dim': {'values': [128, 256, 384]},
            'hidden_dim': {'values': [256, 384, 512]},
            'rnn_type': {'values': ['lstm', 'gru']},  # Removed basic RNN
            'encoder_layers': {'values': [1,2,3]},
            'decoder_layers': {'values': [1,2,3]},
            'dropout': {'values': [0.2, 0.3, 0.4]},
            'learning_rate': {'values': [0.00001, 0.0001,0.001,0.01]},
            'batch_size': {'values': [64, 128]},
            'epochs': {'values': [15,10,20]},
            'beam_size': {'values': [3, 5]},
            'bidirectional': {'values': [True,False]},
            'teacher_forcing_ratio': {'values': [0.0,0.3]},
            'weight_decay': {'values': [1e-5, 1e-4,1e-3]}
        }
    }
    return sweep_config

# Main training function for sweep runs
def train_sweep():
    # Initialize wandb with sweep configuration
    run = wandb.init(project="transliteration-model")

    # Access hyperparameters from wandb.config
    config = run.config

    # Setup device
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"Using device: {device}")

    # Define data paths
    data_dir = '/kaggle/input/dakshina-dataset/dakshina_dataset_v1.0/ta/lexicons'
    train_path = os.path.join(data_dir, 'ta.translit.sampled.train.tsv')
    dev_path = os.path.join(data_dir, 'ta.translit.sampled.dev.tsv')
    test_path = os.path.join(data_dir, 'ta.translit.sampled.test.tsv')

    # Check if files exist
    if not os.path.exists(train_path):
        raise FileNotFoundError(f"Could not find training data at {train_path}. Please check the path.")

    # Load datasets
    print("Loading training dataset...")
    train_dataset = DakshinaDataset(train_path)
    latin_vocab = train_dataset.latin_vocab
    Tamil_vocab = train_dataset.Tamil_vocab

    print("Loading validation dataset...")
    val_dataset = DakshinaDataset(
        dev_path,
        latin_vocab=latin_vocab,
        Tamil_vocab=Tamil_vocab
    )

    print("Loading test dataset...")
    test_dataset = DakshinaDataset(
        test_path,
        latin_vocab=latin_vocab,
        Tamil_vocab=Tamil_vocab
    )

    # Create DataLoaders
    train_loader = DataLoader(
        train_dataset,
        batch_size=config.batch_size,
        shuffle=True,
        collate_fn=collate_fn
    )

    val_loader = DataLoader(
        val_dataset,
        batch_size=config.batch_size,
        shuffle=False,
        collate_fn=collate_fn
    )

    test_loader = DataLoader(
        test_dataset,
        batch_size=config.batch_size,
        shuffle=False,
        collate_fn=collate_fn
    )

    # Get vocabulary information
    latin_vocab_size = len(latin_vocab)
    Tamil_vocab_size = len(Tamil_vocab)
    pad_idx = Tamil_vocab['<pad>']

    # Log vocabulary sizes
    wandb.log({"latin_vocab_size": latin_vocab_size, "Tamil_vocab_size": Tamil_vocab_size})

    # Generate a model name based on hyperparameters
    model_name = f"{config.rnn_type}_ed{config.embed_dim}_hid{config.hidden_dim}_enc{config.encoder_layers}_dec{config.decoder_layers}_drop{config.dropout}_tf{config.teacher_forcing_ratio}-wd{config.weight_decay}"

    wandb.run.name = model_name

    # Create model architecture
    encoder = Encoder(
        input_dim=latin_vocab_size,
        emb_dim=config.embed_dim,
        hidden_dim=config.hidden_dim,
        num_layers=config.encoder_layers,
        rnn_type=config.rnn_type,
        dropout=config.dropout,
        bidirectional=config.bidirectional
    )

    decoder = Decoder(
        output_dim=Tamil_vocab_size,
        emb_dim=config.embed_dim,
        hidden_dim=config.hidden_dim,
        num_layers=config.decoder_layers,
        rnn_type=config.rnn_type,
        dropout=config.dropout,
        # attention=config.use_attention
    )

    model = Seq2Seq(
        encoder,
        decoder,
        rnn_type=config.rnn_type,
        device=device,
    ).to(device)

    # Count and log the number of model parameters
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    wandb.log({
        "total_parameters": total_params,
        "trainable_parameters": trainable_params
    })
    print(f"Model: {model_name}")
    print(f"Total parameters: {total_params}")
    print(f"Trainable parameters: {trainable_params}")

    # Setup optimizer and loss function with weight decay for regularization
    optimizer = torch.optim.Adam(
        model.parameters(),
        lr=config.learning_rate,
        weight_decay=config.weight_decay
    )

    # Learning rate scheduler
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
        optimizer,
        mode='max',
        factor=0.5,
        patience=2,
        verbose=True
    )

    criterion = torch.nn.CrossEntropyLoss(ignore_index=pad_idx)

    # Training loop with early stopping
    best_val_loss = float('inf')
    best_val_acc = 0
    patience = 5  # Increased patience
    patience_counter = 0

    # Save directory for models
    model_dir = '/kaggle/working/models'
    os.makedirs(model_dir, exist_ok=True)

    # Track metrics for each epoch
    for epoch in range(config.epochs):
        print(f"\nEpoch {epoch+1}/{config.epochs}")

        # Train
        train_loss, train_acc = train(model, train_loader, optimizer, criterion, clip=1.0,
                                      teacher_forcing_ratio=config.teacher_forcing_ratio)
        print(f"Train Loss: {train_loss:.4f}, Accuracy: {train_acc:.2f}%")

        # Validate
        val_loss, val_acc = evaluate(model, val_loader, criterion)
        print(f"Val Loss: {val_loss:.4f}, Accuracy: {val_acc:.2f}%")

        # Log metrics to wandb
        wandb.log({
            "epoch": epoch + 1,
            "train_loss": train_loss,
            "train_accuracy": train_acc,
            "val_loss": val_loss,
            "val_accuracy": val_acc
        })

        # Save best model and check for early stopping
        if val_acc > best_val_acc:
            best_val_acc = val_acc
            best_val_loss = val_loss
            patience_counter = 0

            # Save the best model
            best_model_path = os.path.join(model_dir, f"{model_name}_best.pt")
            torch.save(model.state_dict(), best_model_path)
            print(f"New best model saved with val accuracy: {val_acc:.2f}%")
        else:
            patience_counter += 1
            if patience_counter >= patience:
                print(f"Early stopping after {epoch+1} epochs")
                break

    # Load the best model for testing
    best_model_path = os.path.join(model_dir, f"{model_name}_best.pt")
    try:
        model.load_state_dict(torch.load(best_model_path))
        print("Loaded best model for testing")
    except:
        print("Using current model for testing (best model not found)")

    # Final evaluation on test set
    test_loss, test_acc = evaluate(model, test_loader, criterion)
    print(f"\nTest Results -> Loss: {test_loss:.4f}, Accuracy: {test_acc:.2f}%")

    # Log final test metrics
    wandb.log({
        "test_loss": test_loss,
        "test_accuracy": test_acc
    })

    # Create index-to-token dictionaries for prediction display
    latin_index_to_token = {idx: token for token, idx in latin_vocab.items()}
    Tamil_index_to_token = {idx: token for token, idx in Tamil_vocab.items()}

    # Generate prediction examples for visualization
    example_results = predict_examples(
        model,
        test_loader,
        latin_index_to_token,
        Tamil_index_to_token,
        n=5
    )

    # Log the examples as a table in wandb
    example_table = wandb.Table(
        columns=["Input", "Target", "Prediction", "Correct"]
    )
    for result in example_results:
        example_table.add_data(
            result["input"],
            result["target"],
            result["prediction"],
            result["correct"]
        )
    # wandb.log({"prediction_examples": example_table})

    # Test beam search if enabled
    if config.beam_size > 1:
        print(f"\nTesting beam search with beam width {config.beam_size}...")
        beam_correct = 0
        beam_total = 0

        for src, trg in test_loader:
            src = src.to(device)
            trg = trg.to(device)
            for i in range(min(5, src.size(0))):  # Test beam search on a few examples
                # Get input sequence
                src_seq = [latin_index_to_token[idx] for idx in src[i].tolist() if idx != pad_idx]
                src_text = ''.join(src_seq)

                # Get target sequence
                trg_indices = [idx for idx in trg[i].tolist() if idx not in [pad_idx, 1]]  # Remove <pad> and <sos>
                trg_text = ''.join([Tamil_index_to_token.get(idx, '<unk>') for idx in trg_indices])

                # Run beam search
                beam_pred = beam_search(
                    model,
                    src_text,
                    latin_vocab,
                    Tamil_vocab,
                    beam_width=config.beam_size,
                    max_len=30
                )

                beam_correct += 1 if beam_pred == trg_text else 0
                beam_total += 1

                # print(f"Input: {src_text}")
                # print(f"Target: {trg_text}")
                # print(f"Beam Pred: {beam_pred}")
                # print("-" * 60)

        beam_acc = beam_correct / beam_total * 100 if beam_total > 0 else 0
        print(f"Beam search accuracy: {beam_acc:.2f}%")
        wandb.log({"beam_search_accuracy": beam_acc})

    return model, latin_vocab, Tamil_vocab




def run_wandb_sweep():
    sweep_config = get_sweep_config()
    sweep_id = wandb.sweep(sweep_config, project="tamil-english-noattn")
    wandb.agent(sweep_id, train_sweep, count=1)


# Main execution block for Kaggle
if __name__ == "__main__":
    # Run the wandb sweep
    run_wandb_sweep()