{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11819777,"sourceType":"datasetVersion","datasetId":7424359},{"sourceId":11860232,"sourceType":"datasetVersion","datasetId":7452616}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport os\nimport wandb\nfrom torch.utils.data import Dataset, DataLoader\nfrom collections import defaultdict\nfrom IPython.display import clear_output, display, HTML\nimport time\n\n# Initialize wandb - will use WANDB_API_KEY environment variable if available\ntry:\n    # Fix wandb login - only call once\n    wandb.login(key=\"999fe4f321204bd8f10135f3e40de296c23050f9\")\nexcept:\n    print(\"WandB login failed - results will not be logged. Set WANDB_API_KEY in your environment.\")\n\nclass Encoder(nn.Module):\n    def __init__(self, input_dim, emb_dim, hidden_dim, num_layers, rnn_type, dropout=0.5, bidirectional=False):\n        super(Encoder, self).__init__()\n        self.embedding = nn.Embedding(input_dim, emb_dim)\n        self.rnn_type = rnn_type\n        self.num_layers = num_layers\n        self.hidden_dim = hidden_dim\n        self.bidirectional = bidirectional\n        self.dropout = nn.Dropout(dropout)\n\n        self.directions = 2 if bidirectional else 1\n\n        if rnn_type == 'lstm':\n            self.rnn = nn.LSTM(emb_dim, hidden_dim, num_layers, batch_first=True,\n                              dropout=dropout if num_layers > 1 else 0,\n                              bidirectional=bidirectional)\n        elif rnn_type == 'gru':\n            self.rnn = nn.GRU(emb_dim, hidden_dim, num_layers, batch_first=True,\n                             dropout=dropout if num_layers > 1 else 0,\n                             bidirectional=bidirectional)\n        else:  # rnn\n            self.rnn = nn.RNN(emb_dim, hidden_dim, num_layers, batch_first=True,\n                             dropout=dropout if num_layers > 1 else 0,\n                             bidirectional=bidirectional)\n\n        # Projection layer to reduce bidirectional output to the expected dimension\n        if bidirectional:\n            self.projection = nn.Linear(hidden_dim * 2, hidden_dim)\n\n    def forward(self, src):\n        embedded = self.dropout(self.embedding(src))\n\n        if self.rnn_type == 'lstm':\n            outputs, (hidden, cell) = self.rnn(embedded)\n\n            # Process bidirectional states if needed\n            if self.bidirectional:\n                # Process hidden states\n                hidden = hidden.view(self.num_layers, self.directions, -1, self.hidden_dim)\n                # Concat forward and backward directions\n                hidden = torch.cat([hidden[:, 0], hidden[:, 1]], dim=2)\n                # Project to the correct dimension\n                hidden = self.projection(hidden)\n\n                # Process cell states\n                cell = cell.view(self.num_layers, self.directions, -1, self.hidden_dim)\n                cell = torch.cat([cell[:, 0], cell[:, 1]], dim=2)\n                cell = self.projection(cell)\n\n                # If outputs is bidirectional, we need to process it too\n                batch_size = outputs.size(0)\n                seq_len = outputs.size(1)\n                # Reshape and project encoder outputs\n                outputs = outputs.contiguous().view(batch_size, seq_len, self.hidden_dim * 2)\n                outputs = self.projection(outputs)\n\n                return outputs, (hidden, cell)\n            return outputs, (hidden, cell)\n        else:\n            outputs, hidden = self.rnn(embedded)\n\n            # Process bidirectional states if needed\n            if self.bidirectional:\n                hidden = hidden.view(self.num_layers, self.directions, -1, self.hidden_dim)\n                hidden = torch.cat([hidden[:, 0], hidden[:, 1]], dim=2)\n                hidden = self.projection(hidden)\n\n                # Process outputs if bidirectional\n                batch_size = outputs.size(0)\n                seq_len = outputs.size(1)\n                # Reshape and project encoder outputs\n                outputs = outputs.contiguous().view(batch_size, seq_len, self.hidden_dim * 2)\n                outputs = self.projection(outputs)\n\n            return outputs, hidden\n\nclass Decoder(nn.Module):\n    def __init__(self, output_dim, emb_dim, hidden_dim, num_layers, rnn_type, dropout=0.5, attention=False):\n        super(Decoder, self).__init__()\n        self.embedding = nn.Embedding(output_dim, emb_dim)\n        self.rnn_type = rnn_type\n        self.num_layers = num_layers\n        self.hidden_dim = hidden_dim\n        self.output_dim = output_dim\n        self.dropout = nn.Dropout(dropout)\n        self.attention = attention\n\n        # Increase input size if using attention\n        rnn_input_size = emb_dim + hidden_dim if attention else emb_dim\n\n        if rnn_type == 'lstm':\n            self.rnn = nn.LSTM(rnn_input_size, hidden_dim, num_layers, batch_first=True,\n                              dropout=dropout if num_layers > 1 else 0)\n        elif rnn_type == 'gru':\n            self.rnn = nn.GRU(rnn_input_size, hidden_dim, num_layers, batch_first=True,\n                             dropout=dropout if num_layers > 1 else 0)\n        else:  # rnn\n            self.rnn = nn.RNN(rnn_input_size, hidden_dim, num_layers, batch_first=True,\n                             dropout=dropout if num_layers > 1 else 0)\n\n        # Attention mechanism\n        if attention:\n            # Attention layers - simplified approach that works with any encoder output dimensionality\n            self.attn = nn.Linear(hidden_dim * 2, hidden_dim)\n            self.v = nn.Linear(hidden_dim, 1, bias=False)\n\n        self.fc_out = nn.Linear(hidden_dim, output_dim)\n\n    def forward(self, input_char, hidden, encoder_outputs=None):\n        input_char = input_char.unsqueeze(1)  # [batch_size, 1]\n        embedded = self.dropout(self.embedding(input_char))  # [batch_size, 1, emb_dim]\n\n        # Apply attention if enabled and encoder_outputs are provided\n        if self.attention and encoder_outputs is not None:\n            # Make sure we're extracting the hidden state correctly based on RNN type\n            if self.rnn_type == 'lstm':\n                query = hidden[0][-1].unsqueeze(1)  # [batch_size, 1, hidden_dim]\n            else:\n                query = hidden[-1].unsqueeze(1)  # [batch_size, 1, hidden_dim]\n\n            # Get dimensions\n            batch_size = encoder_outputs.size(0)\n            src_len = encoder_outputs.size(1)\n\n            # Create energy by combining query with encoder outputs\n            query = query.repeat(1, src_len, 1)  # [batch_size, src_len, hidden_dim]\n\n            # Concatenate query with encoder outputs\n            energy_input = torch.cat((query, encoder_outputs), dim=2)  # [batch_size, src_len, 2*hidden_dim]\n\n            # Calculate attention scores\n            energy = torch.tanh(self.attn(energy_input))  # [batch_size, src_len, hidden_dim]\n            attention = self.v(energy).squeeze(2)  # [batch_size, src_len]\n\n            # Apply softmax to get attention weights\n            attention_weights = torch.softmax(attention, dim=1).unsqueeze(1)  # [batch_size, 1, src_len]\n\n            # Create context vector by applying attention weights to encoder outputs\n            context = torch.bmm(attention_weights, encoder_outputs)  # [batch_size, 1, hidden_dim]\n\n            # Combine with embedding\n            rnn_input = torch.cat((embedded, context), dim=2)  # [batch_size, 1, emb_dim+hidden_dim]\n        else:\n            rnn_input = embedded\n\n        # Forward pass through RNN\n        if self.rnn_type == 'lstm':\n            output, (hidden, cell) = self.rnn(rnn_input, hidden)\n            hidden_state = (hidden, cell)\n        else:\n            output, hidden = self.rnn(rnn_input, hidden)\n            hidden_state = hidden\n\n        # Generate prediction\n        prediction = self.fc_out(output.squeeze(1))  # [batch_size, output_dim]\n\n        # Return attention_weights if available\n        if self.attention and encoder_outputs is not None:\n            return prediction, hidden_state, attention_weights.squeeze(1)  # [batch_size, src_len]\n        else:\n            return prediction, hidden_state, None\n\n\nclass Seq2Seq(nn.Module):\n    def __init__(self, encoder, decoder, rnn_type, device, use_attention=False):\n        super().__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n        self.rnn_type = rnn_type\n        self.device = device\n        self.use_attention = use_attention\n\n    def forward(self, src, trg, teacher_forcing_ratio=0.0):\n        batch_size = src.size(0)\n        trg_len = trg.size(1)\n        trg_vocab_size = self.decoder.output_dim\n\n        outputs = torch.zeros(batch_size, trg_len, trg_vocab_size).to(self.device)\n\n        # For collecting attention weights\n        all_attention_weights = [] if self.use_attention else None\n\n        # Encode the source sequence\n        encoder_outputs, hidden = self._encode(src)\n\n        # Use the first token as input to start decoding\n        input_char = trg[:, 0]  # <sos> token\n\n        \n\n        for t in range(1, trg_len):\n            # Generate output from decoder\n            if self.use_attention:\n                output, hidden, attn_weights  = self.decoder(input_char, hidden, encoder_outputs)\n                all_attention_weights.append(attn_weights)  # <-- ADD THIS LINE\n            else:\n                output, hidden, attn = self.decoder(input_char, hidden)\n\n            outputs[:, t] = output\n\n            # Teacher forcing: use real target or predicted token\n            teacher_force = torch.rand(1).item() < teacher_forcing_ratio\n            top1 = output.argmax(1)\n            input_char = trg[:, t] if teacher_force else top1\n\n        return outputs, all_attention_weights \n\n    def _encode(self, src):\n        # Get encoder outputs and final hidden state\n        encoder_outputs, hidden = self.encoder(src)\n\n        # Adjust hidden state dimensions if encoder and decoder have different layers\n        encoder_layers = self.encoder.num_layers\n        decoder_layers = self.decoder.num_layers\n\n        if self.rnn_type == 'lstm':\n            hidden_state, cell_state = hidden\n\n            # If encoder has fewer layers than decoder, pad with zeros\n            if encoder_layers < decoder_layers:\n                padding = torch.zeros(\n                    decoder_layers - encoder_layers,\n                    hidden_state.size(1),\n                    hidden_state.size(2)\n                ).to(self.device)\n                hidden_state = torch.cat([hidden_state, padding], dim=0)\n                cell_state = torch.cat([cell_state, padding], dim=0)\n\n            # If encoder has more layers than decoder, truncate\n            elif encoder_layers > decoder_layers:\n                hidden_state = hidden_state[:decoder_layers]\n                cell_state = cell_state[:decoder_layers]\n\n            # Make sure hidden dimensions match decoder's expected dimensions\n            if hidden_state.size(2) != self.decoder.hidden_dim:\n                # Project hidden state to the decoder's dimension using a linear projection\n                batch_size = hidden_state.size(1)\n                proj_hidden = torch.zeros(\n                    hidden_state.size(0),\n                    batch_size,\n                    self.decoder.hidden_dim\n                ).to(self.device)\n\n                for layer in range(hidden_state.size(0)):\n                    # Simple linear projection for each layer\n                    proj_hidden[layer] = hidden_state[layer].clone().view(batch_size, -1)[:, :self.decoder.hidden_dim]\n\n                # Apply the same projection to cell state\n                proj_cell = torch.zeros_like(proj_hidden)\n                for layer in range(cell_state.size(0)):\n                    proj_cell[layer] = cell_state[layer].clone().view(batch_size, -1)[:, :self.decoder.hidden_dim]\n\n                hidden = (proj_hidden, proj_cell)\n            else:\n                hidden = (hidden_state, cell_state)\n        else:\n            # For GRU and RNN\n            # If encoder has fewer layers than decoder, pad with zeros\n            if encoder_layers < decoder_layers:\n                padding = torch.zeros(\n                    decoder_layers - encoder_layers,\n                    hidden.size(1),\n                    hidden.size(2)\n                ).to(self.device)\n                hidden = torch.cat([hidden, padding], dim=0)\n            # If encoder has more layers than decoder, truncate\n            elif encoder_layers > decoder_layers:\n                hidden = hidden[:decoder_layers]\n\n            # Make sure hidden dimensions match decoder's expected dimensions\n            if hidden.size(2) != self.decoder.hidden_dim:\n                # Project hidden state to the decoder's dimension\n                batch_size = hidden.size(1)\n                proj_hidden = torch.zeros(\n                    hidden.size(0),\n                    batch_size,\n                    self.decoder.hidden_dim\n                ).to(self.device)\n\n                for layer in range(hidden.size(0)):\n                    # Simple linear projection for each layer\n                    proj_hidden[layer] = hidden[layer].clone().view(batch_size, -1)[:, :self.decoder.hidden_dim]\n\n                hidden = proj_hidden\n\n        return encoder_outputs, hidden\n\n        def predict(self, src):\n            \"\"\"Generate predictions from the model for evaluation\"\"\"\n            batch_size = src.size(0)\n            max_len = 100  # Maximum prediction length\n            \n            # Create tensor to store predictions\n            predictions = torch.zeros(batch_size, max_len, dtype=torch.long).to(self.device)\n            \n            # Encode the source sequence\n            encoder_outputs, hidden = self._encode(src)\n            \n            # Initialize with start token\n            input_char = torch.tensor([1] * batch_size).to(self.device)  # <sos> token\n            \n            for t in range(max_len):\n                # Generate output from decoder\n                if self.use_attention:\n                    output, hidden, _ = self.decoder(input_char, hidden, encoder_outputs)\n                else:\n                    output, hidden, _ = self.decoder(input_char, hidden)\n                    \n                # Get the most likely next token\n                predictions[:, t] = output.argmax(1)\n                \n                # Update input for next step\n                input_char = predictions[:, t]\n                \n                # Stop if all sequences have predicted <eos>\n                if (input_char == 2).all():  # All sequences generated <eos>\n                    break\n                    \n            return predictions\n\n\n# Character-level vocabulary builder\ndef build_vocab(tokens):\n    vocab = {'<pad>': 0, '<sos>': 1, '<eos>': 2, '<unk>': 3}\n    for token in tokens:\n        for char in token:\n            if char not in vocab:\n                vocab[char] = len(vocab)\n    return vocab\n\ndef encode_sequence(seq, vocab):\n    return [vocab.get(char, vocab['<unk>']) for char in seq]\n\nclass DakshinaDataset(Dataset):\n    def __init__(self, data_path, latin_vocab=None, devanagari_vocab=None):\n        self.latin_words = []\n        self.devanagari_words = []\n\n        # Group all transliterations by Devanagari word\n        candidates = defaultdict(list)\n\n        with open(data_path, encoding='utf-8') as f:\n            for line in f:\n                parts = line.strip().split('\\t')\n                if len(parts) != 3:\n                    continue\n                native, latin, rel = parts[0], parts[1], int(parts[2])\n                candidates[native].append((latin, rel))\n\n        # Keep only the transliteration(s) with highest score for each native word\n        for native, translits in candidates.items():\n            max_rel = max(rel for _, rel in translits)\n            for latin, rel in translits:\n                if rel == max_rel:\n                    self.latin_words.append(latin)\n                    self.devanagari_words.append(native)\n\n        print(f\"Dataset from {data_path}: {len(self.latin_words)} pairs.\")\n\n        self.latin_vocab = latin_vocab or build_vocab(self.latin_words)\n        self.devanagari_vocab = devanagari_vocab or build_vocab(self.devanagari_words)\n\n    def __len__(self):\n        return len(self.latin_words)\n\n    def __getitem__(self, idx):\n        src_seq = encode_sequence(self.latin_words[idx], self.latin_vocab)\n        trg_seq = encode_sequence(self.devanagari_words[idx], self.devanagari_vocab)\n        # add <sos> and <eos> tokens for target sequences\n        trg_seq = [self.devanagari_vocab['<sos>']] + trg_seq + [self.devanagari_vocab['<eos>']]\n        return src_seq, trg_seq\n\ndef collate_fn(batch):\n    # batch is a list of tuples (src_seq, trg_seq)\n    src_seqs, trg_seqs = zip(*batch)\n\n    # find max lengths\n    max_src_len = max(len(seq) for seq in src_seqs)\n    max_trg_len = max(len(seq) for seq in trg_seqs)\n\n    # pad sequences\n    src_padded = [seq + [0]*(max_src_len - len(seq)) for seq in src_seqs]\n    trg_padded = [seq + [0]*(max_trg_len - len(seq)) for seq in trg_seqs]\n\n    # convert to tensors\n    src_tensor = torch.tensor(src_padded, dtype=torch.long)\n    trg_tensor = torch.tensor(trg_padded, dtype=torch.long)\n\n    return src_tensor, trg_tensor\n\ndef compute_word_accuracy(preds, trg, pad_idx, sos_idx=1, eos_idx=2):\n    \"\"\"\n    Compute word-level accuracy: a word is correct only if all tokens match (excluding pad, sos, eos).\n    preds, trg: [batch_size, seq_len]\n    \"\"\"\n    batch_size = preds.size(0)\n    correct = 0\n\n    for i in range(batch_size):\n        # Get sequence for this example (exclude pad, sos, eos tokens)\n        pred_seq = [idx for idx in preds[i].tolist() if idx not in [pad_idx, sos_idx, eos_idx]]\n        trg_seq = [idx for idx in trg[i].tolist() if idx not in [pad_idx, sos_idx, eos_idx]]\n\n        # Compare full sequences (exact match)\n        if pred_seq == trg_seq:\n            correct += 1\n\n    return correct, batch_size\n\ndef beam_search(model, src_seq, src_vocab, tgt_vocab, beam_width=3, max_len=20):\n    model.eval()\n    index_to_char = {v: k for k, v in tgt_vocab.items()}\n    device = model.device\n\n    # Prepare input\n    src_indices = encode_sequence(src_seq, src_vocab)\n    src_tensor = torch.tensor([src_indices], dtype=torch.long).to(device)\n\n    # Get encoder outputs and hidden state\n    encoder_outputs, hidden = model._encode(src_tensor)\n\n    # Start with start-of-sequence token\n    beams = [([tgt_vocab['<sos>']], 0.0, hidden)]\n\n    for _ in range(max_len):\n        new_beams = []\n        for seq, score, hidden in beams:\n            last_token = torch.tensor([seq[-1]], dtype=torch.long).to(device)\n\n            # Use attention if model has it\n            if model.use_attention:\n                output, new_hidden, _ = model.decoder(last_token, hidden, encoder_outputs)\n            else:\n                output, new_hidden = model.decoder(last_token, hidden)\n\n            log_probs = torch.log_softmax(output, dim=-1)\n            topk = torch.topk(log_probs, beam_width)\n\n            for prob, idx in zip(topk.values[0], topk.indices[0]):\n                new_seq = seq + [idx.item()]\n                new_score = score + prob.item()\n                new_beams.append((new_seq, new_score, new_hidden))\n\n        beams = sorted(new_beams, key=lambda x: x[1], reverse=True)[:beam_width]\n\n        # Stop if all beams end with EOS\n        if all(seq[-1] == tgt_vocab['<eos>'] for seq, _, _ in beams):\n            break\n\n    # Pick the best beam\n    best_seq = beams[0][0]\n    # Remove special tokens for output\n    decoded = [index_to_char[i] for i in best_seq if i not in {tgt_vocab['<sos>'], tgt_vocab['<eos>'], tgt_vocab['<pad>']}]\n    return ''.join(decoded)\n\ndef train(model, dataloader, optimizer, criterion, clip=1, teacher_forcing_ratio=0.0):\n    model.train()\n    epoch_loss = 0\n    total_words = 0\n    correct_words = 0\n\n    pad_idx = 0  # Pad index in vocabulary\n    sos_idx = 1  # Start of sequence index\n    eos_idx = 2  # End of sequence index\n\n    for src, trg in dataloader:\n        src, trg = src.to(model.device), trg.to(model.device)\n        optimizer.zero_grad()\n\n        # Generate sequence\n        output, _ = model(src, trg, teacher_forcing_ratio=teacher_forcing_ratio)\n        output_dim = output.shape[-1]\n\n        # Ignore first token (<sos>) in loss calculation\n        output = output[:, 1:].contiguous().view(-1, output_dim)\n        trg_flat = trg[:, 1:].contiguous().view(-1)\n\n        loss = criterion(output, trg_flat)\n        loss.backward()\n\n        # Use gradient clipping\n        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n        optimizer.step()\n\n        epoch_loss += loss.item()\n\n        # Calculate word accuracy\n        pred_tokens = output.argmax(1).view(trg[:, 1:].shape)  # [batch_size, trg_len-1]\n        trg_trimmed = trg[:, 1:]                             # [batch_size, trg_len-1]\n\n        correct, total = compute_word_accuracy(pred_tokens, trg_trimmed, pad_idx, sos_idx, eos_idx)\n        correct_words += correct\n        total_words += total\n\n    avg_loss = epoch_loss / len(dataloader)\n    word_acc = correct_words / total_words if total_words > 0 else 0\n\n    return avg_loss, word_acc * 100\n\ndef evaluate(model, dataloader, criterion):\n    model.eval()\n    epoch_loss = 0\n    total_words = 0\n    correct_words = 0\n\n    pad_idx = 0  # Pad index in vocabulary\n    sos_idx = 1  # Start of sequence index\n    eos_idx = 2  # End of sequence index\n\n    with torch.no_grad():\n        for src, trg in dataloader:\n            src, trg = src.to(model.device), trg.to(model.device)\n\n            # Generate full sequence with no teacher forcing\n            output, _  = model(src, trg, teacher_forcing_ratio=0.0)\n            # Visualize for the first example in batch\n            # attn = torch.stack([aw[0] for aw in attention_weights]).cpu().numpy()\n            output_dim = output.shape[-1]\n\n            # Ignore first token (<sos>) in loss calculation\n            output = output[:, 1:].contiguous().view(-1, output_dim)\n            trg_flat = trg[:, 1:].contiguous().view(-1)\n\n            loss = criterion(output, trg_flat)\n            epoch_loss += loss.item()\n\n            # Calculate word accuracy\n            pred_tokens = output.argmax(1).view(trg[:, 1:].shape)\n            trg_trimmed = trg[:, 1:]\n\n            correct, total = compute_word_accuracy(pred_tokens, trg_trimmed, pad_idx, sos_idx, eos_idx)\n            correct_words += correct\n            total_words += total\n\n    avg_loss = epoch_loss / len(dataloader)\n    word_acc = correct_words / total_words if total_words > 0 else 0\n\n    return avg_loss, word_acc * 100\n\ndef predict_examples(model, dataloader, latin_index_to_token, devanagari_index_to_token, n=5):\n    \"\"\"Show a few examples of model predictions vs actual targets\"\"\"\n    model.eval()\n    pad_idx = 0\n    sos_idx = 1  # Start of sequence\n    eos_idx = 2  # End of sequence\n    count = 0\n    results = []\n\n    print(\"\\nPrediction Examples:\")\n    print(\"-\" * 60)\n\n    with torch.no_grad():\n        for src, trg in dataloader:\n            src, trg = src.to(model.device), trg.to(model.device)\n            output, _ = model(src, trg, teacher_forcing_ratio=0.0)\n            # print(len(output))\n            pred_tokens = output.argmax(-1)  # [batch_size, seq_len]\n\n            for i in range(min(src.size(0), n - count)):\n                # Decode input\n                input_indices = [idx for idx in src[i].tolist() if idx != pad_idx]\n                input_tokens = [latin_index_to_token.get(idx, '<unk>') for idx in input_indices]\n                input_text = \"\".join(input_tokens)\n\n                # Decode target\n                target_indices = [idx for idx in trg[i].tolist() if idx not in [pad_idx, sos_idx, eos_idx]]\n                target_tokens = [devanagari_index_to_token.get(idx, '<unk>') for idx in target_indices]\n                target_text = \"\".join(target_tokens)\n\n                # Decode prediction\n                pred_indices = [idx for idx in pred_tokens[i].tolist() if idx not in [pad_idx, sos_idx, eos_idx]]\n                pred_tokens_text = [devanagari_index_to_token.get(idx, '<unk>') for idx in pred_indices]\n                pred_text = \"\".join(pred_tokens_text)\n\n                result = {\n                    \"input\": input_text,\n                    \"target\": target_text,\n                    \"prediction\": pred_text,\n                    \"correct\": pred_text == target_text\n                }\n                results.append(result)\n\n                print(f\"Input:     {input_text}\")\n                print(f\"Target:    {target_text}\")\n                print(f\"Predicted: {pred_text}\")\n                print(\"-\" * 60)\n\n                count += 1\n                if count >= n:\n                    break\n            if count >= n:\n                break\n\n    return results\n    \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Define sweep configuration with improved parameters\ndef get_sweep_config():\n    sweep_config = {\n        'method': 'random',\n        'metric': {\n            'name': 'val_accuracy',\n            'goal': 'maximize'},\n            'parameters': {\n            'embed_dim': {'values': [128,256,384]},\n            'hidden_dim': {'values': [256,384,512]},\n            'rnn_type': {'values': ['lstm','gru']}, \n            'encoder_layers': {'values': [3,2]},\n            'decoder_layers': {'values': [3,2]},\n            'dropout': {'values': [0.2,0.3]},\n            'learning_rate': {'values': [0.001,1e-4]},\n            'batch_size': {'values': [128,256]},\n            'epochs': {'values': [5,10]},\n            'beam_size': {'values': [3,5]},\n            'use_attention': {'values': [True]},\n            'bidirectional': {'values': [True]},\n            'teacher_forcing_ratio': {'values': [0.0, 0.3]},\n            'weight_decay': {'values': [1e-5, 1e-6]}\n        }\n    }\n    return sweep_config\n\n\n\n# Global variables to track overall best model\noverall_best_val_acc = 0\noverall_best_model_path = \"\"\noverall_best_config = None\noverall_best_run_id = None\n\n\n\ndef save_test_predictions(model, test_loader, latin_vocab, devanagari_vocab, save_dir=\"predictions_attention\"):\n    \"\"\"Saves all test predictions for the best model\"\"\"\n    os.makedirs(save_dir, exist_ok=True)\n    model.eval()\n    \n    latin_idx_to_token = {idx: tok for tok, idx in latin_vocab.items()}\n    devanagari_idx_to_token = {idx: tok for tok, idx in devanagari_vocab.items()}\n    pad_idx = devanagari_vocab['<pad>']\n    \n    with open(os.path.join(save_dir, \"all_predictions.tsv\"), \"w\") as f:\n        f.write(\"Input\\tTarget\\tPrediction\\tCorrect\\n\")\n        \n        correct = 0\n        total = 0\n        \n        for src, trg in test_loader:\n            src, trg = src.to(model.device), trg.to(model.device)\n            outputs = model.predict(src)\n            \n            for i in range(src.size(0)):\n                # Process input\n                input_indices = [idx.item() for idx in src[i] if idx.item() != latin_vocab['<pad>']]\n                input_text = ''.join([latin_idx_to_token[idx] for idx in input_indices])\n                \n                # Process target (skip <sos> and <pad>)\n                target_indices = [idx.item() for idx in trg[i] \n                                 if idx.item() not in [devanagari_vocab['<pad>'], devanagari_vocab['<sos>']]]\n                target_text = ''.join([devanagari_idx_to_token[idx] for idx in target_indices])\n                \n                # Process prediction (skip <pad>)\n                pred_indices = [idx.item() for idx in outputs[i] if idx.item() != pad_idx]\n                pred_text = ''.join([devanagari_idx_to_token.get(idx, '<?>') for idx in pred_indices])\n                \n                # Calculate accuracy\n                is_correct = int(target_text == pred_text)\n                correct += is_correct\n                total += 1\n                \n                f.write(f\"{input_text}\\t{target_text}\\t{pred_text}\\t{is_correct}\\n\")\n        \n        # Save accuracy summary\n        with open(os.path.join(save_dir, \"accuracy.txt\"), \"w\") as acc_file:\n            acc_file.write(f\"Test Accuracy: {correct/total*100:.2f}%\")\n\n\n\n\n# Fix 3: Update plot_attention_grid function to properly extract and visualize attention\ndef plot_attention_grid(model, dataloader, idx_to_src_token, idx_to_tgt_token, num_samples=9):\n    \"\"\"Generates 3x3 grid of attention heatmaps\"\"\"\n    model.eval()\n    samples_collected = 0\n    fig, axes = plt.subplots(3, 3, figsize=(15, 15))\n    axes = axes.flatten()\n    \n    with torch.no_grad():\n        for src, trg in dataloader:\n            if samples_collected >= num_samples:\n                break\n                \n            src, trg = src.to(model.device), trg.to(model.device)\n            \n            # Process one sample at a time to capture attention weights\n            for i in range(min(src.size(0), num_samples - samples_collected)):\n                src_i = src[i:i+1]\n                trg_i = trg[i:i+1]\n                \n                # Forward pass to get attention weights\n                encoder_outputs, hidden = model._encode(src_i)\n                \n                # Initialize decoder\n                decoder_input = torch.tensor([idx_to_tgt_token.index('<sos>')]).to(model.device)\n                \n                # Get source tokens\n                src_tokens = [idx_to_src_token[idx.item()] for idx in src_i[0] \n                              if idx.item() != 0 and idx.item() in idx_to_src_token]\n                src_len = len(src_tokens)\n                \n                # Store output tokens and attention weights\n                output_tokens = []\n                attention_weights = []\n                \n                # Decode step by step to capture attention\n                for _ in range(50):  # Max target length\n                    output, hidden, attn_weights = model.decoder(\n                        decoder_input, hidden, encoder_outputs)\n                    \n                    # Get predicted token\n                    top_token = output.argmax(1)\n                    token_val = top_token.item()\n                    \n                    # Stop if <eos> or <pad>\n                    if token_val == 0 or token_val == 2:\n                        break\n                        \n                    # Add to outputs\n                    if token_val in idx_to_tgt_token:\n                        output_tokens.append(idx_to_tgt_token[token_val])\n                    \n                    # Save attention weights\n                    if attn_weights is not None:\n                        attention_weights.append(attn_weights.cpu().numpy())\n                    \n                    # Next input\n                    decoder_input = top_token\n                \n                # Skip if no attention weights\n                if not attention_weights:\n                    continue\n                \n                # Plot attention matrix\n                ax = axes[samples_collected]\n                attn_matrix = np.stack(attention_weights)[:, 0, :src_len]\n                \n                sns.heatmap(\n                    attn_matrix, \n                    ax=ax,\n                    xticklabels=src_tokens,\n                    yticklabels=output_tokens,\n                    cmap=\"viridis\"\n                )\n                \n                # Format plot\n                ax.set_title(f\"Sample {samples_collected+1}\")\n                ax.set_xlabel(\"Source (Latin)\")\n                ax.set_ylabel(\"Target (Tamil)\")\n                \n                samples_collected += 1\n                if samples_collected >= num_samples:\n                    break\n    \n    plt.tight_layout()\n    plt.savefig(\"attention_heatmaps.png\", bbox_inches='tight', dpi=300)\n    wandb.log({\"attention_heatmaps\": wandb.Image(\"attention_heatmaps.png\")})\n    plt.close()\n\n# Entry point - runs a wandb sweep\ndef run_wandb_sweep():\n    sweep_config = get_sweep_config()\n    sweep_id = wandb.sweep(sweep_config, project=\"transliteration-model-tam1\")\n    wandb.agent(sweep_id, train_sweep, count=2)\n\n\n\ndef train_sweep():\n    global overall_best_val_acc, overall_best_model_path, overall_best_config, overall_best_run_id\n    \n    # Initialize wandb\n    run = wandb.init(project=\"transliteration-model\")\n    config = run.config\n    \n    # Setup device and seed\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    torch.manual_seed(42)  # Add seed for reproducibility\n    \n    # --- Data Loading ---\n    data_dir = '/kaggle/input/dakshina-dataset/dakshina_dataset_v1.0/ta/lexicons/'\n    train_dataset = DakshinaDataset(os.path.join(data_dir, 'ta.translit.sampled.train.tsv'))\n    latin_vocab = train_dataset.latin_vocab\n    devanagari_vocab = train_dataset.devanagari_vocab\n    \n    # Create DataLoaders\n    pad_idx = devanagari_vocab['<pad>']\n    train_loader = DataLoader(\n        train_dataset,\n        batch_size=config.batch_size,\n        shuffle=True,\n        collate_fn=collate_fn\n    )\n    val_loader = DataLoader(\n        DakshinaDataset(os.path.join(data_dir, 'ta.translit.sampled.dev.tsv'),\n                      latin_vocab=latin_vocab,\n                      devanagari_vocab=devanagari_vocab),\n        batch_size=config.batch_size,\n        shuffle=False,\n        collate_fn=collate_fn\n    )\n    test_loader = DataLoader(\n        DakshinaDataset(os.path.join(data_dir, 'ta.translit.sampled.test.tsv'),\n                      latin_vocab=latin_vocab,\n                      devanagari_vocab=devanagari_vocab),\n        batch_size=config.batch_size,\n        shuffle=False,\n        collate_fn=collate_fn\n    )\n\n    # --- Model Setup ---\n    model = Seq2Seq(\n        encoder=Encoder(\n            input_dim=len(latin_vocab),\n            emb_dim=config.embed_dim,\n            hidden_dim=config.hidden_dim,\n            num_layers=config.encoder_layers,\n            rnn_type=config.rnn_type,\n            dropout=config.dropout,\n            bidirectional=config.bidirectional\n        ),\n        decoder=Decoder(\n            output_dim=len(devanagari_vocab),\n            emb_dim=config.embed_dim,\n            hidden_dim=config.hidden_dim,\n            num_layers=config.decoder_layers,\n            rnn_type=config.rnn_type,\n            dropout=config.dropout,\n            attention=config.use_attention\n        ),\n        device=device,\n        rnn_type=config.rnn_type,\n        use_attention=config.use_attention\n    ).to(device)\n\n    # --- Training Setup ---\n    optimizer = torch.optim.Adam(\n        model.parameters(),\n        lr=config.learning_rate,\n        weight_decay=config.weight_decay\n    )\n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n        optimizer, mode='max', factor=0.5, patience=2)\n    criterion = torch.nn.CrossEntropyLoss(ignore_index=pad_idx)\n\n    # --- Training Loop ---\n    best_val_acc = 0\n    for epoch in range(config.epochs):\n        # Training\n        train_loss, train_acc = train(\n            model, train_loader, optimizer, criterion,\n            teacher_forcing_ratio=config.teacher_forcing_ratio\n        )\n        \n        # Validation\n        val_loss, val_acc = evaluate(model, val_loader, criterion)\n        \n        # Update learning rate\n        scheduler.step(val_acc)\n        \n        # Log metrics\n        wandb.log({\n            \"epoch\": epoch + 1,\n            \"train_loss\": train_loss,\n            \"train_accuracy\": train_acc,\n            \"val_loss\": val_loss,\n            \"val_accuracy\": val_acc,\n            \"learning_rate\": optimizer.param_groups[0]['lr']\n        })\n\n        # Track best model in this run\n        if val_acc > best_val_acc:\n            best_val_acc = val_acc\n            # Save model checkpoint\n            model_path = f\"model_run_{run.id}_epoch_{epoch}.pt\"\n            torch.save({\n                'model_state_dict': model.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n                'epoch': epoch,\n                'val_accuracy': val_acc\n            }, model_path)\n            print(f\"New best model saved with val accuracy: {val_acc:.2f}%\")\n\n        # Track overall best model across all runs\n        if val_acc > overall_best_val_acc:\n            overall_best_val_acc = val_acc\n            overall_best_config = dict(config)\n            overall_best_run_id = run.id\n            overall_best_model_path = f\"overall_best_model.pt\"\n            torch.save({\n                'model_state_dict': model.state_dict(),\n                'config': dict(config),\n                'val_accuracy': val_acc\n            }, overall_best_model_path)\n            print(f\"New overall best model saved with val accuracy: {val_acc:.2f}%\")\n\n    # --- Final Evaluation ---\n    # Test evaluation on final model\n    test_loss, test_acc = evaluate(model, test_loader, criterion)\n    wandb.log({\"final_test_accuracy\": test_acc, \"final_test_loss\": test_loss})\n    print(f\"Final Test Accuracy: {test_acc:.2f}%\")\n    \n    # Show prediction examples\n    latin_idx_to_token = {idx: tok for tok, idx in latin_vocab.items()}\n    devanagari_idx_to_token = {idx: tok for tok, idx in devanagari_vocab.items()}\n    predict_examples(model, test_loader, latin_idx_to_token, devanagari_idx_to_token, n=5)\n    \n    # Return model and vocabularies for external use\n    return model, latin_vocab, devanagari_vocab, test_acc\n\n\n\n\ndef train_manual(\n    embed_dim=256,\n    hidden_dim=384,\n    rnn_type='lstm',\n    encoder_layers=2,\n    decoder_layers=2,\n    dropout=0.3,\n    learning_rate=0.001,\n    batch_size=128,\n    epochs=1,\n    use_attention=True,\n    bidirectional=True,\n    teacher_forcing_ratio=0.3,\n    weight_decay=1e-5\n):\n    # Setup device\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    print(f\"Using device: {device}\")\n\n    # Load datasets (same as original)\n    data_dir = '/kaggle/input/dakshina-dataset/dakshina_dataset_v1.0/ta/lexicons'\n    train_path = os.path.join(data_dir, 'ta.translit.sampled.train.tsv')\n    dev_path = os.path.join(data_dir, 'ta.translit.sampled.dev.tsv')\n    test_path = os.path.join(data_dir, 'ta.translit.sampled.test.tsv')\n\n    train_dataset = DakshinaDataset(train_path)\n    latin_vocab = train_dataset.latin_vocab\n    devanagari_vocab = train_dataset.devanagari_vocab\n\n    val_dataset = DakshinaDataset(dev_path, latin_vocab=latin_vocab, devanagari_vocab=devanagari_vocab)\n    test_dataset = DakshinaDataset(test_path, latin_vocab=latin_vocab, devanagari_vocab=devanagari_vocab)\n\n    # Create DataLoaders\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n\n    # Create model\n    encoder = Encoder(\n        input_dim=len(latin_vocab),\n        emb_dim=embed_dim,\n        hidden_dim=hidden_dim,\n        num_layers=encoder_layers,\n        rnn_type=rnn_type,\n        dropout=dropout,\n        bidirectional=bidirectional\n    )\n\n    decoder = Decoder(\n        output_dim=len(devanagari_vocab),\n        emb_dim=embed_dim,\n        hidden_dim=hidden_dim,\n        num_layers=decoder_layers,\n        rnn_type=rnn_type,\n        dropout=dropout,\n        attention=use_attention\n    )\n\n    model = Seq2Seq(\n        encoder,\n        decoder,\n        rnn_type=rnn_type,\n        device=device,\n        use_attention=use_attention\n    ).to(device)\n\n    # Optimizer and criterion\n    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n    criterion = nn.CrossEntropyLoss(ignore_index=devanagari_vocab['<pad>'])\n    \n    # Training loop (same as original)\n    best_val_acc = 0\n    patience = 5\n    patience_counter = 0\n    \n    for epoch in range(epochs):\n        print(f\"\\nEpoch {epoch+1}/{epochs}\")\n        \n        # Training\n        train_loss, train_acc = train(model, train_loader, optimizer, criterion)\n        print(f\"Train Loss: {train_loss:.4f}, Accuracy: {train_acc:.2f}%\")\n        \n        # Validation\n        val_loss, val_acc = evaluate(model, val_loader, criterion)\n        print(f\"Val Loss: {val_loss:.4f}, Accuracy: {val_acc:.2f}%\")\n        \n        # Early stopping check\n        if val_acc > best_val_acc:\n            best_val_acc = val_acc\n            patience_counter = 0\n            torch.save(model.state_dict(), 'best_model.pt')\n            print(\"Saved new best model\")\n        else:\n            patience_counter += 1\n            if patience_counter >= patience:\n                print(\"Early stopping\")\n                break\n\n    # Load best model and test\n    model.load_state_dict(torch.load('best_model.pt'))\n    test_loss, test_acc = evaluate(model, test_loader, criterion)\n    print(f\"\\nFinal Test Results -> Loss: {test_loss:.4f}, Accuracy: {test_acc:.2f}%\")\n\n    return model, latin_vocab, devanagari_vocab\n\n\n# Example configuration\nmodel, latin_vocab, devanagari_vocab = train_manual(\n    embed_dim=256,\n    hidden_dim=512,\n    rnn_type='gru',\n    encoder_layers=2,\n    decoder_layers=2,\n    dropout=0.3,\n    learning_rate=0.001,\n    batch_size=128,\n    epochs=5,\n    use_attention=True,\n    bidirectional=True,\n    teacher_forcing_ratio=0.3,\n    weight_decay=1e-5\n)\n\n\n\n\n\nimport numpy as np\n\ndef show_attention(input_sentence, output_sentence, attentions):\n    \"\"\"Visualize attention weights matrix with axes labels\"\"\"\n    fig = plt.figure(figsize=(10,10))\n    ax = fig.add_subplot(111)\n    \n    # Transpose attention matrix for proper alignment\n    attentions = attentions[:len(output_sentence), :len(input_sentence)].T\n    \n    cax = ax.matshow(attentions, cmap='bone', aspect='auto')\n    fig.colorbar(cax)\n\n    # Set up axes\n    ax.set_xticks(range(len(output_sentence)))\n    ax.set_yticks(range(len(input_sentence)))\n    ax.set_xticklabels(output_sentence)\n    ax.set_yticklabels(input_sentence)\n\n    # Rotate labels and show\n    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"left\")\n    plt.tight_layout()\n    plt.show()\n\n\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n\n\ndef plot_connectivity(input_chars, output_chars, attention_matrix):\n    \"\"\"Visualize attention with Devanagari labels for each row\"\"\"\n    fig = plt.figure(figsize=(12, 8))\n    gs = fig.add_gridspec(1, 2, width_ratios=[1, 10])\n    \n    # Create subplots\n    ax_label = fig.add_subplot(gs[0])\n    ax_heat = fig.add_subplot(gs[1])\n\n    # Plot Devanagari labels on the left\n    ax_label.set_yticks(range(len(output_chars)))\n    ax_label.set_yticklabels(output_chars, fontsize=14, fontfamily='Noto Sans Devanagari')\n    ax_label.set_xticks([])\n    ax_label.set_title(\"Output\\n(Devanagari)\", pad=20)\n\n    # Plot heatmap on the right\n    cax = ax_heat.matshow(attention_matrix, cmap='viridis', aspect='auto')\n    fig.colorbar(cax, ax=ax_heat, fraction=0.046)\n\n    # Set up axes for heatmap\n    ax_heat.set_xticks(range(len(input_chars)))\n    ax_heat.set_xticklabels(input_chars, rotation=45, ha='left', fontsize=12)\n    ax_heat.set_yticks(range(len(output_chars)))\n    ax_heat.set_yticklabels([])  # Remove default yticks\n    ax_heat.xaxis.set_ticks_position('bottom')\n    ax_heat.set_title(\"Attention Weights (Input: Latin)\", pad=20)\n    \n    # Add grid lines\n    ax_heat.set_xticks(np.arange(-.5, len(input_chars)), minor=True)\n    ax_heat.set_yticks(np.arange(-.5, len(output_chars)), minor=True)\n    ax_heat.grid(which='minor', color='gray', linestyle='-', linewidth=0.5)\n    ax_heat.tick_params(which='minor', length=0)\n\n    plt.tight_layout()\n    plt.show()\n\n\n\n\nfrom IPython.display import HTML, display\n\ndef html_print(s):\n    \"\"\"Helper function to render HTML\"\"\"\n    display(HTML(s))\n\ndef cstr(s, color='black'):\n    if s == ' ':\n        return f\"<text style=color:#000;padding-left:10px;background-color:{color}> </text>\"\n    else:\n        return f\"<text style=color:#000;background-color:{color}>{s} </text>\"\n\ndef print_color(t):\n    display(HTML(''.join([cstr(ti, color=ci) for ti,ci in t])))\n\ndef get_clr(value):\n    # Fixed color list (there was a missing comma in your original)\n    colors = ['#85c2e1', '#89c4e2', '#95cae5', '#99cce6', '#a1d0e8',\n              '#b2d9ec', '#baddee', '#c2e1f0', '#eff7fb', '#f9e8e8',\n              '#f9e8e8', '#f9d4d4', '#f9bdbd', '#f8a8a8', '#f68f8f',\n              '#f47676', '#f45f5f', '#f34343', '#f33b3b', '#f42e2e']\n    value = int((value * 100) / 5)\n    # Bound to valid index range\n    value = min(max(value, 0), len(colors)-1)\n    return colors[value]\n\ndef sigmoid(x):\n    return 1/(1 + np.exp(-x))\n\n# def visualize(input_seq, attention_matrix):\n#     \"\"\"Visualize attention weights by coloring characters\"\"\"\n#     # Get the number of output characters in the attention matrix\n#     output_len = attention_matrix.shape[0]\n    \n#     # For each output character (except the last if it's EOS)\n#     for i in range(output_len):\n#         text_colours = []\n#         # For each input character, color based on attention weight\n#         for j in range(len(input_seq)):\n#             # Apply sigmoid to normalize weights (similar to the original)\n#             weight = sigmoid(attention_matrix[i][j] * 5)  # Scale for better color contrast\n#             text = (input_seq[j], get_clr(weight))\n#             text_colours.append(text)\n#         # Print the colored input sequence for this output step\n#         print_color(text_colours)\ndef visualize(input_seq, output_chars, attention_matrix):\n    \"\"\"Visualize attention weights with Devanagari output labels\"\"\"\n    # Get the number of output characters in the attention matrix\n    output_len = len(output_chars)\n    \n    # For each output character (except the last if it's EOS)\n    for i in range(output_len):\n        text_colours = []\n        \n        # Add Devanagari output character at start of line\n        devanagari_char = output_chars[i]\n        text_colours.append((f\"{devanagari_char}: \", 'white'))  # White background for label\n        \n        # For each input character, color based on attention weight\n        for j in range(len(input_seq)):\n            # Apply sigmoid to normalize weights\n            weight = sigmoid(attention_matrix[i][j] * 5)  # Scale for better color contrast\n            text = (input_seq[j], get_clr(weight))\n            text_colours.append(text)\n            \n        # Print the line with output char + attention weights\n        print_color(text_colours)\ndef visualize_attention(model, src_vocab, tgt_vocab, input_str, max_len=20):\n    model.eval()\n    \n    # Create reverse mapping\n    devanagari_index_to_token = {v: k for k, v in tgt_vocab.items()}\n    \n    input_indices = [src_vocab.get(char, src_vocab['<unk>']) for char in input_str]\n    input_tensor = torch.tensor([input_indices]).to(model.device)\n    \n    with torch.no_grad():\n        encoder_outputs, hidden = model.encoder(input_tensor)\n        attentions = []\n        output_indices = [tgt_vocab['<sos>']]\n        output_chars = []\n\n        for _ in range(max_len):\n            decoder_input = torch.tensor([output_indices[-1]]).to(model.device)\n            output, hidden, attn_weights = model.decoder(\n                decoder_input, hidden, encoder_outputs\n            )\n            attentions.append(attn_weights.squeeze().cpu().numpy())\n            \n            top_idx = output.argmax().item()\n            if top_idx == tgt_vocab['<eos>']:\n                break\n                \n            # Use reverse mapping with fallback\n            output_chars.append(devanagari_index_to_token.get(top_idx, '<?>'))  # Fixed here\n            output_indices.append(top_idx)\n    attention_matrix = np.array(attentions)\n    attention_matrix = attention_matrix[:len(output_chars), :len(input_str)]\n    attention_matrix = torch.softmax(torch.tensor(attention_matrix), dim=1).numpy()\n\n    print(f\"Input: {input_str}\")\n    print(f\"Predicted Output: {''.join(output_chars)}\")\n    print(\"â”€\" * 60)\n    \n    # Pass output_chars to visualize\n    visualize(input_str, output_chars, attention_matrix)\n\n# 1. Load test words from your dataset\ntest_words = []\ntest_path = '/kaggle/input/dakshina-dataset/dakshina_dataset_v1.0/ta/lexicons/ta.translit.sampled.test.tsv'\n\nwith open(test_path, 'r', encoding='utf-8') as f:\n    next(f)  # skip header\n    for line in f:\n        parts = line.strip().split('\\t')\n        if len(parts) >= 2:\n            test_words.append(parts[1])  # Latin script words\n\n# Visualize multiple random examples\nnp.random.seed(11)\ntest_samples = np.random.choice(test_words, 9)\n\n# for word in test_samples:\n#     # print(f\"Input: {word}\")\n#     # print(\"â”€\" * 40)\n#     visualize_attention(\n#         model,\n#         latin_vocab,\n#         devanagari_vocab,\n#         input_str=word,\n#         max_len=20\n#     )\n    \n\n\ndef visualize_attention_html_only(model, src_vocab, tgt_vocab, input_str, delay=1.0, max_len=20):\n    \"\"\"\n    HTML-only attention visualization with step-by-step character attention (no heatmap).\n    \"\"\"\n    model.eval()\n    \n    # Reverse vocab lookup\n    target_index_to_token = {v: k for k, v in tgt_vocab.items()}\n    \n    input_indices = [src_vocab.get(char, src_vocab['<unk>']) for char in input_str]\n    input_tensor = torch.tensor([input_indices]).to(model.device)\n    \n    with torch.no_grad():\n        encoder_outputs, hidden = model.encoder(input_tensor)\n        attentions = []\n        output_indices = [tgt_vocab['<sos>']]\n        output_chars = []\n\n        # Step-by-step generation\n        for t in range(max_len):\n            decoder_input = torch.tensor([output_indices[-1]]).to(model.device)\n            output, hidden, attn_weights = model.decoder(decoder_input, hidden, encoder_outputs)\n            \n            top_idx = output.argmax().item()\n            if top_idx == tgt_vocab['<eos>']:\n                break\n                \n            curr_char = target_index_to_token.get(top_idx, '<?>')\n            output_chars.append(curr_char)\n            output_indices.append(top_idx)\n            \n            attention = attn_weights.squeeze().cpu().numpy()\n            norm_attention = torch.softmax(torch.tensor(attention), dim=0).numpy()\n            attentions.append(norm_attention)\n            \n            # Show current character attention only (no heatmap)\n            clear_output(wait=True)\n            html_parts = []\n            html_parts.append(f\"<h3>Input: {input_str}</h3>\")\n            html_parts.append(f\"<h4>Current output: {''.join(output_chars)}</h4>\")\n            html_parts.append(f\"<h4>Currently generating: <span style='color: red;'>{curr_char}</span></h4>\")\n            html_parts.append(\"<div style='margin: 10px 0; font-size: 1.2em;'>\")\n            \n            for j, char in enumerate(input_str):\n                weight = norm_attention[j]\n                html_parts.append(cstr(char, get_clr(weight)))\n                \n            html_parts.append(\"</div><hr/>\")\n            display(HTML(''.join(html_parts)))\n            time.sleep(delay)\n        \n        # Final display\n        clear_output(wait=True)\n        html_parts = []\n        html_parts.append(f\"<h3>Final Transliteration:</h3>\")\n        html_parts.append(f\"<h4>Latin (input): {input_str}</h4>\")\n        html_parts.append(f\"<h4 style='font-family: \\\"DejaVu Sans\\\", Arial, sans-serif;'>Target script (output): {''.join(output_chars)}</h4>\")\n        html_parts.append(\"<hr/>\")\n        \n        html_parts.append(\"<h3>Character-by-character attention:</h3>\")\n        for i, (char, attn) in enumerate(zip(output_chars, attentions)):\n            html_parts.append(f\"<div style='margin: 10px 0; font-family: \\\"DejaVu Sans\\\", Arial, sans-serif;'><b>{char}</b> â†’ \")\n            for j, in_char in enumerate(input_str):\n                weight = attn[j]\n                html_parts.append(cstr(in_char, get_clr(weight)))\n            html_parts.append(\"</div>\")\n        \n        display(HTML(''.join(html_parts)))\n\n\n\n\n# visualize_attention_html_only(model, latin_vocab, devanagari_vocab, \"arivippukalai\", delay=1.0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T17:38:28.592985Z","iopub.execute_input":"2025-05-21T17:38:28.593265Z","iopub.status.idle":"2025-05-21T17:41:08.323135Z","shell.execute_reply.started":"2025-05-21T17:38:28.593244Z","shell.execute_reply":"2025-05-21T17:41:08.322362Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"name":"stdout","text":"Using device: cuda\nDataset from /kaggle/input/dakshina-dataset/dakshina_dataset_v1.0/ta/lexicons/ta.translit.sampled.train.tsv: 38716 pairs.\nDataset from /kaggle/input/dakshina-dataset/dakshina_dataset_v1.0/ta/lexicons/ta.translit.sampled.dev.tsv: 4054 pairs.\nDataset from /kaggle/input/dakshina-dataset/dakshina_dataset_v1.0/ta/lexicons/ta.translit.sampled.test.tsv: 3938 pairs.\n\nEpoch 1/5\nTrain Loss: 1.2500, Accuracy: 16.79%\nVal Loss: 0.6729, Accuracy: 11.12%\nSaved new best model\n\nEpoch 2/5\nTrain Loss: 0.4964, Accuracy: 30.83%\nVal Loss: 0.5030, Accuracy: 34.85%\nSaved new best model\n\nEpoch 3/5\nTrain Loss: 0.3952, Accuracy: 30.99%\nVal Loss: 0.4685, Accuracy: 22.13%\n\nEpoch 4/5\nTrain Loss: 0.3489, Accuracy: 31.59%\nVal Loss: 0.4837, Accuracy: 32.86%\n\nEpoch 5/5\nTrain Loss: 0.3205, Accuracy: 34.00%\nVal Loss: 0.4694, Accuracy: 34.73%\n\nFinal Test Results -> Loss: 0.5135, Accuracy: 33.90%\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"!apt-get install -y wkhtmltopdf\n!pip install imgkit","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T17:41:13.664014Z","iopub.execute_input":"2025-05-21T17:41:13.664522Z","iopub.status.idle":"2025-05-21T17:41:45.797433Z","shell.execute_reply.started":"2025-05-21T17:41:13.664500Z","shell.execute_reply":"2025-05-21T17:41:45.796637Z"}},"outputs":[{"name":"stdout","text":"Reading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\nThe following additional packages will be installed:\n  avahi-daemon geoclue-2.0 glib-networking glib-networking-common glib-networking-services\n  gsettings-desktop-schemas iio-sensor-proxy libavahi-core7 libavahi-glib1 libdaemon0 libevdev2\n  libgudev-1.0-0 libhyphen0 libinput-bin libinput10 libjson-glib-1.0-0 libjson-glib-1.0-common\n  libmbim-glib4 libmbim-proxy libmd4c0 libmm-glib0 libmtdev1 libnl-genl-3-200 libnotify4\n  libnss-mdns libproxy1v5 libqmi-glib5 libqmi-proxy libqt5core5a libqt5dbus5 libqt5gui5\n  libqt5network5 libqt5positioning5 libqt5printsupport5 libqt5qml5 libqt5qmlmodels5 libqt5quick5\n  libqt5sensors5 libqt5svg5 libqt5webchannel5 libqt5webkit5 libqt5widgets5 libsoup2.4-1\n  libsoup2.4-common libudev1 libwacom-bin libwacom-common libwacom9 libwoff1 libxcb-icccm4\n  libxcb-image0 libxcb-keysyms1 libxcb-render-util0 libxcb-util1 libxcb-xinerama0 libxcb-xinput0\n  libxcb-xkb1 libxkbcommon-x11-0 modemmanager qt5-gtk-platformtheme qttranslations5-l10n\n  session-migration systemd-hwe-hwdb udev usb-modeswitch usb-modeswitch-data wpasupplicant\nSuggested packages:\n  avahi-autoipd gnome-shell | notification-daemon avahi-autoipd | zeroconf\n  qt5-image-formats-plugins qtwayland5 qt5-qmltooling-plugins comgt wvdial wpagui\n  libengine-pkcs11-openssl\nThe following NEW packages will be installed:\n  avahi-daemon geoclue-2.0 glib-networking glib-networking-common glib-networking-services\n  gsettings-desktop-schemas iio-sensor-proxy libavahi-core7 libavahi-glib1 libdaemon0 libevdev2\n  libgudev-1.0-0 libhyphen0 libinput-bin libinput10 libjson-glib-1.0-0 libjson-glib-1.0-common\n  libmbim-glib4 libmbim-proxy libmd4c0 libmm-glib0 libmtdev1 libnl-genl-3-200 libnotify4\n  libnss-mdns libproxy1v5 libqmi-glib5 libqmi-proxy libqt5core5a libqt5dbus5 libqt5gui5\n  libqt5network5 libqt5positioning5 libqt5printsupport5 libqt5qml5 libqt5qmlmodels5 libqt5quick5\n  libqt5sensors5 libqt5svg5 libqt5webchannel5 libqt5webkit5 libqt5widgets5 libsoup2.4-1\n  libsoup2.4-common libwacom-bin libwacom-common libwacom9 libwoff1 libxcb-icccm4 libxcb-image0\n  libxcb-keysyms1 libxcb-render-util0 libxcb-util1 libxcb-xinerama0 libxcb-xinput0 libxcb-xkb1\n  libxkbcommon-x11-0 modemmanager qt5-gtk-platformtheme qttranslations5-l10n session-migration\n  systemd-hwe-hwdb udev usb-modeswitch usb-modeswitch-data wkhtmltopdf wpasupplicant\nThe following packages will be upgraded:\n  libudev1\n1 upgraded, 67 newly installed, 0 to remove and 86 not upgraded.\nNeed to get 35.5 MB of archives.\nAfter this operation, 141 MB of additional disk space will be used.\nGet:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libavahi-core7 amd64 0.8-5ubuntu5.2 [90.8 kB]\nGet:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 libdaemon0 amd64 0.14-7.1ubuntu3 [14.1 kB]\nGet:3 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 avahi-daemon amd64 0.8-5ubuntu5.2 [69.7 kB]\nGet:4 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libqt5core5a amd64 5.15.3+dfsg-2ubuntu0.2 [2,006 kB]\nGet:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 libevdev2 amd64 1.12.1+dfsg-1 [39.5 kB]\nGet:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 libmtdev1 amd64 1.1.6-1build4 [14.5 kB]\nGet:7 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libudev1 amd64 249.11-0ubuntu3.15 [76.6 kB]\nGet:8 http://archive.ubuntu.com/ubuntu jammy/main amd64 libgudev-1.0-0 amd64 1:237-2build1 [16.3 kB]\nGet:9 http://archive.ubuntu.com/ubuntu jammy/main amd64 libwacom-common all 2.2.0-1 [54.3 kB]\nGet:10 http://archive.ubuntu.com/ubuntu jammy/main amd64 libwacom9 amd64 2.2.0-1 [22.0 kB]\nGet:11 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libinput-bin amd64 1.20.0-1ubuntu0.3 [19.9 kB]\nGet:12 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libinput10 amd64 1.20.0-1ubuntu0.3 [131 kB]\nGet:13 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libmd4c0 amd64 0.4.8-1 [42.0 kB]\nGet:14 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libqt5dbus5 amd64 5.15.3+dfsg-2ubuntu0.2 [222 kB]\nGet:15 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libqt5network5 amd64 5.15.3+dfsg-2ubuntu0.2 [731 kB]\nGet:16 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-icccm4 amd64 0.4.1-1.1build2 [11.5 kB]\nGet:17 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-util1 amd64 0.4.0-1build2 [11.4 kB]\nGet:18 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-image0 amd64 0.4.0-2 [11.5 kB]\nGet:19 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-keysyms1 amd64 0.4.0-1build3 [8,746 B]\nGet:20 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-render-util0 amd64 0.3.9-1build3 [10.3 kB]\nGet:21 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-xinerama0 amd64 1.14-3ubuntu3 [5,414 B]\nGet:22 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-xinput0 amd64 1.14-3ubuntu3 [34.3 kB]\nGet:23 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-xkb1 amd64 1.14-3ubuntu3 [32.8 kB]\nGet:24 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxkbcommon-x11-0 amd64 1.4.0-1 [14.4 kB]\nGet:25 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libqt5gui5 amd64 5.15.3+dfsg-2ubuntu0.2 [3,722 kB]\nGet:26 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libqt5widgets5 amd64 5.15.3+dfsg-2ubuntu0.2 [2,561 kB]\nGet:27 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libqt5svg5 amd64 5.15.3-1 [149 kB]\nGet:28 http://archive.ubuntu.com/ubuntu jammy/main amd64 libhyphen0 amd64 2.8.8-7build2 [28.2 kB]\nGet:29 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libqt5positioning5 amd64 5.15.3+dfsg-3 [223 kB]\nGet:30 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libqt5printsupport5 amd64 5.15.3+dfsg-2ubuntu0.2 [214 kB]\nGet:31 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libqt5qml5 amd64 5.15.3+dfsg-1 [1,472 kB]\nGet:32 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libqt5qmlmodels5 amd64 5.15.3+dfsg-1 [205 kB]\nGet:33 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libqt5quick5 amd64 5.15.3+dfsg-1 [1,748 kB]\nGet:34 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libqt5sensors5 amd64 5.15.3-1 [123 kB]\nGet:35 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libqt5webchannel5 amd64 5.15.3-1 [62.9 kB]\nGet:36 http://archive.ubuntu.com/ubuntu jammy/main amd64 libwoff1 amd64 1.0.2-1build4 [45.2 kB]\nGet:37 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libqt5webkit5 amd64 5.212.0~alpha4-15ubuntu1 [12.8 MB]\nGet:38 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 udev amd64 249.11-0ubuntu3.15 [1,557 kB]\nGet:39 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libavahi-glib1 amd64 0.8-5ubuntu5.2 [8,296 B]\nGet:40 http://archive.ubuntu.com/ubuntu jammy/main amd64 libjson-glib-1.0-common all 1.6.6-1build1 [4,432 B]\nGet:41 http://archive.ubuntu.com/ubuntu jammy/main amd64 libjson-glib-1.0-0 amd64 1.6.6-1build1 [69.9 kB]\nGet:42 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libmm-glib0 amd64 1.20.0-1~ubuntu22.04.4 [262 kB]\nGet:43 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libnotify4 amd64 0.7.9-3ubuntu5.22.04.1 [20.3 kB]\nGet:44 http://archive.ubuntu.com/ubuntu jammy/main amd64 libproxy1v5 amd64 0.4.17-2 [51.9 kB]\nGet:45 http://archive.ubuntu.com/ubuntu jammy/main amd64 glib-networking-common all 2.72.0-1 [3,718 B]\nGet:46 http://archive.ubuntu.com/ubuntu jammy/main amd64 glib-networking-services amd64 2.72.0-1 [9,982 B]\nGet:47 http://archive.ubuntu.com/ubuntu jammy/main amd64 session-migration amd64 0.3.6 [9,774 B]\nGet:48 http://archive.ubuntu.com/ubuntu jammy/main amd64 gsettings-desktop-schemas all 42.0-1ubuntu1 [31.1 kB]\nGet:49 http://archive.ubuntu.com/ubuntu jammy/main amd64 glib-networking amd64 2.72.0-1 [69.8 kB]\nGet:50 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libsoup2.4-common all 2.74.2-3ubuntu0.4 [4,658 B]\nGet:51 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libsoup2.4-1 amd64 2.74.2-3ubuntu0.4 [287 kB]\nGet:52 http://archive.ubuntu.com/ubuntu jammy/main amd64 geoclue-2.0 amd64 2.5.7-3ubuntu3 [111 kB]\nGet:53 http://archive.ubuntu.com/ubuntu jammy/main amd64 iio-sensor-proxy amd64 3.3-0ubuntu6 [34.4 kB]\nGet:54 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libmbim-glib4 amd64 1.28.0-1~ubuntu20.04.2 [192 kB]\nGet:55 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libmbim-proxy amd64 1.28.0-1~ubuntu20.04.2 [6,160 B]\nGet:56 http://archive.ubuntu.com/ubuntu jammy/main amd64 libnl-genl-3-200 amd64 3.5.0-0.1 [12.4 kB]\nGet:57 http://archive.ubuntu.com/ubuntu jammy/main amd64 libnss-mdns amd64 0.15.1-1ubuntu1 [27.0 kB]\nGet:58 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libqmi-glib5 amd64 1.32.0-1ubuntu0.22.04.1 [772 kB]\nGet:59 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libqmi-proxy amd64 1.32.0-1ubuntu0.22.04.1 [6,072 B]\nGet:60 http://archive.ubuntu.com/ubuntu jammy/main amd64 libwacom-bin amd64 2.2.0-1 [13.6 kB]\nGet:61 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 modemmanager amd64 1.20.0-1~ubuntu22.04.4 [1,094 kB]\nGet:62 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 qt5-gtk-platformtheme amd64 5.15.3+dfsg-2ubuntu0.2 [130 kB]\nGet:63 http://archive.ubuntu.com/ubuntu jammy/universe amd64 qttranslations5-l10n all 5.15.3-1 [1,983 kB]\nGet:64 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 systemd-hwe-hwdb all 249.11.5 [3,228 B]\nGet:65 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 wpasupplicant amd64 2:2.10-6ubuntu2.2 [1,482 kB]\nGet:66 http://archive.ubuntu.com/ubuntu jammy/main amd64 usb-modeswitch-data all 20191128-4 [33.2 kB]\nGet:67 http://archive.ubuntu.com/ubuntu jammy/main amd64 usb-modeswitch amd64 2.6.1-3ubuntu2 [46.0 kB]\nGet:68 http://archive.ubuntu.com/ubuntu jammy/universe amd64 wkhtmltopdf amd64 0.12.6-2 [173 kB]\nFetched 35.5 MB in 5s (7,030 kB/s)     \nExtracting templates from packages: 100%\nSelecting previously unselected package libavahi-core7:amd64.\n(Reading database ... 129184 files and directories currently installed.)\nPreparing to unpack .../0-libavahi-core7_0.8-5ubuntu5.2_amd64.deb ...\nUnpacking libavahi-core7:amd64 (0.8-5ubuntu5.2) ...\nSelecting previously unselected package libdaemon0:amd64.\nPreparing to unpack .../1-libdaemon0_0.14-7.1ubuntu3_amd64.deb ...\nUnpacking libdaemon0:amd64 (0.14-7.1ubuntu3) ...\nSelecting previously unselected package avahi-daemon.\nPreparing to unpack .../2-avahi-daemon_0.8-5ubuntu5.2_amd64.deb ...\nUnpacking avahi-daemon (0.8-5ubuntu5.2) ...\nSelecting previously unselected package libqt5core5a:amd64.\nPreparing to unpack .../3-libqt5core5a_5.15.3+dfsg-2ubuntu0.2_amd64.deb ...\nUnpacking libqt5core5a:amd64 (5.15.3+dfsg-2ubuntu0.2) ...\nSelecting previously unselected package libevdev2:amd64.\nPreparing to unpack .../4-libevdev2_1.12.1+dfsg-1_amd64.deb ...\nUnpacking libevdev2:amd64 (1.12.1+dfsg-1) ...\nSelecting previously unselected package libmtdev1:amd64.\nPreparing to unpack .../5-libmtdev1_1.1.6-1build4_amd64.deb ...\nUnpacking libmtdev1:amd64 (1.1.6-1build4) ...\nPreparing to unpack .../6-libudev1_249.11-0ubuntu3.15_amd64.deb ...\nUnpacking libudev1:amd64 (249.11-0ubuntu3.15) over (249.11-0ubuntu3.12) ...\nSetting up libudev1:amd64 (249.11-0ubuntu3.15) ...\nSelecting previously unselected package libgudev-1.0-0:amd64.\n(Reading database ... 129253 files and directories currently installed.)\nPreparing to unpack .../00-libgudev-1.0-0_1%3a237-2build1_amd64.deb ...\nUnpacking libgudev-1.0-0:amd64 (1:237-2build1) ...\nSelecting previously unselected package libwacom-common.\nPreparing to unpack .../01-libwacom-common_2.2.0-1_all.deb ...\nUnpacking libwacom-common (2.2.0-1) ...\nSelecting previously unselected package libwacom9:amd64.\nPreparing to unpack .../02-libwacom9_2.2.0-1_amd64.deb ...\nUnpacking libwacom9:amd64 (2.2.0-1) ...\nSelecting previously unselected package libinput-bin.\nPreparing to unpack .../03-libinput-bin_1.20.0-1ubuntu0.3_amd64.deb ...\nUnpacking libinput-bin (1.20.0-1ubuntu0.3) ...\nSelecting previously unselected package libinput10:amd64.\nPreparing to unpack .../04-libinput10_1.20.0-1ubuntu0.3_amd64.deb ...\nUnpacking libinput10:amd64 (1.20.0-1ubuntu0.3) ...\nSelecting previously unselected package libmd4c0:amd64.\nPreparing to unpack .../05-libmd4c0_0.4.8-1_amd64.deb ...\nUnpacking libmd4c0:amd64 (0.4.8-1) ...\nSelecting previously unselected package libqt5dbus5:amd64.\nPreparing to unpack .../06-libqt5dbus5_5.15.3+dfsg-2ubuntu0.2_amd64.deb ...\nUnpacking libqt5dbus5:amd64 (5.15.3+dfsg-2ubuntu0.2) ...\nSelecting previously unselected package libqt5network5:amd64.\nPreparing to unpack .../07-libqt5network5_5.15.3+dfsg-2ubuntu0.2_amd64.deb ...\nUnpacking libqt5network5:amd64 (5.15.3+dfsg-2ubuntu0.2) ...\nSelecting previously unselected package libxcb-icccm4:amd64.\nPreparing to unpack .../08-libxcb-icccm4_0.4.1-1.1build2_amd64.deb ...\nUnpacking libxcb-icccm4:amd64 (0.4.1-1.1build2) ...\nSelecting previously unselected package libxcb-util1:amd64.\nPreparing to unpack .../09-libxcb-util1_0.4.0-1build2_amd64.deb ...\nUnpacking libxcb-util1:amd64 (0.4.0-1build2) ...\nSelecting previously unselected package libxcb-image0:amd64.\nPreparing to unpack .../10-libxcb-image0_0.4.0-2_amd64.deb ...\nUnpacking libxcb-image0:amd64 (0.4.0-2) ...\nSelecting previously unselected package libxcb-keysyms1:amd64.\nPreparing to unpack .../11-libxcb-keysyms1_0.4.0-1build3_amd64.deb ...\nUnpacking libxcb-keysyms1:amd64 (0.4.0-1build3) ...\nSelecting previously unselected package libxcb-render-util0:amd64.\nPreparing to unpack .../12-libxcb-render-util0_0.3.9-1build3_amd64.deb ...\nUnpacking libxcb-render-util0:amd64 (0.3.9-1build3) ...\nSelecting previously unselected package libxcb-xinerama0:amd64.\nPreparing to unpack .../13-libxcb-xinerama0_1.14-3ubuntu3_amd64.deb ...\nUnpacking libxcb-xinerama0:amd64 (1.14-3ubuntu3) ...\nSelecting previously unselected package libxcb-xinput0:amd64.\nPreparing to unpack .../14-libxcb-xinput0_1.14-3ubuntu3_amd64.deb ...\nUnpacking libxcb-xinput0:amd64 (1.14-3ubuntu3) ...\nSelecting previously unselected package libxcb-xkb1:amd64.\nPreparing to unpack .../15-libxcb-xkb1_1.14-3ubuntu3_amd64.deb ...\nUnpacking libxcb-xkb1:amd64 (1.14-3ubuntu3) ...\nSelecting previously unselected package libxkbcommon-x11-0:amd64.\nPreparing to unpack .../16-libxkbcommon-x11-0_1.4.0-1_amd64.deb ...\nUnpacking libxkbcommon-x11-0:amd64 (1.4.0-1) ...\nSelecting previously unselected package libqt5gui5:amd64.\nPreparing to unpack .../17-libqt5gui5_5.15.3+dfsg-2ubuntu0.2_amd64.deb ...\nUnpacking libqt5gui5:amd64 (5.15.3+dfsg-2ubuntu0.2) ...\nSelecting previously unselected package libqt5widgets5:amd64.\nPreparing to unpack .../18-libqt5widgets5_5.15.3+dfsg-2ubuntu0.2_amd64.deb ...\nUnpacking libqt5widgets5:amd64 (5.15.3+dfsg-2ubuntu0.2) ...\nSelecting previously unselected package libqt5svg5:amd64.\nPreparing to unpack .../19-libqt5svg5_5.15.3-1_amd64.deb ...\nUnpacking libqt5svg5:amd64 (5.15.3-1) ...\nSelecting previously unselected package libhyphen0:amd64.\nPreparing to unpack .../20-libhyphen0_2.8.8-7build2_amd64.deb ...\nUnpacking libhyphen0:amd64 (2.8.8-7build2) ...\nSelecting previously unselected package libqt5positioning5:amd64.\nPreparing to unpack .../21-libqt5positioning5_5.15.3+dfsg-3_amd64.deb ...\nUnpacking libqt5positioning5:amd64 (5.15.3+dfsg-3) ...\nSelecting previously unselected package libqt5printsupport5:amd64.\nPreparing to unpack .../22-libqt5printsupport5_5.15.3+dfsg-2ubuntu0.2_amd64.deb ...\nUnpacking libqt5printsupport5:amd64 (5.15.3+dfsg-2ubuntu0.2) ...\nSelecting previously unselected package libqt5qml5:amd64.\nPreparing to unpack .../23-libqt5qml5_5.15.3+dfsg-1_amd64.deb ...\nUnpacking libqt5qml5:amd64 (5.15.3+dfsg-1) ...\nSelecting previously unselected package libqt5qmlmodels5:amd64.\nPreparing to unpack .../24-libqt5qmlmodels5_5.15.3+dfsg-1_amd64.deb ...\nUnpacking libqt5qmlmodels5:amd64 (5.15.3+dfsg-1) ...\nSelecting previously unselected package libqt5quick5:amd64.\nPreparing to unpack .../25-libqt5quick5_5.15.3+dfsg-1_amd64.deb ...\nUnpacking libqt5quick5:amd64 (5.15.3+dfsg-1) ...\nSelecting previously unselected package libqt5sensors5:amd64.\nPreparing to unpack .../26-libqt5sensors5_5.15.3-1_amd64.deb ...\nUnpacking libqt5sensors5:amd64 (5.15.3-1) ...\nSelecting previously unselected package libqt5webchannel5:amd64.\nPreparing to unpack .../27-libqt5webchannel5_5.15.3-1_amd64.deb ...\nUnpacking libqt5webchannel5:amd64 (5.15.3-1) ...\nSelecting previously unselected package libwoff1:amd64.\nPreparing to unpack .../28-libwoff1_1.0.2-1build4_amd64.deb ...\nUnpacking libwoff1:amd64 (1.0.2-1build4) ...\nSelecting previously unselected package libqt5webkit5:amd64.\nPreparing to unpack .../29-libqt5webkit5_5.212.0~alpha4-15ubuntu1_amd64.deb ...\nUnpacking libqt5webkit5:amd64 (5.212.0~alpha4-15ubuntu1) ...\nSelecting previously unselected package udev.\nPreparing to unpack .../30-udev_249.11-0ubuntu3.15_amd64.deb ...\nUnpacking udev (249.11-0ubuntu3.15) ...\nSelecting previously unselected package libavahi-glib1:amd64.\nPreparing to unpack .../31-libavahi-glib1_0.8-5ubuntu5.2_amd64.deb ...\nUnpacking libavahi-glib1:amd64 (0.8-5ubuntu5.2) ...\nSelecting previously unselected package libjson-glib-1.0-common.\nPreparing to unpack .../32-libjson-glib-1.0-common_1.6.6-1build1_all.deb ...\nUnpacking libjson-glib-1.0-common (1.6.6-1build1) ...\nSelecting previously unselected package libjson-glib-1.0-0:amd64.\nPreparing to unpack .../33-libjson-glib-1.0-0_1.6.6-1build1_amd64.deb ...\nUnpacking libjson-glib-1.0-0:amd64 (1.6.6-1build1) ...\nSelecting previously unselected package libmm-glib0:amd64.\nPreparing to unpack .../34-libmm-glib0_1.20.0-1~ubuntu22.04.4_amd64.deb ...\nUnpacking libmm-glib0:amd64 (1.20.0-1~ubuntu22.04.4) ...\nSelecting previously unselected package libnotify4:amd64.\nPreparing to unpack .../35-libnotify4_0.7.9-3ubuntu5.22.04.1_amd64.deb ...\nUnpacking libnotify4:amd64 (0.7.9-3ubuntu5.22.04.1) ...\nSelecting previously unselected package libproxy1v5:amd64.\nPreparing to unpack .../36-libproxy1v5_0.4.17-2_amd64.deb ...\nUnpacking libproxy1v5:amd64 (0.4.17-2) ...\nSelecting previously unselected package glib-networking-common.\nPreparing to unpack .../37-glib-networking-common_2.72.0-1_all.deb ...\nUnpacking glib-networking-common (2.72.0-1) ...\nSelecting previously unselected package glib-networking-services.\nPreparing to unpack .../38-glib-networking-services_2.72.0-1_amd64.deb ...\nUnpacking glib-networking-services (2.72.0-1) ...\nSelecting previously unselected package session-migration.\nPreparing to unpack .../39-session-migration_0.3.6_amd64.deb ...\nUnpacking session-migration (0.3.6) ...\nSelecting previously unselected package gsettings-desktop-schemas.\nPreparing to unpack .../40-gsettings-desktop-schemas_42.0-1ubuntu1_all.deb ...\nUnpacking gsettings-desktop-schemas (42.0-1ubuntu1) ...\nSelecting previously unselected package glib-networking:amd64.\nPreparing to unpack .../41-glib-networking_2.72.0-1_amd64.deb ...\nUnpacking glib-networking:amd64 (2.72.0-1) ...\nSelecting previously unselected package libsoup2.4-common.\nPreparing to unpack .../42-libsoup2.4-common_2.74.2-3ubuntu0.4_all.deb ...\nUnpacking libsoup2.4-common (2.74.2-3ubuntu0.4) ...\nSelecting previously unselected package libsoup2.4-1:amd64.\nPreparing to unpack .../43-libsoup2.4-1_2.74.2-3ubuntu0.4_amd64.deb ...\nUnpacking libsoup2.4-1:amd64 (2.74.2-3ubuntu0.4) ...\nSelecting previously unselected package geoclue-2.0.\nPreparing to unpack .../44-geoclue-2.0_2.5.7-3ubuntu3_amd64.deb ...\nUnpacking geoclue-2.0 (2.5.7-3ubuntu3) ...\nSelecting previously unselected package iio-sensor-proxy.\nPreparing to unpack .../45-iio-sensor-proxy_3.3-0ubuntu6_amd64.deb ...\nUnpacking iio-sensor-proxy (3.3-0ubuntu6) ...\nSelecting previously unselected package libmbim-glib4:amd64.\nPreparing to unpack .../46-libmbim-glib4_1.28.0-1~ubuntu20.04.2_amd64.deb ...\nUnpacking libmbim-glib4:amd64 (1.28.0-1~ubuntu20.04.2) ...\nSelecting previously unselected package libmbim-proxy.\nPreparing to unpack .../47-libmbim-proxy_1.28.0-1~ubuntu20.04.2_amd64.deb ...\nUnpacking libmbim-proxy (1.28.0-1~ubuntu20.04.2) ...\nSelecting previously unselected package libnl-genl-3-200:amd64.\nPreparing to unpack .../48-libnl-genl-3-200_3.5.0-0.1_amd64.deb ...\nUnpacking libnl-genl-3-200:amd64 (3.5.0-0.1) ...\nSelecting previously unselected package libnss-mdns:amd64.\nPreparing to unpack .../49-libnss-mdns_0.15.1-1ubuntu1_amd64.deb ...\nUnpacking libnss-mdns:amd64 (0.15.1-1ubuntu1) ...\nSelecting previously unselected package libqmi-glib5:amd64.\nPreparing to unpack .../50-libqmi-glib5_1.32.0-1ubuntu0.22.04.1_amd64.deb ...\nUnpacking libqmi-glib5:amd64 (1.32.0-1ubuntu0.22.04.1) ...\nSelecting previously unselected package libqmi-proxy.\nPreparing to unpack .../51-libqmi-proxy_1.32.0-1ubuntu0.22.04.1_amd64.deb ...\nUnpacking libqmi-proxy (1.32.0-1ubuntu0.22.04.1) ...\nSelecting previously unselected package libwacom-bin.\nPreparing to unpack .../52-libwacom-bin_2.2.0-1_amd64.deb ...\nUnpacking libwacom-bin (2.2.0-1) ...\nSelecting previously unselected package modemmanager.\nPreparing to unpack .../53-modemmanager_1.20.0-1~ubuntu22.04.4_amd64.deb ...\nUnpacking modemmanager (1.20.0-1~ubuntu22.04.4) ...\nSelecting previously unselected package qt5-gtk-platformtheme:amd64.\nPreparing to unpack .../54-qt5-gtk-platformtheme_5.15.3+dfsg-2ubuntu0.2_amd64.deb ...\nUnpacking qt5-gtk-platformtheme:amd64 (5.15.3+dfsg-2ubuntu0.2) ...\nSelecting previously unselected package qttranslations5-l10n.\nPreparing to unpack .../55-qttranslations5-l10n_5.15.3-1_all.deb ...\nUnpacking qttranslations5-l10n (5.15.3-1) ...\nSelecting previously unselected package systemd-hwe-hwdb.\nPreparing to unpack .../56-systemd-hwe-hwdb_249.11.5_all.deb ...\nUnpacking systemd-hwe-hwdb (249.11.5) ...\nSelecting previously unselected package wpasupplicant.\nPreparing to unpack .../57-wpasupplicant_2%3a2.10-6ubuntu2.2_amd64.deb ...\nUnpacking wpasupplicant (2:2.10-6ubuntu2.2) ...\nSelecting previously unselected package usb-modeswitch-data.\nPreparing to unpack .../58-usb-modeswitch-data_20191128-4_all.deb ...\nUnpacking usb-modeswitch-data (20191128-4) ...\nSelecting previously unselected package usb-modeswitch.\nPreparing to unpack .../59-usb-modeswitch_2.6.1-3ubuntu2_amd64.deb ...\nUnpacking usb-modeswitch (2.6.1-3ubuntu2) ...\nSelecting previously unselected package wkhtmltopdf.\nPreparing to unpack .../60-wkhtmltopdf_0.12.6-2_amd64.deb ...\nUnpacking wkhtmltopdf (0.12.6-2) ...\nSetting up session-migration (0.3.6) ...\nCreated symlink /etc/systemd/user/graphical-session-pre.target.wants/session-migration.service â†’ /usr/lib/systemd/user/session-migration.service.\nSetting up libproxy1v5:amd64 (0.4.17-2) ...\nSetting up libxcb-xinput0:amd64 (1.14-3ubuntu3) ...\nSetting up libwoff1:amd64 (1.0.2-1build4) ...\nSetting up libhyphen0:amd64 (2.8.8-7build2) ...\nSetting up libxcb-keysyms1:amd64 (0.4.0-1build3) ...\nSetting up libxcb-render-util0:amd64 (0.3.9-1build3) ...\nSetting up libxcb-icccm4:amd64 (0.4.1-1.1build2) ...\nSetting up libxcb-util1:amd64 (0.4.0-1build2) ...\nSetting up libxcb-xkb1:amd64 (1.14-3ubuntu3) ...\nSetting up libxcb-image0:amd64 (0.4.0-2) ...\nSetting up libxcb-xinerama0:amd64 (1.14-3ubuntu3) ...\nSetting up qttranslations5-l10n (5.15.3-1) ...\nSetting up libnotify4:amd64 (0.7.9-3ubuntu5.22.04.1) ...\nSetting up libxkbcommon-x11-0:amd64 (1.4.0-1) ...\nSetting up usb-modeswitch-data (20191128-4) ...\nSetting up udev (249.11-0ubuntu3.15) ...\ninvoke-rc.d: could not determine current runlevel\ninvoke-rc.d: policy-rc.d denied execution of start.\nSetting up libqt5core5a:amd64 (5.15.3+dfsg-2ubuntu0.2) ...\nSetting up libmtdev1:amd64 (1.1.6-1build4) ...\nSetting up libsoup2.4-common (2.74.2-3ubuntu0.4) ...\nSetting up systemd-hwe-hwdb (249.11.5) ...\nSetting up libmm-glib0:amd64 (1.20.0-1~ubuntu22.04.4) ...\nSetting up libqt5dbus5:amd64 (5.15.3+dfsg-2ubuntu0.2) ...\nSetting up libnl-genl-3-200:amd64 (3.5.0-0.1) ...\nSetting up libmd4c0:amd64 (0.4.8-1) ...\nSetting up libavahi-glib1:amd64 (0.8-5ubuntu5.2) ...\nSetting up libjson-glib-1.0-common (1.6.6-1build1) ...\nSetting up usb-modeswitch (2.6.1-3ubuntu2) ...\nSetting up glib-networking-common (2.72.0-1) ...\nSetting up libqt5sensors5:amd64 (5.15.3-1) ...\nSetting up libdaemon0:amd64 (0.14-7.1ubuntu3) ...\nSetting up libavahi-core7:amd64 (0.8-5ubuntu5.2) ...\nSetting up libnss-mdns:amd64 (0.15.1-1ubuntu1) ...\nFirst installation detected...\nChecking NSS setup...\nSetting up libevdev2:amd64 (1.12.1+dfsg-1) ...\nSetting up libgudev-1.0-0:amd64 (1:237-2build1) ...\nSetting up libmbim-glib4:amd64 (1.28.0-1~ubuntu20.04.2) ...\nSetting up libwacom-common (2.2.0-1) ...\nSetting up gsettings-desktop-schemas (42.0-1ubuntu1) ...\nSetting up glib-networking-services (2.72.0-1) ...\nSetting up iio-sensor-proxy (3.3-0ubuntu6) ...\nSetting up libwacom9:amd64 (2.2.0-1) ...\nSetting up libqt5positioning5:amd64 (5.15.3+dfsg-3) ...\nSetting up libmbim-proxy (1.28.0-1~ubuntu20.04.2) ...\nSetting up libqt5network5:amd64 (5.15.3+dfsg-2ubuntu0.2) ...\nSetting up libjson-glib-1.0-0:amd64 (1.6.6-1build1) ...\nSetting up libinput-bin (1.20.0-1ubuntu0.3) ...\nSetting up wpasupplicant (2:2.10-6ubuntu2.2) ...\nCreated symlink /etc/systemd/system/dbus-fi.w1.wpa_supplicant1.service â†’ /lib/systemd/system/wpa_supplicant.service.\nCreated symlink /etc/systemd/system/multi-user.target.wants/wpa_supplicant.service â†’ /lib/systemd/system/wpa_supplicant.service.\nSetting up libqt5qml5:amd64 (5.15.3+dfsg-1) ...\nSetting up libqt5webchannel5:amd64 (5.15.3-1) ...\nSetting up libwacom-bin (2.2.0-1) ...\nSetting up avahi-daemon (0.8-5ubuntu5.2) ...\ninvoke-rc.d: could not determine current runlevel\ninvoke-rc.d: policy-rc.d denied execution of force-reload.\ninvoke-rc.d: could not determine current runlevel\ninvoke-rc.d: policy-rc.d denied execution of start.\nCreated symlink /etc/systemd/system/dbus-org.freedesktop.Avahi.service â†’ /lib/systemd/system/avahi-daemon.service.\nCreated symlink /etc/systemd/system/multi-user.target.wants/avahi-daemon.service â†’ /lib/systemd/system/avahi-daemon.service.\nCreated symlink /etc/systemd/system/sockets.target.wants/avahi-daemon.socket â†’ /lib/systemd/system/avahi-daemon.socket.\nSetting up libinput10:amd64 (1.20.0-1ubuntu0.3) ...\nSetting up libqt5qmlmodels5:amd64 (5.15.3+dfsg-1) ...\nSetting up libqt5gui5:amd64 (5.15.3+dfsg-2ubuntu0.2) ...\nSetting up libqmi-glib5:amd64 (1.32.0-1ubuntu0.22.04.1) ...\nSetting up libqt5widgets5:amd64 (5.15.3+dfsg-2ubuntu0.2) ...\nSetting up qt5-gtk-platformtheme:amd64 (5.15.3+dfsg-2ubuntu0.2) ...\nSetting up libqt5printsupport5:amd64 (5.15.3+dfsg-2ubuntu0.2) ...\nSetting up libqt5quick5:amd64 (5.15.3+dfsg-1) ...\nSetting up libqt5svg5:amd64 (5.15.3-1) ...\nSetting up libqmi-proxy (1.32.0-1ubuntu0.22.04.1) ...\nSetting up libqt5webkit5:amd64 (5.212.0~alpha4-15ubuntu1) ...\nSetting up modemmanager (1.20.0-1~ubuntu22.04.4) ...\nCreated symlink /etc/systemd/system/dbus-org.freedesktop.ModemManager1.service â†’ /lib/systemd/system/ModemManager.service.\nCreated symlink /etc/systemd/system/multi-user.target.wants/ModemManager.service â†’ /lib/systemd/system/ModemManager.service.\nSetting up wkhtmltopdf (0.12.6-2) ...\nProcessing triggers for man-db (2.10.2-1) ...\nProcessing triggers for dbus (1.12.20-2ubuntu4.1) ...\nProcessing triggers for mailcap (3.70+nmu1ubuntu1) ...\nProcessing triggers for hicolor-icon-theme (0.17-2) ...\nProcessing triggers for libglib2.0-0:amd64 (2.72.4-0ubuntu2.4) ...\nProcessing triggers for libc-bin (2.35-0ubuntu3.8) ...\n/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n\nSetting up glib-networking:amd64 (2.72.0-1) ...\nSetting up libsoup2.4-1:amd64 (2.74.2-3ubuntu0.4) ...\nSetting up geoclue-2.0 (2.5.7-3ubuntu3) ...\nProcessing triggers for libc-bin (2.35-0ubuntu3.8) ...\n/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n\nProcessing triggers for dbus (1.12.20-2ubuntu4.1) ...\nCollecting imgkit\n  Downloading imgkit-1.2.3-py3-none-any.whl.metadata (8.1 kB)\nRequirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from imgkit) (1.17.0)\nDownloading imgkit-1.2.3-py3-none-any.whl (10 kB)\nInstalling collected packages: imgkit\nSuccessfully installed imgkit-1.2.3\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"!pip install html2image","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T17:41:45.798911Z","iopub.execute_input":"2025-05-21T17:41:45.799171Z","iopub.status.idle":"2025-05-21T17:41:48.992452Z","shell.execute_reply.started":"2025-05-21T17:41:45.799147Z","shell.execute_reply":"2025-05-21T17:41:48.991635Z"}},"outputs":[{"name":"stdout","text":"Collecting html2image\n  Downloading html2image-2.0.7-py3-none-any.whl.metadata (20 kB)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from html2image) (2.32.3)\nRequirement already satisfied: websocket-client~=1.0 in /usr/local/lib/python3.11/dist-packages (from html2image) (1.8.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->html2image) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->html2image) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->html2image) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->html2image) (2025.4.26)\nDownloading html2image-2.0.7-py3-none-any.whl (31 kB)\nInstalling collected packages: html2image\nSuccessfully installed html2image-2.0.7\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"from IPython.display import display, HTML\nfrom IPython.display import clear_output\nfrom PIL import Image\nimport io\nimport time\nimport wandb\nimport torch\nimport imgkit\n\ndef cstr(s, color='black'):\n    \"\"\"Add color to a string for HTML display\"\"\"\n    return f\"<span style='background-color: {color}; padding: 2px; margin: 1px; display: inline-block; width: 30px; text-align: center;'>{s}</span>\"\n\ndef get_clr(weight):\n    \"\"\"Get color based on attention weight\"\"\"\n    import matplotlib.colors as mcolors\n    return mcolors.to_hex(plt.cm.Blues(min(weight * 1.5, 1.0)))\n\ndef html_to_image(html_str):\n    \"\"\"Convert HTML to PIL Image with proper font support\"\"\"\n    # Create complete HTML with proper font embedding\n    full_html = f\"\"\"\n    <!DOCTYPE html>\n    <html>\n    <head>\n        <meta charset=\"UTF-8\">\n        <style>\n            @import url('https://fonts.googleapis.com/css2?family=Noto+Sans+Tamil:wght@400;700&display=swap');\n            body {{\n                font-family: Arial, sans-serif;\n            }}\n            .tamil-text {{\n                font-family: 'Noto Sans Tamil', 'Arial Unicode MS', sans-serif;\n            }}\n        </style>\n    </head>\n    <body>\n        {html_str}\n    </body>\n    </html>\n    \"\"\"\n    \n    options = {\n        'format': 'png',\n        'encoding': \"UTF-8\",\n        'quiet': '',\n        'width': 800,  # Set reasonable width\n        'enable-local-file-access': None,\n    }\n\n    img_bytes = imgkit.from_string(full_html, False, options=options)\n    return Image.open(io.BytesIO(img_bytes))\n\ndef visualize_attention_html_only(model, src_vocab, tgt_vocab, input_str, delay=1.0, max_len=20, gif_path=\"attention.gif\"):\n    import matplotlib.pyplot as plt  # Add this import\n    model.eval()\n    \n    target_index_to_token = {v: k for k, v in tgt_vocab.items()}\n    input_indices = [src_vocab.get(char, src_vocab['<unk>']) for char in input_str]\n    input_tensor = torch.tensor([input_indices]).to(model.device)\n\n    images = []  # To store all frames as images\n\n    with torch.no_grad():\n        encoder_outputs, hidden = model.encoder(input_tensor)\n        attentions = []\n        output_indices = [tgt_vocab['<sos>']]\n        output_chars = []\n\n        for t in range(max_len):\n            decoder_input = torch.tensor([output_indices[-1]]).to(model.device)\n            output, hidden, attn_weights = model.decoder(decoder_input, hidden, encoder_outputs)\n            \n            top_idx = output.argmax().item()\n            if top_idx == tgt_vocab['<eos>']:\n                break\n            \n            curr_char = target_index_to_token.get(top_idx, '<?>')\n            output_chars.append(curr_char)\n            output_indices.append(top_idx)\n            \n            attention = attn_weights.squeeze().cpu().numpy()\n            norm_attention = torch.softmax(torch.tensor(attention), dim=0).numpy()\n            attentions.append(norm_attention)\n            \n            html_parts = []\n            html_parts.append(f\"<h3>Input: {input_str}</h3>\")\n            html_parts.append(f\"<h4>Current output: <span class='tamil-text'>{''.join(output_chars)}</span></h4>\")\n            html_parts.append(f\"<h4>Currently generating: <span style='color: darkblue;' class='tamil-text'>{curr_char}</span></h4>\")\n            html_parts.append(\"<div style='margin: 10px 0; font-size: 2em;'>\")\n            \n            for j, char in enumerate(input_str):\n                weight = norm_attention[j]\n                html_parts.append(cstr(char, get_clr(weight)))\n            html_parts.append(\"</div><hr/>\")\n\n            # Convert HTML to image using PIL\n            image = html_to_image(''.join(html_parts))\n            images.append(image)\n\n            clear_output(wait=True)\n            display(HTML(''.join(html_parts)))\n            time.sleep(delay)\n        \n        # Add final summary screen\n        summary_html = []\n        summary_html.append(f\"<h3>Final Transliteration:</h3>\")\n        summary_html.append(f\"<h4>Latin (input): {input_str}</h4>\")\n        summary_html.append(f\"<h4 class='tamil-text'>Target script (output): {''.join(output_chars)}</h4>\")\n        summary_html.append(\"<hr/>\")\n        summary_html.append(\"<h3>Character-by-character attention:</h3>\")\n        \n        for i, (char, attn) in enumerate(zip(output_chars, attentions)):\n            summary_html.append(f\"<div style='margin: 10px 0;'><b class='tamil-text'>{char}</b> â†’ \")\n            for j, in_char in enumerate(input_str):\n                weight = attn[j]\n                summary_html.append(cstr(in_char, get_clr(weight)))\n            summary_html.append(\"</div>\")\n\n        summary_image = html_to_image(''.join(summary_html))\n        images.append(summary_image)\n        \n        # Save GIF\n        images[0].save(gif_path, save_all=True, append_images=images[1:], duration=int(delay * 1000), loop=0)\n        \n        # Log to wandb\n        wandb.log({\"attention_gif\": wandb.Image(gif_path)})\n        \n        # Also log the final summary as a separate image\n        wandb.log({\"final_summary\": wandb.Image(summary_image)})\n\n# Example usage\n# First, initialize wandb\nwandb.init(project=\"attention-visualize-1\", name=\"tamil-attention-run1\")\n\n# Then call the visualization function\nvisualize_attention_html_only(\n    model=model,\n    src_vocab=latin_vocab,\n    tgt_vocab=devanagari_vocab,  # This seems to be Tamil vocab based on your output\n    input_str=\"amma\",\n    delay=1.0,\n    gif_path=\"attention.gif\"\n)\n\nwandb.finish()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T17:43:30.777462Z","iopub.execute_input":"2025-05-21T17:43:30.777961Z","iopub.status.idle":"2025-05-21T17:44:15.479145Z","shell.execute_reply.started":"2025-05-21T17:43:30.777934Z","shell.execute_reply":"2025-05-21T17:44:15.478088Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<h3>Input: amma</h3><h4>Current output: <span class='tamil-text'>à®…à®…à®®à®®à®à®à®•à®ªà®•à®ªà®•à®ªà®•à®ªà®•à®ªà®ªà®•à®ªà®ª</span></h4><h4>Currently generating: <span style='color: darkblue;' class='tamil-text'>à®ª</span></h4><div style='margin: 10px 0; font-size: 2em;'><span style='background-color: #bfd8ed; padding: 2px; margin: 1px; display: inline-block; width: 30px; text-align: center;'>a</span><span style='background-color: #bfd8ed; padding: 2px; margin: 1px; display: inline-block; width: 30px; text-align: center;'>m</span><span style='background-color: #b9d6ea; padding: 2px; margin: 1px; display: inline-block; width: 30px; text-align: center;'>m</span><span style='background-color: #3787c0; padding: 2px; margin: 1px; display: inline-block; width: 30px; text-align: center;'>a</span></div><hr/>"},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_35/3208963213.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    144\u001b[0m )\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wandb/sdk/wandb_run.py\u001b[0m in \u001b[0;36mfinish\u001b[0;34m(exit_code, quiet)\u001b[0m\n\u001b[1;32m   4236\u001b[0m     \"\"\"\n\u001b[1;32m   4237\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4238\u001b[0;31m         \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexit_code\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexit_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquiet\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquiet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wandb/sdk/wandb_run.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    385\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mwb_logging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_to_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 387\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wandb/sdk/wandb_run.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    481\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDummy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 483\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wandb/sdk/wandb_run.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    423\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m                 \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_attaching\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 425\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wandb/sdk/wandb_run.py\u001b[0m in \u001b[0;36mfinish\u001b[0;34m(self, exit_code, quiet)\u001b[0m\n\u001b[1;32m   2180\u001b[0m                 ),\n\u001b[1;32m   2181\u001b[0m             )\n\u001b[0;32m-> 2182\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_finish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexit_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2184\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0m_log_to_run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wandb/sdk/wandb_run.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    385\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mwb_logging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_to_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 387\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wandb/sdk/wandb_run.py\u001b[0m in \u001b[0;36m_finish\u001b[0;34m(self, exit_code)\u001b[0m\n\u001b[1;32m   2195\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_teardown_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2196\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstage\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mTeardownStage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEARLY\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2197\u001b[0;31m                 \u001b[0mhook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2199\u001b[0m         \u001b[0;31m# Early-stage hooks may use methods that require _is_finished\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wandb/sdk/wandb_init.py\u001b[0m in \u001b[0;36m_jupyter_teardown\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    557\u001b[0m         \u001b[0mipython\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotebook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshell\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotebook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_history\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 559\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotebook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_ipynb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    560\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wandb/jupyter.py\u001b[0m in \u001b[0;36msave_ipynb\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 388\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_ipynb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    389\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m             \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtermerror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Failed to save notebook.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wandb/jupyter.py\u001b[0m in \u001b[0;36m_save_ipynb\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    406\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m         \u001b[0;31m# TODO: likely only save if the code has changed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 408\u001b[0;31m         \u001b[0mcolab_ipynb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattempt_colab_load_ipynb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    409\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcolab_ipynb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wandb/jupyter.py\u001b[0m in \u001b[0;36mattempt_colab_load_ipynb\u001b[0;34m()\u001b[0m\n\u001b[1;32m    272\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcolab\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m         \u001b[0;31m# This isn't thread safe, never call in a thread\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocking_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"get_ipynb\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"ipynb\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     if (\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":8},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport os\nimport wandb\nfrom torch.utils.data import Dataset, DataLoader\nfrom collections import defaultdict\n\n# Initialize wandb - will use WANDB_API_KEY environment variable if available\ntry:\n    # Fix wandb login - only call once\n    wandb.login(key=\"999fe4f321204bd8f10135f3e40de296c23050f9\")\nexcept:\n    print(\"WandB login failed - results will not be logged. Set WANDB_API_KEY in your environment.\")\n\nclass Encoder(nn.Module):\n    def __init__(self, input_dim, emb_dim, hidden_dim, num_layers, rnn_type, dropout=0.5, bidirectional=False):\n        super(Encoder, self).__init__()\n        self.embedding = nn.Embedding(input_dim, emb_dim)\n        self.rnn_type = rnn_type\n        self.num_layers = num_layers\n        self.hidden_dim = hidden_dim\n        self.bidirectional = bidirectional\n        self.dropout = nn.Dropout(dropout)\n        \n        self.directions = 2 if bidirectional else 1\n\n        if rnn_type == 'lstm':\n            self.rnn = nn.LSTM(emb_dim, hidden_dim, num_layers, batch_first=True, \n                              dropout=dropout if num_layers > 1 else 0, \n                              bidirectional=bidirectional)\n        elif rnn_type == 'gru':\n            self.rnn = nn.GRU(emb_dim, hidden_dim, num_layers, batch_first=True, \n                             dropout=dropout if num_layers > 1 else 0, \n                             bidirectional=bidirectional)\n        else:  # rnn\n            self.rnn = nn.RNN(emb_dim, hidden_dim, num_layers, batch_first=True, \n                             dropout=dropout if num_layers > 1 else 0, \n                             bidirectional=bidirectional)\n\n        # Projection layer to reduce bidirectional output to the expected dimension\n        if bidirectional:\n            self.projection = nn.Linear(hidden_dim * 2, hidden_dim)\n\n    def forward(self, src):\n        embedded = self.dropout(self.embedding(src))\n        \n        if self.rnn_type == 'lstm':\n            outputs, (hidden, cell) = self.rnn(embedded)\n            \n            # Process bidirectional states if needed\n            if self.bidirectional:\n                # Process hidden states\n                hidden = hidden.view(self.num_layers, self.directions, -1, self.hidden_dim)\n                # Concat forward and backward directions\n                hidden = torch.cat([hidden[:, 0], hidden[:, 1]], dim=2)\n                # Project to the correct dimension\n                hidden = self.projection(hidden)\n                \n                # Process cell states\n                cell = cell.view(self.num_layers, self.directions, -1, self.hidden_dim)\n                cell = torch.cat([cell[:, 0], cell[:, 1]], dim=2)\n                cell = self.projection(cell)\n                \n                return outputs, (hidden, cell)\n            return outputs, (hidden, cell)\n        else:\n            outputs, hidden = self.rnn(embedded)\n            \n            # Process bidirectional states if needed\n            if self.bidirectional:\n                hidden = hidden.view(self.num_layers, self.directions, -1, self.hidden_dim)\n                hidden = torch.cat([hidden[:, 0], hidden[:, 1]], dim=2)\n                hidden = self.projection(hidden)\n                \n            return outputs, hidden\n\n\n\n# class Decoder(nn.Module):\n#     def __init__(self, output_dim, emb_dim, hidden_dim, num_layers, rnn_type, dropout=0.5, attention=False):\n#         super().__init__()\n#         self.embedding = nn.Embedding(output_dim, emb_dim)\n#         self.rnn_type = rnn_type\n#         self.num_layers = num_layers\n#         self.hidden_dim = hidden_dim\n#         self.output_dim = output_dim\n#         self.dropout = nn.Dropout(dropout)\n#         self.attention = attention\n\n#         # Increase input size if using attention\n#         rnn_input_size = emb_dim + hidden_dim if attention else emb_dim\n\n#         if rnn_type == 'lstm':\n#             self.rnn = nn.LSTM(rnn_input_size, hidden_dim, num_layers, batch_first=True, \n#                               dropout=dropout if num_layers > 1 else 0)\n#         elif rnn_type == 'gru':\n#             self.rnn = nn.GRU(rnn_input_size, hidden_dim, num_layers, batch_first=True, \n#                              dropout=dropout if num_layers > 1 else 0)\n#         else:  # rnn\n#             self.rnn = nn.RNN(rnn_input_size, hidden_dim, num_layers, batch_first=True, \n#                              dropout=dropout if num_layers > 1 else 0)\n\n#         # Attention mechanism\n#         if attention:\n#             # Make sure the dimensionality is correct for the attention mechanism\n#             # It should match the encoder output dimension\n#             self.attention_layer = nn.Linear(hidden_dim * 2, 1)\n        \n#         self.fc_out = nn.Linear(hidden_dim, output_dim)\n\n#     def forward(self, input_char, hidden, encoder_outputs=None):\n#         input_char = input_char.unsqueeze(1)  # [batch_size, 1]\n#         embedded = self.dropout(self.embedding(input_char))  # [batch_size, 1, emb_dim]\n        \n#         # Apply attention if enabled and encoder_outputs are provided\n#         if self.attention and encoder_outputs is not None:\n#             # Make sure we're extracting the hidden state correctly based on RNN type\n#             if self.rnn_type == 'lstm':\n#                 last_hidden = hidden[0][-1].unsqueeze(1)  # [batch_size, 1, hidden_dim]\n#             else:\n#                 last_hidden = hidden[-1].unsqueeze(1)  # [batch_size, 1, hidden_dim]\n            \n#             # Make sure the shapes are compatible for attention calculation\n#             batch_size = encoder_outputs.size(0)\n#             seq_len = encoder_outputs.size(1)\n            \n#             # Repeat hidden state for each encoder output position\n#             last_hidden_repeated = last_hidden.repeat(1, seq_len, 1)\n            \n#             # Ensure dimensions match before concatenation\n#             if last_hidden_repeated.size(2) != encoder_outputs.size(2):\n#                 raise ValueError(f\"Hidden size ({last_hidden_repeated.size(2)}) does not match encoder_outputs size ({encoder_outputs.size(2)})\")\n            \n#             # Concatenate hidden state with encoder outputs\n#             attention_input = torch.cat((last_hidden_repeated, encoder_outputs), dim=2)\n            \n#             # Calculate attention scores\n#             attention_scores = self.attention_layer(attention_input).squeeze(-1)\n#             attention_weights = torch.softmax(attention_scores, dim=1).unsqueeze(1)\n            \n#             # Apply attention weights to encoder outputs\n#             context_vector = torch.bmm(attention_weights, encoder_outputs)\n            \n#             # Combine with embedding\n#             rnn_input = torch.cat((embedded, context_vector), dim=2)\n#         else:\n#             rnn_input = embedded\n        \n#         # Forward pass through RNN\n#         if self.rnn_type == 'lstm':\n#             output, (hidden, cell) = self.rnn(rnn_input, hidden)\n#             hidden_state = (hidden, cell)\n#         else:\n#             output, hidden = self.rnn(rnn_input, hidden)\n#             hidden_state = hidden\n            \n#         prediction = self.fc_out(output.squeeze(1))  # [batch_size, output_dim]\n        \n#         return prediction, hidden_state\n\nclass Decoder(nn.Module):\n    def __init__(self, output_dim, emb_dim, hidden_dim, num_layers, rnn_type, dropout=0.5, attention=False):\n        super(Decoder, self).__init__()\n        self.embedding = nn.Embedding(output_dim, emb_dim)\n        self.rnn_type = rnn_type\n        self.num_layers = num_layers\n        self.hidden_dim = hidden_dim\n        self.output_dim = output_dim\n        self.dropout = nn.Dropout(dropout)\n        self.attention = attention\n\n        # Increase input size if using attention\n        rnn_input_size = emb_dim + hidden_dim if attention else emb_dim\n\n        if rnn_type == 'lstm':\n            self.rnn = nn.LSTM(rnn_input_size, hidden_dim, num_layers, batch_first=True, \n                              dropout=dropout if num_layers > 1 else 0)\n        elif rnn_type == 'gru':\n            self.rnn = nn.GRU(rnn_input_size, hidden_dim, num_layers, batch_first=True, \n                             dropout=dropout if num_layers > 1 else 0)\n        else:  # rnn\n            self.rnn = nn.RNN(rnn_input_size, hidden_dim, num_layers, batch_first=True, \n                             dropout=dropout if num_layers > 1 else 0)\n\n        # Attention mechanism\n        if attention:\n            # Create input projection layer to handle possibly different dimensions\n            self.encoder_projection = nn.Linear(hidden_dim * 2, hidden_dim)  # For bidirectional encoder outputs\n            self.attention_layer = nn.Linear(hidden_dim * 2, 1)  # hidden_dim (decoder) + hidden_dim (projected encoder)\n        \n        self.fc_out = nn.Linear(hidden_dim, output_dim)\n\n    def forward(self, input_char, hidden, encoder_outputs=None):\n        input_char = input_char.unsqueeze(1)  # [batch_size, 1]\n        embedded = self.dropout(self.embedding(input_char))  # [batch_size, 1, emb_dim]\n        \n        # Apply attention if enabled and encoder_outputs are provided\n        if self.attention and encoder_outputs is not None:\n            # Make sure we're extracting the hidden state correctly based on RNN type\n            if self.rnn_type == 'lstm':\n                last_hidden = hidden[0][-1].unsqueeze(1)  # [batch_size, 1, hidden_dim]\n            else:\n                last_hidden = hidden[-1].unsqueeze(1)  # [batch_size, 1, hidden_dim]\n            \n            # Get dimensions for better debugging\n            batch_size = encoder_outputs.size(0)\n            seq_len = encoder_outputs.size(1)\n            encoder_dim = encoder_outputs.size(2)\n            \n            # Adapt encoder outputs if dimensions don't match (e.g., bidirectional encoder)\n            if encoder_dim != self.hidden_dim:\n                # Project encoder outputs to match decoder's hidden dimension\n                encoder_outputs_reshaped = encoder_outputs.view(batch_size * seq_len, encoder_dim)\n                encoder_outputs = self.encoder_projection(encoder_outputs_reshaped)\n                encoder_outputs = encoder_outputs.view(batch_size, seq_len, self.hidden_dim)\n            \n            # Repeat hidden state for each encoder output position\n            last_hidden_repeated = last_hidden.repeat(1, seq_len, 1)  # [batch_size, seq_len, hidden_dim]\n            \n            # No need for dimension check here, as we've already projected encoder outputs if necessary\n            \n            # Concatenate hidden state with encoder outputs\n            attention_input = torch.cat((last_hidden_repeated, encoder_outputs), dim=2)\n            \n            # Calculate attention scores\n            attention_scores = self.attention_layer(attention_input).squeeze(-1)\n            attention_weights = torch.softmax(attention_scores, dim=1).unsqueeze(1)\n            \n            # Apply attention weights to encoder outputs\n            context_vector = torch.bmm(attention_weights, encoder_outputs)\n            \n            # Combine with embedding\n            rnn_input = torch.cat((embedded, context_vector), dim=2)\n        else:\n            rnn_input = embedded\n        \n        # Forward pass through RNN\n        if self.rnn_type == 'lstm':\n            output, (hidden, cell) = self.rnn(rnn_input, hidden)\n            hidden_state = (hidden, cell)\n        else:\n            output, hidden = self.rnn(rnn_input, hidden)\n            hidden_state = hidden\n            \n        prediction = self.fc_out(output.squeeze(1))  # [batch_size, output_dim]\n        \n        return prediction, hidden_state\nclass Seq2Seq(nn.Module):\n    def __init__(self, encoder, decoder, rnn_type, device, use_attention=False):\n        super().__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n        self.rnn_type = rnn_type\n        self.device = device\n        self.use_attention = use_attention\n\n    def forward(self, src, trg, teacher_forcing_ratio=0.0):\n        batch_size = src.size(0)\n        trg_len = trg.size(1)\n        trg_vocab_size = self.decoder.output_dim\n\n        outputs = torch.zeros(batch_size, trg_len, trg_vocab_size).to(self.device)\n\n        # Encode the source sequence\n        encoder_outputs, hidden = self._encode(src)\n\n        # Use the first token as input to start decoding\n        input_char = trg[:, 0]  # <sos> token\n\n        for t in range(1, trg_len):\n            # Generate output from decoder\n            if self.use_attention:\n                output, hidden = self.decoder(input_char, hidden, encoder_outputs)\n            else:\n                output, hidden = self.decoder(input_char, hidden)\n                \n            outputs[:, t] = output\n            \n            # Teacher forcing: use real target or predicted token\n            teacher_force = torch.rand(1).item() < teacher_forcing_ratio\n            top1 = output.argmax(1)\n            input_char = trg[:, t] if teacher_force else top1\n\n        return outputs\n\n    def _encode(self, src):\n        # Get encoder outputs and final hidden state\n        encoder_outputs, hidden = self.encoder(src)\n            \n        # Adjust hidden state dimensions if encoder and decoder have different layers or bidirectional settings\n        encoder_layers = self.encoder.num_layers\n        decoder_layers = self.decoder.num_layers\n        \n        if self.rnn_type == 'lstm':\n            hidden_state, cell_state = hidden\n            \n            # Handle bidirectional case\n            if self.encoder.bidirectional:\n                # hidden_state shape: [num_layers, batch_size, hidden_dim]\n                # If it's already been processed by the encoder's forward method with bidirectional=True,\n                # no further changes needed here\n                pass\n                \n            # If encoder has fewer layers than decoder, pad with zeros\n            if encoder_layers < decoder_layers:\n                padding = torch.zeros(\n                    decoder_layers - encoder_layers,\n                    hidden_state.size(1),\n                    hidden_state.size(2)\n                ).to(self.device)\n                hidden_state = torch.cat([hidden_state, padding], dim=0)\n                cell_state = torch.cat([cell_state, padding], dim=0)\n            \n            # If encoder has more layers than decoder, truncate\n            elif encoder_layers > decoder_layers:\n                hidden_state = hidden_state[:decoder_layers]\n                cell_state = cell_state[:decoder_layers]\n                \n            # Make sure hidden dimensions match decoder's expected dimensions\n            if hidden_state.size(2) != self.decoder.hidden_dim:\n                # Project hidden state to the decoder's dimension using a linear layer\n                batch_size = hidden_state.size(1)\n                proj_hidden = torch.zeros(\n                    hidden_state.size(0),\n                    batch_size,\n                    self.decoder.hidden_dim\n                ).to(self.device)\n                \n                for layer in range(hidden_state.size(0)):\n                    # Simple linear projection for each layer\n                    proj_hidden[layer] = hidden_state[layer].clone().view(batch_size, -1)[:, :self.decoder.hidden_dim]\n                    \n                # Apply the same projection to cell state\n                proj_cell = torch.zeros_like(proj_hidden)\n                for layer in range(cell_state.size(0)):\n                    proj_cell[layer] = cell_state[layer].clone().view(batch_size, -1)[:, :self.decoder.hidden_dim]\n                    \n                hidden = (proj_hidden, proj_cell)\n            else:\n                hidden = (hidden_state, cell_state)\n        else:\n            # For GRU and RNN\n            # Handle bidirectional case\n            if self.encoder.bidirectional:\n                # If it's already been processed by the encoder's forward method with bidirectional=True,\n                # no further changes needed here\n                pass\n                \n            # If encoder has fewer layers than decoder, pad with zeros\n            if encoder_layers < decoder_layers:\n                padding = torch.zeros(\n                    decoder_layers - encoder_layers,\n                    hidden.size(1),\n                    hidden.size(2)\n                ).to(self.device)\n                hidden = torch.cat([hidden, padding], dim=0)\n            # If encoder has more layers than decoder, truncate\n            elif encoder_layers > decoder_layers:\n                hidden = hidden[:decoder_layers]\n            \n            # Make sure hidden dimensions match decoder's expected dimensions\n            if hidden.size(2) != self.decoder.hidden_dim:\n                # Project hidden state to the decoder's dimension\n                batch_size = hidden.size(1)\n                proj_hidden = torch.zeros(\n                    hidden.size(0),\n                    batch_size,\n                    self.decoder.hidden_dim\n                ).to(self.device)\n                \n                for layer in range(hidden.size(0)):\n                    # Simple linear projection for each layer\n                    proj_hidden[layer] = hidden[layer].clone().view(batch_size, -1)[:, :self.decoder.hidden_dim]\n                    \n                hidden = proj_hidden\n                \n        return encoder_outputs, hidden\n        \n    \n# Character-level vocabulary builder\ndef build_vocab(tokens):\n    vocab = {'<pad>': 0, '<sos>': 1, '<eos>': 2, '<unk>': 3}\n    for token in tokens:\n        for char in token:\n            if char not in vocab:\n                vocab[char] = len(vocab)\n    return vocab\n\ndef encode_sequence(seq, vocab):\n    return [vocab.get(char, vocab['<unk>']) for char in seq]\n\ndef pad_sequence(seq, max_len, pad_idx):\n    return seq + [pad_idx] * (max_len - len(seq))\n\n\nclass DakshinaDataset(Dataset):\n    def __init__(self, data_path, latin_vocab=None, devanagari_vocab=None):\n        self.latin_words = []\n        self.devanagari_words = []\n\n        # Group all transliterations by Devanagari word\n        candidates = defaultdict(list)\n\n        with open(data_path, encoding='utf-8') as f:\n            for line in f:\n                parts = line.strip().split('\\t')\n                if len(parts) != 3:\n                    continue\n                native, latin, rel = parts[0], parts[1], int(parts[2])\n                candidates[native].append((latin, rel))\n\n        # Keep only the transliteration(s) with highest score for each native word\n        for native, translits in candidates.items():\n            max_rel = max(rel for _, rel in translits)\n            for latin, rel in translits:\n                if rel == max_rel:\n                    self.latin_words.append(latin)\n                    self.devanagari_words.append(native)\n\n        print(f\"Dataset from {data_path}: {len(self.latin_words)} pairs.\")\n\n        self.latin_vocab = latin_vocab or build_vocab(self.latin_words)\n        self.devanagari_vocab = devanagari_vocab or build_vocab(self.devanagari_words)\n\n    def __len__(self):\n        return len(self.latin_words)\n\n    def __getitem__(self, idx):\n        src_seq = encode_sequence(self.latin_words[idx], self.latin_vocab)\n        trg_seq = encode_sequence(self.devanagari_words[idx], self.devanagari_vocab)\n        # add <sos> and <eos> tokens for target sequences\n        trg_seq = [self.devanagari_vocab['<sos>']] + trg_seq + [self.devanagari_vocab['<eos>']]\n        return src_seq, trg_seq\n\n\ndef beam_search(model, src_seq, src_vocab, tgt_vocab, beam_width=3, max_len=20):\n    model.eval()\n    index_to_char = {v: k for k, v in tgt_vocab.items()}\n    device = model.device\n\n    # Encode input\n    src_tensor = torch.tensor([encode_sequence(['<sos>'] + list(src_seq) + ['<eos>'], src_vocab)], dtype=torch.long).to(device)\n    \n    # Get encoder outputs and hidden state\n    encoder_outputs, hidden = model._encode(src_tensor)\n\n    beams = [([tgt_vocab['<sos>']], 0.0, hidden)]\n\n    for _ in range(max_len):\n        new_beams = []\n        for seq, score, hidden in beams:\n            last_token = torch.tensor([seq[-1]], dtype=torch.long).to(device)\n            \n            # Use attention if model has it\n            if model.use_attention:\n                output, new_hidden = model.decoder(last_token, hidden, encoder_outputs)\n            else:\n                output, new_hidden = model.decoder(last_token, hidden)\n                \n            log_probs = torch.log_softmax(output, dim=-1)\n            topk = torch.topk(log_probs, beam_width)\n\n            for prob, idx in zip(topk.values[0], topk.indices[0]):\n                new_seq = seq + [idx.item()]\n                new_score = score + prob.item()\n                new_beams.append((new_seq, new_score, new_hidden))\n\n        beams = sorted(new_beams, key=lambda x: x[1], reverse=True)[:beam_width]\n\n        if all(seq[-1] == tgt_vocab['<eos>'] for seq, _, _ in beams):\n            break\n\n    best_seq = beams[0][0]\n    decoded = [index_to_char[i] for i in best_seq if i not in {tgt_vocab['<sos>'], tgt_vocab['<eos>'], tgt_vocab['<pad>']}]\n    return ''.join(decoded)\n\n\ndef collate_fn(batch):\n    # batch is a list of tuples (src_seq, trg_seq)\n    src_seqs, trg_seqs = zip(*batch)\n\n    # find max lengths\n    max_src_len = max(len(seq) for seq in src_seqs)\n    max_trg_len = max(len(seq) for seq in trg_seqs)\n\n    # pad sequences\n    src_padded = [seq + [0]*(max_src_len - len(seq)) for seq in src_seqs]\n    trg_padded = [seq + [0]*(max_trg_len - len(seq)) for seq in trg_seqs]\n\n    # convert to tensors\n    src_tensor = torch.tensor(src_padded, dtype=torch.long)\n    trg_tensor = torch.tensor(trg_padded, dtype=torch.long)\n\n    return src_tensor, trg_tensor\n\n\ndef compute_word_accuracy(preds, trg, pad_idx, sos_idx=1, eos_idx=2):\n    \"\"\"\n    Compute word-level accuracy: a word is correct only if all tokens match (excluding pad, sos, eos).\n    preds, trg: [batch_size, seq_len]\n    \"\"\"\n    batch_size = preds.size(0)\n    correct = 0\n    \n    for i in range(batch_size):\n        # Get sequence for this example (exclude pad, sos, eos tokens)\n        pred_seq = [idx for idx in preds[i].tolist() if idx not in [pad_idx, sos_idx, eos_idx]]\n        trg_seq = [idx for idx in trg[i].tolist() if idx not in [pad_idx, sos_idx, eos_idx]]\n\n        # Compare full sequences (exact match)\n        if pred_seq == trg_seq:\n            correct += 1\n\n    return correct, batch_size\n\n\ndef train(model, dataloader, optimizer, criterion, clip=1, teacher_forcing_ratio=0.0):\n    model.train()\n    epoch_loss = 0\n    total_words = 0\n    correct_words = 0\n\n    pad_idx = 0  # Pad index in vocabulary\n    sos_idx = 1  # Start of sequence index\n    eos_idx = 2  # End of sequence index\n\n    for src, trg in dataloader:\n        src, trg = src.to(model.device), trg.to(model.device)\n        optimizer.zero_grad()\n\n        # Generate sequence\n        output = model(src, trg, teacher_forcing_ratio=teacher_forcing_ratio)\n        output_dim = output.shape[-1]\n\n        # Ignore first token (<sos>) in loss calculation\n        output = output[:, 1:].contiguous().view(-1, output_dim)\n        trg_flat = trg[:, 1:].contiguous().view(-1)\n\n        loss = criterion(output, trg_flat)\n        loss.backward()\n\n        # Use gradient clipping\n        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n        optimizer.step()\n\n        epoch_loss += loss.item()\n\n        # Calculate word accuracy\n        pred_tokens = output.argmax(1).view(trg[:, 1:].shape)  # [batch_size, trg_len-1]\n        trg_trimmed = trg[:, 1:]                             # [batch_size, trg_len-1]\n\n        correct, total = compute_word_accuracy(pred_tokens, trg_trimmed, pad_idx, sos_idx, eos_idx)\n        correct_words += correct\n        total_words += total\n\n    avg_loss = epoch_loss / len(dataloader)\n    word_acc = correct_words / total_words if total_words > 0 else 0\n\n    return avg_loss, word_acc * 100\n\n\ndef evaluate(model, dataloader, criterion):\n    model.eval()\n    epoch_loss = 0\n    total_words = 0\n    correct_words = 0\n\n    pad_idx = 0  # Pad index in vocabulary\n    sos_idx = 1  # Start of sequence index\n    eos_idx = 2  # End of sequence index\n\n    with torch.no_grad():\n        for src, trg in dataloader:\n            src, trg = src.to(model.device), trg.to(model.device)\n\n            # Generate full sequence with no teacher forcing\n            output = model(src, trg, teacher_forcing_ratio=0.0)\n            output_dim = output.shape[-1]\n\n            # Ignore first token (<sos>) in loss calculation\n            output = output[:, 1:].contiguous().view(-1, output_dim)\n            trg_flat = trg[:, 1:].contiguous().view(-1)\n\n            loss = criterion(output, trg_flat)\n            epoch_loss += loss.item()\n\n            # Calculate word accuracy\n            pred_tokens = output.argmax(1).view(trg[:, 1:].shape)\n            trg_trimmed = trg[:, 1:]\n\n            correct, total = compute_word_accuracy(pred_tokens, trg_trimmed, pad_idx, sos_idx, eos_idx)\n            correct_words += correct\n            total_words += total\n\n    avg_loss = epoch_loss / len(dataloader)\n    word_acc = correct_words / total_words if total_words > 0 else 0\n\n    return avg_loss, word_acc * 100\n\n\ndef predict_examples(model, dataloader, latin_index_to_token, devanagari_index_to_token, n=5):\n    \"\"\"Show a few examples of model predictions vs actual targets\"\"\"\n    model.eval()\n    pad_idx = 0\n    sos_idx = 1  # Start of sequence\n    eos_idx = 2  # End of sequence\n    count = 0\n\n    print(\"\\nPrediction Examples:\")\n    print(\"-\" * 60)\n\n    results = []\n\n    with torch.no_grad():\n        for src, trg in dataloader:\n            src, trg = src.to(model.device), trg.to(model.device)\n            output = model(src, trg, teacher_forcing_ratio=0.0)\n            pred_tokens = output.argmax(-1)  # [batch_size, seq_len]\n\n            for i in range(min(src.size(0), n - count)):\n                # Decode input\n                input_indices = [idx for idx in src[i].tolist() if idx != pad_idx]\n                input_tokens = [latin_index_to_token.get(idx, '<unk>') for idx in input_indices]\n                input_text = \"\".join(input_tokens)\n\n                # Decode target\n                target_indices = [idx for idx in trg[i].tolist() if idx not in [pad_idx, sos_idx, eos_idx]]\n                target_tokens = [devanagari_index_to_token.get(idx, '<unk>') for idx in target_indices]\n                target_text = \"\".join(target_tokens)\n\n                # Decode prediction\n                pred_indices = [idx for idx in pred_tokens[i].tolist() if idx not in [pad_idx, sos_idx, eos_idx]]\n                pred_tokens_text = [devanagari_index_to_token.get(idx, '<unk>') for idx in pred_indices]\n                pred_text = \"\".join(pred_tokens_text)\n\n                result = {\n                    \"input\": input_text,\n                    \"target\": target_text,\n                    \"prediction\": pred_text,\n                    \"correct\": pred_text == target_text\n                }\n                results.append(result)\n\n                print(f\"Input:     {input_text}\")\n                print(f\"Target:    {target_text}\")\n                print(f\"Predicted: {pred_text}\")\n                print(\"-\" * 60)\n\n                count += 1\n                if count >= n:\n                    break\n            if count >= n:\n                break\n\n    return results\n\n\n# Define the improved sweep configuration\ndef get_sweep_config():\n    sweep_config = {\n        'method': 'random',\n        'metric': {\n            'name': 'val_accuracy',\n            'goal': 'maximize'\n        },\n        'parameters': {\n            'embed_dim': {'values': [128, 256, 384]},\n            'hidden_dim': {'values': [256, 384, 512]},\n            'rnn_type': {'values': [ 'gru','rnn']},  # Removed basic RNN as it's typically not as effective\n            'encoder_layers': {'values': [ 1,2]},\n            'decoder_layers': {'values': [3]},\n            'dropout': {'values': [0.2, 0.3, 0.4]},\n            'learning_rate': {'values': [0.001, 0.0005,0.01,0.1]},\n            'batch_size': {'values': [64, 128]},\n            'epochs': {'values': [5,10,15, 20]},\n            'beam_size': {'values': [3, 5]},\n            'use_attention': {'values': [True]},  # Removed attention option to fix the error\n            'bidirectional': {'values': [True]},  # Set bidirectional to False to fix errors\n            'teacher_forcing_ratio': {'values': [0.0]},  # Disabled teacher forcing\n            'weight_decay': {'values': [1e-5, 1e-6]}\n        }\n    }\n    return sweep_config\n\n\n# Main training function that will be called for each sweep run\ndef train_sweep():\n    # Initialize wandb with sweep configuration\n    run = wandb.init(project=\"transliteration-model\")\n\n    # Access hyperparameters from wandb.config\n    config = run.config\n\n    # Setup device\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    print(f\"Using device: {device}\")\n\n    # Define data paths (adjust for Kaggle environment)\n    data_dir = '/kaggle/input/dakshina-dataset/dakshina_dataset_v1.0/ta/lexicons'\n    train_path = os.path.join(data_dir, 'ta.translit.sampled.train.tsv')\n    dev_path = os.path.join(data_dir, 'ta.translit.sampled.dev.tsv')\n    test_path = os.path.join(data_dir, 'ta.translit.sampled.test.tsv')\n\n    # Check if files exist\n    if not os.path.exists(train_path):\n        raise FileNotFoundError(f\"Could not find training data at {train_path}. Please check the path.\")\n\n    # Load datasets\n    print(\"Loading training dataset...\")\n    train_dataset = DakshinaDataset(train_path)\n    latin_vocab = train_dataset.latin_vocab\n    devanagari_vocab = train_dataset.devanagari_vocab\n\n    print(\"Loading validation dataset...\")\n    val_dataset = DakshinaDataset(\n        dev_path,\n        latin_vocab=latin_vocab,\n        devanagari_vocab=devanagari_vocab\n    )\n\n    print(\"Loading test dataset...\")\n    test_dataset = DakshinaDataset(\n        test_path,\n        latin_vocab=latin_vocab,\n        devanagari_vocab=devanagari_vocab\n    )\n\n    # Create DataLoaders\n    train_loader = DataLoader(\n        train_dataset,\n        batch_size=config.batch_size,\n        shuffle=True,\n        collate_fn=collate_fn\n    )\n\n    val_loader = DataLoader(\n        val_dataset,\n        batch_size=config.batch_size,\n        shuffle=False,\n        collate_fn=collate_fn\n    )\n\n    test_loader = DataLoader(\n        test_dataset,\n        batch_size=config.batch_size,\n        shuffle=False,\n        collate_fn=collate_fn\n    )\n\n    # Get vocabulary information\n    latin_vocab_size = len(latin_vocab)\n    devanagari_vocab_size = len(devanagari_vocab)\n    pad_idx = devanagari_vocab['<pad>']\n\n    # Log vocabulary sizes\n    wandb.log({\"latin_vocab_size\": latin_vocab_size, \"devanagari_vocab_size\": devanagari_vocab_size})\n\n    # Generate a model name based on hyperparameters\n    model_name = f\"{config.rnn_type}_ed{config.embed_dim}_hid{config.hidden_dim}_enc{config.encoder_layers}_dec{config.decoder_layers}_attn{config.use_attention}_drop{config.dropout}\"\n    wandb.run.name = model_name\n\n    # Create model architecture\n    encoder = Encoder(\n        input_dim=latin_vocab_size,\n        emb_dim=config.embed_dim,\n        hidden_dim=config.hidden_dim,\n        num_layers=config.encoder_layers,\n        rnn_type=config.rnn_type,\n        dropout=config.dropout,\n        bidirectional=config.bidirectional\n    )\n\n    decoder = Decoder(\n        output_dim=devanagari_vocab_size,\n        emb_dim=config.embed_dim,\n        hidden_dim=config.hidden_dim,\n        num_layers=config.decoder_layers,\n        rnn_type=config.rnn_type,\n        dropout=config.dropout,\n        attention=config.use_attention\n    )\n\n    model = Seq2Seq(\n        encoder, \n        decoder, \n        rnn_type=config.rnn_type, \n        device=device,\n        use_attention=config.use_attention\n    ).to(device)\n\n    # Count and log the number of model parameters\n    total_params = sum(p.numel() for p in model.parameters())\n    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n    wandb.log({\n        \"total_parameters\": total_params,\n        \"trainable_parameters\": trainable_params\n    })\n    print(f\"Model: {model_name}\")\n    print(f\"Total parameters: {total_params}\")\n    print(f\"Trainable parameters: {trainable_params}\")\n\n    # Setup optimizer and loss function with weight decay for regularization\n    optimizer = torch.optim.Adam(\n        model.parameters(), \n        lr=config.learning_rate, \n        weight_decay=config.weight_decay\n    )\n    \n    # Learning rate scheduler\n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n        optimizer, \n        mode='max', \n        factor=0.5, \n        patience=2, \n        verbose=True\n    )\n    \n    criterion = torch.nn.CrossEntropyLoss(ignore_index=pad_idx)\n\n    # Training loop with early stopping\n    best_val_loss = float('inf')\n    best_val_acc = 0\n    patience = 5  # Increased patience\n    patience_counter = 0\n\n    # Save directory for models\n    model_dir = '/kaggle/working/models'\n    os.makedirs(model_dir, exist_ok=True)\n\n    # Track metrics for each epoch\n    for epoch in range(config.epochs):\n        print(f\"\\nEpoch {epoch+1}/{config.epochs}\")\n        \n        # Train\n        train_loss, train_acc = train(model, train_loader, optimizer, criterion, clip=1.0, \n                                      teacher_forcing_ratio=config.teacher_forcing_ratio)\n        print(f\"Train Loss: {train_loss:.4f}, Accuracy: {train_acc:.2f}%\")\n        \n        # Validate\n        val_loss, val_acc = evaluate(model, val_loader, criterion)\n        print(f\"Val Loss: {val_loss:.4f}, Accuracy: {val_acc:.2f}%\")\n        \n        # Log metrics to wandb\n        wandb.log({\n            \"epoch\": epoch + 1,\n            \"train_loss\": train_loss,\n            \"train_accuracy\": train_acc,\n            \"val_loss\": val_loss,\n            \"val_accuracy\": val_acc\n        })\n        \n        # Save best model and check for early stopping\n        if val_acc > best_val_acc:\n            best_val_acc = val_acc\n            best_val_loss = val_loss\n            patience_counter = 0\n            \n            # Save the best model\n            best_model_path = os.path.join(model_dir, f\"{model_name}_best.pt\")\n            torch.save(model.state_dict(), best_model_path)\n            print(f\"New best model saved with val accuracy: {val_acc:.2f}%\")\n        else:\n            patience_counter += 1\n            if patience_counter >= patience:\n                print(f\"Early stopping after {epoch+1} epochs\")\n                break\n    \n    # Load the best model for testing\n    best_model_path = os.path.join(model_dir, f\"{model_name}_best.pt\")\n    try:\n        model.load_state_dict(torch.load(best_model_path))\n        print(\"Loaded best model for testing\")\n    except:\n        print(\"Using current model for testing (best model not found)\")\n    \n    # Final evaluation on test set\n    test_loss, test_acc = evaluate(model, test_loader, criterion)\n    print(f\"\\nTest Results -> Loss: {test_loss:.4f}, Accuracy: {test_acc:.2f}%\")\n    \n    # Log final test metrics\n    wandb.log({\n        \"test_loss\": test_loss,\n        \"test_accuracy\": test_acc\n    })\n    \n    # Create index-to-token dictionaries for prediction display\n    latin_index_to_token = {idx: token for token, idx in latin_vocab.items()}\n    devanagari_index_to_token = {idx: token for token, idx in devanagari_vocab.items()}\n    \n    # Generate prediction examples for visualization\n    example_results = predict_examples(\n        model, \n        test_loader, \n        latin_index_to_token, \n        devanagari_index_to_token, \n        n=5\n    )\n    \n    # Log the examples as a table in wandb\n    example_table = wandb.Table(\n        columns=[\"Input\", \"Target\", \"Prediction\", \"Correct\"]\n    )\n    for result in example_results:\n        example_table.add_data(\n            result[\"input\"], \n            result[\"target\"], \n            result[\"prediction\"], \n            result[\"correct\"]\n        )\n    wandb.log({\"prediction_examples\": example_table})\n    \n    # Test beam search if enabled\n    if config.beam_size > 1:\n        print(f\"\\nTesting beam search with beam width {config.beam_size}...\")\n        beam_correct = 0\n        beam_total = 0\n        \n        for src, trg in test_loader:\n            src = src.to(device)\n            trg = trg.to(device)\n            for i in range(min(5, src.size(0))):  # Test beam search on a few examples\n                # Get input sequence\n                src_seq = [latin_index_to_token[idx] for idx in src[i].tolist() if idx != pad_idx]\n                src_text = ''.join(src_seq)\n                \n                # Get target sequence\n                trg_indices = [idx for idx in trg[i].tolist() if idx not in [pad_idx, 1]]  # Remove <pad> and <sos>\n                trg_text = ''.join([devanagari_index_to_token.get(idx, '<unk>') for idx in trg_indices])\n                \n                # Run beam search\n                beam_pred = beam_search(\n                    model,\n                    src_text,\n                    latin_vocab,\n                    devanagari_vocab,\n                    beam_width=config.beam_size,\n                    max_len=30\n                )\n                \n                beam_correct += 1 if beam_pred == trg_text else 0\n                beam_total += 1\n                \n                # print(f\"Input: {src_text}\")\n                # print(f\"Target: {trg_text}\")\n                # print(f\"Beam Pred: {beam_pred}\")\n                # print(\"-\" * 60)\n        \n        beam_acc = beam_correct / beam_total * 100 if beam_total > 0 else 0\n        print(f\"Beam search accuracy: {beam_acc:.2f}%\")\n        wandb.log({\"beam_search_accuracy\": beam_acc})\n    \n    return model, latin_vocab, devanagari_vocab\n\n        \n\n# Entry point - runs a wandb sweep\ndef run_wandb_sweep():\n    sweep_config = get_sweep_config()\n    sweep_id = wandb.sweep(sweep_config, project=\"transliteration-model-tamil\")\n    wandb.agent(sweep_id, train_sweep, count=2)  \n\n\n# Main execution block for Kaggle\nif __name__ == \"__main__\":\n    # Run the wandb sweep\n    run_wandb_sweep()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-18T13:35:01.063Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport os\nimport wandb\nfrom torch.utils.data import Dataset, DataLoader\nfrom collections import defaultdict\n\n# Initialize wandb - will use WANDB_API_KEY environment variable if available\ntry:\n    # Fix wandb login - only call once\n    wandb.login(key=\"999fe4f321204bd8f10135f3e40de296c23050f9\")\nexcept:\n    print(\"WandB login failed - results will not be logged. Set WANDB_API_KEY in your environment.\")\n\nclass Encoder(nn.Module):\n    def __init__(self, input_dim, emb_dim, hidden_dim, num_layers, rnn_type, dropout=0.5, bidirectional=False):\n        super(Encoder, self).__init__()\n        self.embedding = nn.Embedding(input_dim, emb_dim)\n        self.rnn_type = rnn_type\n        self.num_layers = num_layers\n        self.hidden_dim = hidden_dim\n        self.bidirectional = bidirectional\n        self.dropout = nn.Dropout(dropout)\n        \n        self.directions = 2 if bidirectional else 1\n\n        if rnn_type == 'lstm':\n            self.rnn = nn.LSTM(emb_dim, hidden_dim, num_layers, batch_first=True, \n                              dropout=dropout if num_layers > 1 else 0, \n                              bidirectional=bidirectional)\n        elif rnn_type == 'gru':\n            self.rnn = nn.GRU(emb_dim, hidden_dim, num_layers, batch_first=True, \n                             dropout=dropout if num_layers > 1 else 0, \n                             bidirectional=bidirectional)\n        else:  # rnn\n            self.rnn = nn.RNN(emb_dim, hidden_dim, num_layers, batch_first=True, \n                             dropout=dropout if num_layers > 1 else 0, \n                             bidirectional=bidirectional)\n\n        # Projection layer to reduce bidirectional output to the expected dimension\n        if bidirectional:\n            self.projection = nn.Linear(hidden_dim * 2, hidden_dim)\n\n    def forward(self, src):\n        embedded = self.dropout(self.embedding(src))\n        \n        if self.rnn_type == 'lstm':\n            outputs, (hidden, cell) = self.rnn(embedded)\n            \n            # Process bidirectional states if needed\n            if self.bidirectional:\n                # Process hidden states\n                hidden = hidden.view(self.num_layers, self.directions, -1, self.hidden_dim)\n                # Concat forward and backward directions\n                hidden = torch.cat([hidden[:, 0], hidden[:, 1]], dim=2)\n                # Project to the correct dimension\n                hidden = self.projection(hidden)\n                \n                # Process cell states\n                cell = cell.view(self.num_layers, self.directions, -1, self.hidden_dim)\n                cell = torch.cat([cell[:, 0], cell[:, 1]], dim=2)\n                cell = self.projection(cell)\n                \n                # If outputs is bidirectional, we need to process it too\n                batch_size = outputs.size(0)\n                seq_len = outputs.size(1)\n                # Reshape and project encoder outputs\n                outputs = outputs.contiguous().view(batch_size, seq_len, self.hidden_dim * 2)\n                outputs = self.projection(outputs)\n                \n                return outputs, (hidden, cell)\n            return outputs, (hidden, cell)\n        else:\n            outputs, hidden = self.rnn(embedded)\n            \n            # Process bidirectional states if needed\n            if self.bidirectional:\n                hidden = hidden.view(self.num_layers, self.directions, -1, self.hidden_dim)\n                hidden = torch.cat([hidden[:, 0], hidden[:, 1]], dim=2)\n                hidden = self.projection(hidden)\n                \n                # Process outputs if bidirectional\n                batch_size = outputs.size(0)\n                seq_len = outputs.size(1)\n                # Reshape and project encoder outputs\n                outputs = outputs.contiguous().view(batch_size, seq_len, self.hidden_dim * 2)\n                outputs = self.projection(outputs)\n                \n            return outputs, hidden\n\nclass Decoder(nn.Module):\n    def __init__(self, output_dim, emb_dim, hidden_dim, num_layers, rnn_type, dropout=0.5, attention=False):\n        super(Decoder, self).__init__()\n        self.embedding = nn.Embedding(output_dim, emb_dim)\n        self.rnn_type = rnn_type\n        self.num_layers = num_layers\n        self.hidden_dim = hidden_dim\n        self.output_dim = output_dim\n        self.dropout = nn.Dropout(dropout)\n        self.attention = attention\n\n        # Increase input size if using attention\n        rnn_input_size = emb_dim + hidden_dim if attention else emb_dim\n\n        if rnn_type == 'lstm':\n            self.rnn = nn.LSTM(rnn_input_size, hidden_dim, num_layers, batch_first=True, \n                              dropout=dropout if num_layers > 1 else 0)\n        elif rnn_type == 'gru':\n            self.rnn = nn.GRU(rnn_input_size, hidden_dim, num_layers, batch_first=True, \n                             dropout=dropout if num_layers > 1 else 0)\n        else:  # rnn\n            self.rnn = nn.RNN(rnn_input_size, hidden_dim, num_layers, batch_first=True, \n                             dropout=dropout if num_layers > 1 else 0)\n\n        # Attention mechanism\n        if attention:\n            # Attention layers - simplified approach that works with any encoder output dimensionality\n            self.attn = nn.Linear(hidden_dim * 2, hidden_dim)\n            self.v = nn.Linear(hidden_dim, 1, bias=False)\n        \n        self.fc_out = nn.Linear(hidden_dim, output_dim)\n\n    def forward(self, input_char, hidden, encoder_outputs=None):\n        input_char = input_char.unsqueeze(1)  # [batch_size, 1]\n        embedded = self.dropout(self.embedding(input_char))  # [batch_size, 1, emb_dim]\n        \n        # Apply attention if enabled and encoder_outputs are provided\n        if self.attention and encoder_outputs is not None:\n            # Make sure we're extracting the hidden state correctly based on RNN type\n            if self.rnn_type == 'lstm':\n                query = hidden[0][-1].unsqueeze(1)  # [batch_size, 1, hidden_dim]\n            else:\n                query = hidden[-1].unsqueeze(1)  # [batch_size, 1, hidden_dim]\n            \n            # Get dimensions\n            batch_size = encoder_outputs.size(0)\n            src_len = encoder_outputs.size(1)\n            \n            # Create energy by combining query with encoder outputs\n            query = query.repeat(1, src_len, 1)  # [batch_size, src_len, hidden_dim]\n            \n            # Concatenate query with encoder outputs\n            energy_input = torch.cat((query, encoder_outputs), dim=2)  # [batch_size, src_len, 2*hidden_dim]\n            \n            # Calculate attention scores\n            energy = torch.tanh(self.attn(energy_input))  # [batch_size, src_len, hidden_dim]\n            attention = self.v(energy).squeeze(2)  # [batch_size, src_len]\n            \n            # Apply softmax to get attention weights\n            attention_weights = torch.softmax(attention, dim=1).unsqueeze(1)  # [batch_size, 1, src_len]\n            \n            # Create context vector by applying attention weights to encoder outputs\n            context = torch.bmm(attention_weights, encoder_outputs)  # [batch_size, 1, hidden_dim]\n            \n            # Combine with embedding\n            rnn_input = torch.cat((embedded, context), dim=2)  # [batch_size, 1, emb_dim+hidden_dim]\n        else:\n            rnn_input = embedded\n        \n        # Forward pass through RNN\n        if self.rnn_type == 'lstm':\n            output, (hidden, cell) = self.rnn(rnn_input, hidden)\n            hidden_state = (hidden, cell)\n        else:\n            output, hidden = self.rnn(rnn_input, hidden)\n            hidden_state = hidden\n            \n        # Generate prediction\n        prediction = self.fc_out(output.squeeze(1))  # [batch_size, output_dim]\n        \n        return prediction, hidden_state\n\nclass Seq2Seq(nn.Module):\n    def __init__(self, encoder, decoder, rnn_type, device, use_attention=False):\n        super().__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n        self.rnn_type = rnn_type\n        self.device = device\n        self.use_attention = use_attention\n\n    def forward(self, src, trg, teacher_forcing_ratio=0.0):\n        batch_size = src.size(0)\n        trg_len = trg.size(1)\n        trg_vocab_size = self.decoder.output_dim\n\n        outputs = torch.zeros(batch_size, trg_len, trg_vocab_size).to(self.device)\n\n        # Encode the source sequence\n        encoder_outputs, hidden = self._encode(src)\n\n        # Use the first token as input to start decoding\n        input_char = trg[:, 0]  # <sos> token\n\n        for t in range(1, trg_len):\n            # Generate output from decoder\n            if self.use_attention:\n                output, hidden = self.decoder(input_char, hidden, encoder_outputs)\n            else:\n                output, hidden = self.decoder(input_char, hidden)\n                \n            outputs[:, t] = output\n            \n            # Teacher forcing: use real target or predicted token\n            teacher_force = torch.rand(1).item() < teacher_forcing_ratio\n            top1 = output.argmax(1)\n            input_char = trg[:, t] if teacher_force else top1\n\n        return outputs\n\n    def _encode(self, src):\n        # Get encoder outputs and final hidden state\n        encoder_outputs, hidden = self.encoder(src)\n            \n        # Adjust hidden state dimensions if encoder and decoder have different layers\n        encoder_layers = self.encoder.num_layers\n        decoder_layers = self.decoder.num_layers\n        \n        if self.rnn_type == 'lstm':\n            hidden_state, cell_state = hidden\n            \n            # If encoder has fewer layers than decoder, pad with zeros\n            if encoder_layers < decoder_layers:\n                padding = torch.zeros(\n                    decoder_layers - encoder_layers,\n                    hidden_state.size(1),\n                    hidden_state.size(2)\n                ).to(self.device)\n                hidden_state = torch.cat([hidden_state, padding], dim=0)\n                cell_state = torch.cat([cell_state, padding], dim=0)\n            \n            # If encoder has more layers than decoder, truncate\n            elif encoder_layers > decoder_layers:\n                hidden_state = hidden_state[:decoder_layers]\n                cell_state = cell_state[:decoder_layers]\n                \n            # Make sure hidden dimensions match decoder's expected dimensions\n            if hidden_state.size(2) != self.decoder.hidden_dim:\n                # Project hidden state to the decoder's dimension using a linear projection\n                batch_size = hidden_state.size(1)\n                proj_hidden = torch.zeros(\n                    hidden_state.size(0),\n                    batch_size,\n                    self.decoder.hidden_dim\n                ).to(self.device)\n                \n                for layer in range(hidden_state.size(0)):\n                    # Simple linear projection for each layer\n                    proj_hidden[layer] = hidden_state[layer].clone().view(batch_size, -1)[:, :self.decoder.hidden_dim]\n                    \n                # Apply the same projection to cell state\n                proj_cell = torch.zeros_like(proj_hidden)\n                for layer in range(cell_state.size(0)):\n                    proj_cell[layer] = cell_state[layer].clone().view(batch_size, -1)[:, :self.decoder.hidden_dim]\n                    \n                hidden = (proj_hidden, proj_cell)\n            else:\n                hidden = (hidden_state, cell_state)\n        else:\n            # For GRU and RNN\n            # If encoder has fewer layers than decoder, pad with zeros\n            if encoder_layers < decoder_layers:\n                padding = torch.zeros(\n                    decoder_layers - encoder_layers,\n                    hidden.size(1),\n                    hidden.size(2)\n                ).to(self.device)\n                hidden = torch.cat([hidden, padding], dim=0)\n            # If encoder has more layers than decoder, truncate\n            elif encoder_layers > decoder_layers:\n                hidden = hidden[:decoder_layers]\n            \n            # Make sure hidden dimensions match decoder's expected dimensions\n            if hidden.size(2) != self.decoder.hidden_dim:\n                # Project hidden state to the decoder's dimension\n                batch_size = hidden.size(1)\n                proj_hidden = torch.zeros(\n                    hidden.size(0),\n                    batch_size,\n                    self.decoder.hidden_dim\n                ).to(self.device)\n                \n                for layer in range(hidden.size(0)):\n                    # Simple linear projection for each layer\n                    proj_hidden[layer] = hidden[layer].clone().view(batch_size, -1)[:, :self.decoder.hidden_dim]\n                    \n                hidden = proj_hidden\n                \n        return encoder_outputs, hidden\n        \n# Character-level vocabulary builder\ndef build_vocab(tokens):\n    vocab = {'<pad>': 0, '<sos>': 1, '<eos>': 2, '<unk>': 3}\n    for token in tokens:\n        for char in token:\n            if char not in vocab:\n                vocab[char] = len(vocab)\n    return vocab\n\ndef encode_sequence(seq, vocab):\n    return [vocab.get(char, vocab['<unk>']) for char in seq]\n\nclass DakshinaDataset(Dataset):\n    def __init__(self, data_path, latin_vocab=None, devanagari_vocab=None):\n        self.latin_words = []\n        self.devanagari_words = []\n\n        # Group all transliterations by Devanagari word\n        candidates = defaultdict(list)\n\n        with open(data_path, encoding='utf-8') as f:\n            for line in f:\n                parts = line.strip().split('\\t')\n                if len(parts) != 3:\n                    continue\n                native, latin, rel = parts[0], parts[1], int(parts[2])\n                candidates[native].append((latin, rel))\n\n        # Keep only the transliteration(s) with highest score for each native word\n        for native, translits in candidates.items():\n            max_rel = max(rel for _, rel in translits)\n            for latin, rel in translits:\n                if rel == max_rel:\n                    self.latin_words.append(latin)\n                    self.devanagari_words.append(native)\n\n        print(f\"Dataset from {data_path}: {len(self.latin_words)} pairs.\")\n\n        self.latin_vocab = latin_vocab or build_vocab(self.latin_words)\n        self.devanagari_vocab = devanagari_vocab or build_vocab(self.devanagari_words)\n\n    def __len__(self):\n        return len(self.latin_words)\n\n    def __getitem__(self, idx):\n        src_seq = encode_sequence(self.latin_words[idx], self.latin_vocab)\n        trg_seq = encode_sequence(self.devanagari_words[idx], self.devanagari_vocab)\n        # add <sos> and <eos> tokens for target sequences\n        trg_seq = [self.devanagari_vocab['<sos>']] + trg_seq + [self.devanagari_vocab['<eos>']]\n        return src_seq, trg_seq\n\ndef collate_fn(batch):\n    # batch is a list of tuples (src_seq, trg_seq)\n    src_seqs, trg_seqs = zip(*batch)\n\n    # find max lengths\n    max_src_len = max(len(seq) for seq in src_seqs)\n    max_trg_len = max(len(seq) for seq in trg_seqs)\n\n    # pad sequences\n    src_padded = [seq + [0]*(max_src_len - len(seq)) for seq in src_seqs]\n    trg_padded = [seq + [0]*(max_trg_len - len(seq)) for seq in trg_seqs]\n\n    # convert to tensors\n    src_tensor = torch.tensor(src_padded, dtype=torch.long)\n    trg_tensor = torch.tensor(trg_padded, dtype=torch.long)\n\n    return src_tensor, trg_tensor\n\ndef compute_word_accuracy(preds, trg, pad_idx, sos_idx=1, eos_idx=2):\n    \"\"\"\n    Compute word-level accuracy: a word is correct only if all tokens match (excluding pad, sos, eos).\n    preds, trg: [batch_size, seq_len]\n    \"\"\"\n    batch_size = preds.size(0)\n    correct = 0\n    \n    for i in range(batch_size):\n        # Get sequence for this example (exclude pad, sos, eos tokens)\n        pred_seq = [idx for idx in preds[i].tolist() if idx not in [pad_idx, sos_idx, eos_idx]]\n        trg_seq = [idx for idx in trg[i].tolist() if idx not in [pad_idx, sos_idx, eos_idx]]\n\n        # Compare full sequences (exact match)\n        if pred_seq == trg_seq:\n            correct += 1\n\n    return correct, batch_size\n\ndef beam_search(model, src_seq, src_vocab, tgt_vocab, beam_width=3, max_len=20):\n    model.eval()\n    index_to_char = {v: k for k, v in tgt_vocab.items()}\n    device = model.device\n\n    # Prepare input\n    src_indices = encode_sequence(src_seq, src_vocab)\n    src_tensor = torch.tensor([src_indices], dtype=torch.long).to(device)\n    \n    # Get encoder outputs and hidden state\n    encoder_outputs, hidden = model._encode(src_tensor)\n\n    # Start with start-of-sequence token\n    beams = [([tgt_vocab['<sos>']], 0.0, hidden)]\n\n    for _ in range(max_len):\n        new_beams = []\n        for seq, score, hidden in beams:\n            last_token = torch.tensor([seq[-1]], dtype=torch.long).to(device)\n            \n            # Use attention if model has it\n            if model.use_attention:\n                output, new_hidden = model.decoder(last_token, hidden, encoder_outputs)\n            else:\n                output, new_hidden = model.decoder(last_token, hidden)\n                \n            log_probs = torch.log_softmax(output, dim=-1)\n            topk = torch.topk(log_probs, beam_width)\n\n            for prob, idx in zip(topk.values[0], topk.indices[0]):\n                new_seq = seq + [idx.item()]\n                new_score = score + prob.item()\n                new_beams.append((new_seq, new_score, new_hidden))\n\n        beams = sorted(new_beams, key=lambda x: x[1], reverse=True)[:beam_width]\n\n        # Stop if all beams end with EOS\n        if all(seq[-1] == tgt_vocab['<eos>'] for seq, _, _ in beams):\n            break\n\n    # Pick the best beam\n    best_seq = beams[0][0]\n    # Remove special tokens for output\n    decoded = [index_to_char[i] for i in best_seq if i not in {tgt_vocab['<sos>'], tgt_vocab['<eos>'], tgt_vocab['<pad>']}]\n    return ''.join(decoded)\n\ndef train(model, dataloader, optimizer, criterion, clip=1, teacher_forcing_ratio=0.0):\n    model.train()\n    epoch_loss = 0\n    total_words = 0\n    correct_words = 0\n\n    pad_idx = 0  # Pad index in vocabulary\n    sos_idx = 1  # Start of sequence index\n    eos_idx = 2  # End of sequence index\n\n    for src, trg in dataloader:\n        src, trg = src.to(model.device), trg.to(model.device)\n        optimizer.zero_grad()\n\n        # Generate sequence\n        output = model(src, trg, teacher_forcing_ratio=teacher_forcing_ratio)\n        output_dim = output.shape[-1]\n\n        # Ignore first token (<sos>) in loss calculation\n        output = output[:, 1:].contiguous().view(-1, output_dim)\n        trg_flat = trg[:, 1:].contiguous().view(-1)\n\n        loss = criterion(output, trg_flat)\n        loss.backward()\n\n        # Use gradient clipping\n        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n        optimizer.step()\n\n        epoch_loss += loss.item()\n\n        # Calculate word accuracy\n        pred_tokens = output.argmax(1).view(trg[:, 1:].shape)  # [batch_size, trg_len-1]\n        trg_trimmed = trg[:, 1:]                             # [batch_size, trg_len-1]\n\n        correct, total = compute_word_accuracy(pred_tokens, trg_trimmed, pad_idx, sos_idx, eos_idx)\n        correct_words += correct\n        total_words += total\n\n    avg_loss = epoch_loss / len(dataloader)\n    word_acc = correct_words / total_words if total_words > 0 else 0\n\n    return avg_loss, word_acc * 100\n\ndef evaluate(model, dataloader, criterion):\n    model.eval()\n    epoch_loss = 0\n    total_words = 0\n    correct_words = 0\n\n    pad_idx = 0  # Pad index in vocabulary\n    sos_idx = 1  # Start of sequence index\n    eos_idx = 2  # End of sequence index\n\n    with torch.no_grad():\n        for src, trg in dataloader:\n            src, trg = src.to(model.device), trg.to(model.device)\n\n            # Generate full sequence with no teacher forcing\n            output = model(src, trg, teacher_forcing_ratio=0.0)\n            output_dim = output.shape[-1]\n\n            # Ignore first token (<sos>) in loss calculation\n            output = output[:, 1:].contiguous().view(-1, output_dim)\n            trg_flat = trg[:, 1:].contiguous().view(-1)\n\n            loss = criterion(output, trg_flat)\n            epoch_loss += loss.item()\n\n            # Calculate word accuracy\n            pred_tokens = output.argmax(1).view(trg[:, 1:].shape)\n            trg_trimmed = trg[:, 1:]\n\n            correct, total = compute_word_accuracy(pred_tokens, trg_trimmed, pad_idx, sos_idx, eos_idx)\n            correct_words += correct\n            total_words += total\n\n    avg_loss = epoch_loss / len(dataloader)\n    word_acc = correct_words / total_words if total_words > 0 else 0\n\n    return avg_loss, word_acc * 100\n\ndef predict_examples(model, dataloader, latin_index_to_token, devanagari_index_to_token, n=5):\n    \"\"\"Show a few examples of model predictions vs actual targets\"\"\"\n    model.eval()\n    pad_idx = 0\n    sos_idx = 1  # Start of sequence\n    eos_idx = 2  # End of sequence\n    count = 0\n    results = []\n\n    print(\"\\nPrediction Examples:\")\n    print(\"-\" * 60)\n\n    with torch.no_grad():\n        for src, trg in dataloader:\n            src, trg = src.to(model.device), trg.to(model.device)\n            output = model(src, trg, teacher_forcing_ratio=0.0)\n            print(output.shape)\n            pred_tokens = output.argmax(-1)  # [batch_size, seq_len]\n\n            for i in range(min(src.size(0), n - count)):\n                # Decode input\n                input_indices = [idx for idx in src[i].tolist() if idx != pad_idx]\n                input_tokens = [latin_index_to_token.get(idx, '<unk>') for idx in input_indices]\n                input_text = \"\".join(input_tokens)\n\n                # Decode target\n                target_indices = [idx for idx in trg[i].tolist() if idx not in [pad_idx, sos_idx, eos_idx]]\n                target_tokens = [devanagari_index_to_token.get(idx, '<unk>') for idx in target_indices]\n                target_text = \"\".join(target_tokens)\n\n                # Decode prediction\n                pred_indices = [idx for idx in pred_tokens[i].tolist() if idx not in [pad_idx, sos_idx, eos_idx]]\n                pred_tokens_text = [devanagari_index_to_token.get(idx, '<unk>') for idx in pred_indices]\n                pred_text = \"\".join(pred_tokens_text)\n\n                result = {\n                    \"input\": input_text,\n                    \"target\": target_text,\n                    \"prediction\": pred_text,\n                    \"correct\": pred_text == target_text\n                }\n                results.append(result)\n\n                print(f\"Input:     {input_text}\")\n                print(f\"Target:    {target_text}\")\n                print(f\"Predicted: {pred_text}\")\n                print(\"-\" * 60)\n\n                count += 1\n                if count >= n:\n                    break\n            if count >= n:\n                break\n\n    return results\n\n# Define sweep configuration with improved parameters\ndef get_sweep_config():\n    sweep_config = {\n        'method': 'random',\n        'metric': {\n            'name': 'val_accuracy',\n            'goal': 'maximize'\n        },\n        'parameters': {\n            'embed_dim': {'values': [128, 256, 384]},\n            'hidden_dim': {'values': [256, 384, 512]},\n            'rnn_type': {'values': ['lstm', 'gru']},  # Removed basic RNN\n            'encoder_layers': {'values': [1, 2,3]},\n            'decoder_layers': {'values': [1,2, 3]},\n            'dropout': {'values': [0.2, 0.3, 0.4]},\n            'learning_rate': {'values': [0.001, 0.0005,0.1,0.001]},\n            'batch_size': {'values': [64, 128]},\n            'epochs': {'values': [2]},\n            'beam_size': {'values': [3, 5]},\n            'use_attention': {'values': [True, False]},\n            'bidirectional': {'values': [True,False]},\n            'teacher_forcing_ratio': {'values': [0.0]},\n            'weight_decay': {'values': [1e-5, 1e-6]}\n        }\n    }\n    return sweep_config\n\n# Main training function for sweep runs\ndef train_sweep():\n    # Initialize wandb with sweep configuration\n    run = wandb.init(project=\"transliteration-model\")\n\n    # Access hyperparameters from wandb.config\n    config = run.config\n\n    # Setup device\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    print(f\"Using device: {device}\")\n\n    # Define data paths (adjust for Kaggle environment)\n    data_dir = '/kaggle/input/dakshina-dataset/dakshina_dataset_v1.0/ta/lexicons'\n    train_path = os.path.join(data_dir, 'ta.translit.sampled.train.tsv')\n    dev_path = os.path.join(data_dir, 'ta.translit.sampled.dev.tsv')\n    test_path = os.path.join(data_dir, 'ta.translit.sampled.test.tsv')\n\n    # Check if files exist\n    if not os.path.exists(train_path):\n        raise FileNotFoundError(f\"Could not find training data at {train_path}. Please check the path.\")\n\n    # Load datasets\n    print(\"Loading training dataset...\")\n    train_dataset = DakshinaDataset(train_path)\n    latin_vocab = train_dataset.latin_vocab\n    devanagari_vocab = train_dataset.devanagari_vocab\n\n    print(\"Loading validation dataset...\")\n    val_dataset = DakshinaDataset(\n        dev_path,\n        latin_vocab=latin_vocab,\n        devanagari_vocab=devanagari_vocab\n    )\n\n    print(\"Loading test dataset...\")\n    test_dataset = DakshinaDataset(\n        test_path,\n        latin_vocab=latin_vocab,\n        devanagari_vocab=devanagari_vocab\n    )\n\n    # Create DataLoaders\n    train_loader = DataLoader(\n        train_dataset,\n        batch_size=config.batch_size,\n        shuffle=True,\n        collate_fn=collate_fn\n    )\n\n    val_loader = DataLoader(\n        val_dataset,\n        batch_size=config.batch_size,\n        shuffle=False,\n        collate_fn=collate_fn\n    )\n\n    test_loader = DataLoader(\n        test_dataset,\n        batch_size=config.batch_size,\n        shuffle=False,\n        collate_fn=collate_fn\n    )\n\n    # Get vocabulary information\n    latin_vocab_size = len(latin_vocab)\n    devanagari_vocab_size = len(devanagari_vocab)\n    pad_idx = devanagari_vocab['<pad>']\n\n    # Log vocabulary sizes\n    wandb.log({\"latin_vocab_size\": latin_vocab_size, \"devanagari_vocab_size\": devanagari_vocab_size})\n\n    # Generate a model name based on hyperparameters\n    model_name = f\"{config.rnn_type}_ed{config.embed_dim}_hid{config.hidden_dim}_enc{config.encoder_layers}_dec{config.decoder_layers}_attn{config.use_attention}_drop{config.dropout}\"\n    wandb.run.name = model_name\n\n    # Create model architecture\n    encoder = Encoder(\n        input_dim=latin_vocab_size,\n        emb_dim=config.embed_dim,\n        hidden_dim=config.hidden_dim,\n        num_layers=config.encoder_layers,\n        rnn_type=config.rnn_type,\n        dropout=config.dropout,\n        bidirectional=config.bidirectional\n    )\n\n    decoder = Decoder(\n        output_dim=devanagari_vocab_size,\n        emb_dim=config.embed_dim,\n        hidden_dim=config.hidden_dim,\n        num_layers=config.decoder_layers,\n        rnn_type=config.rnn_type,\n        dropout=config.dropout,\n        attention=config.use_attention\n    )\n\n    model = Seq2Seq(\n        encoder, \n        decoder, \n        rnn_type=config.rnn_type, \n        device=device,\n        use_attention=config.use_attention\n    ).to(device)\n\n    # Count and log the number of model parameters\n    total_params = sum(p.numel() for p in model.parameters())\n    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n    wandb.log({\n        \"total_parameters\": total_params,\n        \"trainable_parameters\": trainable_params\n    })\n    print(f\"Model: {model_name}\")\n    print(f\"Total parameters: {total_params}\")\n    print(f\"Trainable parameters: {trainable_params}\")\n\n    # Setup optimizer and loss function with weight decay for regularization\n    optimizer = torch.optim.Adam(\n        model.parameters(), \n        lr=config.learning_rate, \n        weight_decay=config.weight_decay\n    )\n    \n    # Learning rate scheduler\n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n        optimizer, \n        mode='max', \n        factor=0.5, \n        patience=2, \n        verbose=True\n    )\n    \n    criterion = torch.nn.CrossEntropyLoss(ignore_index=pad_idx)\n\n    # Training loop with early stopping\n    best_val_loss = float('inf')\n    best_val_acc = 0\n    patience = 5  # Increased patience\n    patience_counter = 0\n\n    # Save directory for models\n    model_dir = '/kaggle/working'\n    os.makedirs(model_dir, exist_ok=True)\n\n    # Track metrics for each epoch\n    for epoch in range(config.epochs):\n        print(f\"\\nEpoch {epoch+1}/{config.epochs}\")\n        \n        # Train\n        train_loss, train_acc = train(model, train_loader, optimizer, criterion, clip=1.0, \n                                      teacher_forcing_ratio=config.teacher_forcing_ratio)\n        print(f\"Train Loss: {train_loss:.4f}, Accuracy: {train_acc:.2f}%\")\n        \n        # Validate\n        val_loss, val_acc = evaluate(model, val_loader, criterion)\n        print(f\"Val Loss: {val_loss:.4f}, Accuracy: {val_acc:.2f}%\")\n        \n        # Log metrics to wandb\n        wandb.log({\n            \"epoch\": epoch + 1,\n            \"train_loss\": train_loss,\n            \"train_accuracy\": train_acc,\n            \"val_loss\": val_loss,\n            \"val_accuracy\": val_acc\n        })\n        \n        # Save best model and check for early stopping\n        if val_acc > best_val_acc:\n            best_val_acc = val_acc\n            best_val_loss = val_loss\n            patience_counter = 0\n            \n            # Save the best model\n            best_model_path = os.path.join(model_dir, f\"{model_name}_best.pt\")\n            torch.save(model.state_dict(), best_model_path)\n            print(f\"New best model saved with val accuracy: {val_acc:.2f}%\")\n        else:\n            patience_counter += 1\n            if patience_counter >= patience:\n                print(f\"Early stopping after {epoch+1} epochs\")\n                break\n    \n    # Load the best model for testing\n    best_model_path = os.path.join(model_dir, f\"{model_name}_best.pt\")\n    try:\n        model.load_state_dict(torch.load(best_model_path))\n        print(\"Loaded best model for testing\")\n    except:\n        print(\"Using current model for testing (best model not found)\")\n    \n    # Final evaluation on test set\n    test_loss, test_acc = evaluate(model, test_loader, criterion)\n    print(f\"\\nTest Results -> Loss: {test_loss:.4f}, Accuracy: {test_acc:.2f}%\")\n    \n    # Log final test metrics\n    wandb.log({\n        \"test_loss\": test_loss,\n        \"test_accuracy\": test_acc\n    })\n    \n    # Create index-to-token dictionaries for prediction display\n    latin_index_to_token = {idx: token for token, idx in latin_vocab.items()}\n    devanagari_index_to_token = {idx: token for token, idx in devanagari_vocab.items()}\n    \n    # Generate prediction examples for visualization\n    example_results = predict_examples(\n        model, \n        test_loader, \n        latin_index_to_token, \n        devanagari_index_to_token, \n        n=5\n    )\n    \n    # Log the examples as a table in wandb\n    example_table = wandb.Table(\n        columns=[\"Input\", \"Target\", \"Prediction\", \"Correct\"]\n    )\n    for result in example_results:\n        example_table.add_data(\n            result[\"input\"], \n            result[\"target\"], \n            result[\"prediction\"], \n            result[\"correct\"]\n        )\n    wandb.log({\"prediction_examples\": example_table})\n    \n    # Test beam search if enabled\n    if config.beam_size > 1:\n        print(f\"\\nTesting beam search with beam width {config.beam_size}...\")\n        beam_correct = 0\n        beam_total = 0\n        \n        for src, trg in test_loader:\n            src = src.to(device)\n            trg = trg.to(device)\n            for i in range(min(5, src.size(0))):  # Test beam search on a few examples\n                # Get input sequence\n                src_seq = [latin_index_to_token[idx] for idx in src[i].tolist() if idx != pad_idx]\n                src_text = ''.join(src_seq)\n                \n                # Get target sequence\n                trg_indices = [idx for idx in trg[i].tolist() if idx not in [pad_idx, 1]]  # Remove <pad> and <sos>\n                trg_text = ''.join([devanagari_index_to_token.get(idx, '<unk>') for idx in trg_indices])\n                \n                # Run beam search\n                beam_pred = beam_search(\n                    model,\n                    src_text,\n                    latin_vocab,\n                    devanagari_vocab,\n                    beam_width=config.beam_size,\n                    max_len=30\n                )\n                \n                beam_correct += 1 if beam_pred == trg_text else 0\n                beam_total += 1\n                \n                # print(f\"Input: {src_text}\")\n                # print(f\"Target: {trg_text}\")\n                # print(f\"Beam Pred: {beam_pred}\")\n                # print(\"-\" * 60)\n        \n        beam_acc = beam_correct / beam_total * 100 if beam_total > 0 else 0\n        print(f\"Beam search accuracy: {beam_acc:.2f}%\")\n        wandb.log({\"beam_search_accuracy\": beam_acc})\n    \n    return model, latin_vocab, devanagari_vocab\n\n        \n\n# Entry point - runs a wandb sweep\ndef run_wandb_sweep():\n    sweep_config = get_sweep_config()\n    sweep_id = wandb.sweep(sweep_config, project=\"transliteration-model\")\n    wandb.agent(sweep_id, train_sweep, count=1)  \n\n\n# Main execution block for Kaggle\nif __name__ == \"__main__\":\n    # Run the wandb sweep\n    run_wandb_sweep()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T14:49:10.492727Z","iopub.execute_input":"2025-05-20T14:49:10.493237Z","execution_failed":"2025-05-20T14:50:35.527Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mma23m007\u001b[0m (\u001b[33mma23m007-iit-madras\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"name":"stdout","text":"Create sweep with ID: i5rysnj4\nSweep URL: https://wandb.ai/ma23m007-iit-madras/transliteration-model/sweeps/i5rysnj4\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 6danckjf with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_size: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbidirectional: True\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdecoder_layers: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembed_dim: 384\n\u001b[34m\u001b[1mwandb\u001b[0m: \tencoder_layers: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dim: 512\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.1\n\u001b[34m\u001b[1mwandb\u001b[0m: \trnn_type: lstm\n\u001b[34m\u001b[1mwandb\u001b[0m: \tteacher_forcing_ratio: 0\n\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_attention: False\n\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 1e-05\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Ignoring project 'transliteration-model' when running a sweep."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250520_144935-6danckjf</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/ma23m007-iit-madras/transliteration-model/runs/6danckjf' target=\"_blank\">sleek-sweep-1</a></strong> to <a href='https://wandb.ai/ma23m007-iit-madras/transliteration-model' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/ma23m007-iit-madras/transliteration-model/sweeps/i5rysnj4' target=\"_blank\">https://wandb.ai/ma23m007-iit-madras/transliteration-model/sweeps/i5rysnj4</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/ma23m007-iit-madras/transliteration-model' target=\"_blank\">https://wandb.ai/ma23m007-iit-madras/transliteration-model</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/ma23m007-iit-madras/transliteration-model/sweeps/i5rysnj4' target=\"_blank\">https://wandb.ai/ma23m007-iit-madras/transliteration-model/sweeps/i5rysnj4</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/ma23m007-iit-madras/transliteration-model/runs/6danckjf' target=\"_blank\">https://wandb.ai/ma23m007-iit-madras/transliteration-model/runs/6danckjf</a>"},"metadata":{}},{"name":"stdout","text":"Using device: cpu\nLoading training dataset...\nDataset from /kaggle/input/dakshina-dataset/dakshina_dataset_v1.0/ta/lexicons/ta.translit.sampled.train.tsv: 38716 pairs.\nLoading validation dataset...\nDataset from /kaggle/input/dakshina-dataset/dakshina_dataset_v1.0/ta/lexicons/ta.translit.sampled.dev.tsv: 4054 pairs.\nLoading test dataset...\nDataset from /kaggle/input/dakshina-dataset/dakshina_dataset_v1.0/ta/lexicons/ta.translit.sampled.test.tsv: 3938 pairs.\nModel: lstm_ed384_hid512_enc3_dec2_attnFalse_drop0.3\nTotal parameters: 20799026\nTrainable parameters: 20799026\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 1/2\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport os\nimport wandb\nfrom torch.utils.data import Dataset, DataLoader\nfrom collections import defaultdict\n\n# Initialize wandb - will use WANDB_API_KEY environment variable if available\ntry:\n    # Fix wandb login - only call once\n    wandb.login(key=\"999fe4f321204bd8f10135f3e40de296c23050f9\")\nexcept:\n    print(\"WandB login failed - results will not be logged. Set WANDB_API_KEY in your environment.\")\n\nclass Encoder(nn.Module):\n    def __init__(self, input_dim, emb_dim, hidden_dim, num_layers, rnn_type, dropout=0.5, bidirectional=False):\n        super(Encoder, self).__init__()\n        self.embedding = nn.Embedding(input_dim, emb_dim)\n        self.rnn_type = rnn_type\n        self.num_layers = num_layers\n        self.hidden_dim = hidden_dim\n        self.bidirectional = bidirectional\n        self.dropout = nn.Dropout(dropout)\n\n        self.directions = 2 if bidirectional else 1\n\n        if rnn_type == 'lstm':\n            self.rnn = nn.LSTM(emb_dim, hidden_dim, num_layers, batch_first=True,\n                              dropout=dropout if num_layers > 1 else 0,\n                              bidirectional=bidirectional)\n        elif rnn_type == 'gru':\n            self.rnn = nn.GRU(emb_dim, hidden_dim, num_layers, batch_first=True,\n                             dropout=dropout if num_layers > 1 else 0,\n                             bidirectional=bidirectional)\n        else:  # rnn\n            self.rnn = nn.RNN(emb_dim, hidden_dim, num_layers, batch_first=True,\n                             dropout=dropout if num_layers > 1 else 0,\n                             bidirectional=bidirectional)\n\n        # Projection layer to reduce bidirectional output to the expected dimension\n        if bidirectional:\n            self.projection = nn.Linear(hidden_dim * 2, hidden_dim)\n\n    def forward(self, src):\n        embedded = self.dropout(self.embedding(src))\n\n        if self.rnn_type == 'lstm':\n            outputs, (hidden, cell) = self.rnn(embedded)\n\n            # Process bidirectional states if needed\n            if self.bidirectional:\n                # Process hidden states\n                hidden = hidden.view(self.num_layers, self.directions, -1, self.hidden_dim)\n                # Concat forward and backward directions\n                hidden = torch.cat([hidden[:, 0], hidden[:, 1]], dim=2)\n                # Project to the correct dimension\n                hidden = self.projection(hidden)\n\n                # Process cell states\n                cell = cell.view(self.num_layers, self.directions, -1, self.hidden_dim)\n                cell = torch.cat([cell[:, 0], cell[:, 1]], dim=2)\n                cell = self.projection(cell)\n\n                # If outputs is bidirectional, we need to process it too\n                batch_size = outputs.size(0)\n                seq_len = outputs.size(1)\n                # Reshape and project encoder outputs\n                outputs = outputs.contiguous().view(batch_size, seq_len, self.hidden_dim * 2)\n                outputs = self.projection(outputs)\n\n                return outputs, (hidden, cell)\n            return outputs, (hidden, cell)\n        else:\n            outputs, hidden = self.rnn(embedded)\n\n            # Process bidirectional states if needed\n            if self.bidirectional:\n                hidden = hidden.view(self.num_layers, self.directions, -1, self.hidden_dim)\n                hidden = torch.cat([hidden[:, 0], hidden[:, 1]], dim=2)\n                hidden = self.projection(hidden)\n\n                # Process outputs if bidirectional\n                batch_size = outputs.size(0)\n                seq_len = outputs.size(1)\n                # Reshape and project encoder outputs\n                outputs = outputs.contiguous().view(batch_size, seq_len, self.hidden_dim * 2)\n                outputs = self.projection(outputs)\n\n            return outputs, hidden\n\nclass Decoder(nn.Module):\n    def __init__(self, output_dim, emb_dim, hidden_dim, num_layers, rnn_type, dropout=0.5, attention=False):\n        super(Decoder, self).__init__()\n        self.embedding = nn.Embedding(output_dim, emb_dim)\n        self.rnn_type = rnn_type\n        self.num_layers = num_layers\n        self.hidden_dim = hidden_dim\n        self.output_dim = output_dim\n        self.dropout = nn.Dropout(dropout)\n        self.attention = attention\n\n        # Increase input size if using attention\n        rnn_input_size = emb_dim + hidden_dim if attention else emb_dim\n\n        if rnn_type == 'lstm':\n            self.rnn = nn.LSTM(rnn_input_size, hidden_dim, num_layers, batch_first=True,\n                              dropout=dropout if num_layers > 1 else 0)\n        elif rnn_type == 'gru':\n            self.rnn = nn.GRU(rnn_input_size, hidden_dim, num_layers, batch_first=True,\n                             dropout=dropout if num_layers > 1 else 0)\n        else:  # rnn\n            self.rnn = nn.RNN(rnn_input_size, hidden_dim, num_layers, batch_first=True,\n                             dropout=dropout if num_layers > 1 else 0)\n\n        # Attention mechanism\n        if attention:\n            # Attention layers - simplified approach that works with any encoder output dimensionality\n            self.attn = nn.Linear(hidden_dim * 2, hidden_dim)\n            self.v = nn.Linear(hidden_dim, 1, bias=False)\n\n        self.fc_out = nn.Linear(hidden_dim, output_dim)\n\n    def forward(self, input_char, hidden, encoder_outputs=None):\n        input_char = input_char.unsqueeze(1)  # [batch_size, 1]\n        embedded = self.dropout(self.embedding(input_char))  # [batch_size, 1, emb_dim]\n\n        # Apply attention if enabled and encoder_outputs are provided\n        if self.attention and encoder_outputs is not None:\n            # Make sure we're extracting the hidden state correctly based on RNN type\n            if self.rnn_type == 'lstm':\n                query = hidden[0][-1].unsqueeze(1)  # [batch_size, 1, hidden_dim]\n            else:\n                query = hidden[-1].unsqueeze(1)  # [batch_size, 1, hidden_dim]\n\n            # Get dimensions\n            batch_size = encoder_outputs.size(0)\n            src_len = encoder_outputs.size(1)\n\n            # Create energy by combining query with encoder outputs\n            query = query.repeat(1, src_len, 1)  # [batch_size, src_len, hidden_dim]\n\n            # Concatenate query with encoder outputs\n            energy_input = torch.cat((query, encoder_outputs), dim=2)  # [batch_size, src_len, 2*hidden_dim]\n\n            # Calculate attention scores\n            energy = torch.tanh(self.attn(energy_input))  # [batch_size, src_len, hidden_dim]\n            attention = self.v(energy).squeeze(2)  # [batch_size, src_len]\n\n            # Apply softmax to get attention weights\n            attention_weights = torch.softmax(attention, dim=1).unsqueeze(1)  # [batch_size, 1, src_len]\n\n            # Create context vector by applying attention weights to encoder outputs\n            context = torch.bmm(attention_weights, encoder_outputs)  # [batch_size, 1, hidden_dim]\n\n            # Combine with embedding\n            rnn_input = torch.cat((embedded, context), dim=2)  # [batch_size, 1, emb_dim+hidden_dim]\n        else:\n            rnn_input = embedded\n\n        # Forward pass through RNN\n        if self.rnn_type == 'lstm':\n            output, (hidden, cell) = self.rnn(rnn_input, hidden)\n            hidden_state = (hidden, cell)\n        else:\n            output, hidden = self.rnn(rnn_input, hidden)\n            hidden_state = hidden\n\n        # Generate prediction\n        prediction = self.fc_out(output.squeeze(1))  # [batch_size, output_dim]\n\n        # Return attention_weights if available\n        if self.attention and encoder_outputs is not None:\n            return prediction, hidden_state, attention_weights.squeeze(1)  # [batch_size, src_len]\n        else:\n            return prediction, hidden_state, None\n\n\nclass Seq2Seq(nn.Module):\n    def __init__(self, encoder, decoder, rnn_type, device, use_attention=False):\n        super().__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n        self.rnn_type = rnn_type\n        self.device = device\n        self.use_attention = use_attention\n\n    def forward(self, src, trg, teacher_forcing_ratio=0.0):\n        batch_size = src.size(0)\n        trg_len = trg.size(1)\n        trg_vocab_size = self.decoder.output_dim\n\n        outputs = torch.zeros(batch_size, trg_len, trg_vocab_size).to(self.device)\n\n        # For collecting attention weights\n        all_attention_weights = [] if self.use_attention else None\n\n        # Encode the source sequence\n        encoder_outputs, hidden = self._encode(src)\n\n        # Use the first token as input to start decoding\n        input_char = trg[:, 0]  # <sos> token\n\n        for t in range(1, trg_len):\n            # Generate output from decoder\n            if self.use_attention:\n                output, hidden, attn_weights  = self.decoder(input_char, hidden, encoder_outputs)\n                all_attention_weights.append(attn_weights)  # <-- ADD THIS LINE\n            else:\n                output, hidden, _ = self.decoder(input_char, hidden)\n\n            outputs[:, t] = output\n\n            # Teacher forcing: use real target or predicted token\n            teacher_force = torch.rand(1).item() < teacher_forcing_ratio\n            top1 = output.argmax(1)\n            input_char = trg[:, t] if teacher_force else top1\n\n        return outputs, all_attention_weights\n\n    def _encode(self, src):\n        # Get encoder outputs and final hidden state\n        encoder_outputs, hidden = self.encoder(src)\n\n        # Adjust hidden state dimensions if encoder and decoder have different layers\n        encoder_layers = self.encoder.num_layers\n        decoder_layers = self.decoder.num_layers\n\n        if self.rnn_type == 'lstm':\n            hidden_state, cell_state = hidden\n\n            # If encoder has fewer layers than decoder, pad with zeros\n            if encoder_layers < decoder_layers:\n                padding = torch.zeros(\n                    decoder_layers - encoder_layers,\n                    hidden_state.size(1),\n                    hidden_state.size(2)\n                ).to(self.device)\n                hidden_state = torch.cat([hidden_state, padding], dim=0)\n                cell_state = torch.cat([cell_state, padding], dim=0)\n\n            # If encoder has more layers than decoder, truncate\n            elif encoder_layers > decoder_layers:\n                hidden_state = hidden_state[:decoder_layers]\n                cell_state = cell_state[:decoder_layers]\n\n            # Make sure hidden dimensions match decoder's expected dimensions\n            if hidden_state.size(2) != self.decoder.hidden_dim:\n                # Project hidden state to the decoder's dimension using a linear projection\n                batch_size = hidden_state.size(1)\n                proj_hidden = torch.zeros(\n                    hidden_state.size(0),\n                    batch_size,\n                    self.decoder.hidden_dim\n                ).to(self.device)\n\n                for layer in range(hidden_state.size(0)):\n                    # Simple linear projection for each layer\n                    proj_hidden[layer] = hidden_state[layer].clone().view(batch_size, -1)[:, :self.decoder.hidden_dim]\n\n                # Apply the same projection to cell state\n                proj_cell = torch.zeros_like(proj_hidden)\n                for layer in range(cell_state.size(0)):\n                    proj_cell[layer] = cell_state[layer].clone().view(batch_size, -1)[:, :self.decoder.hidden_dim]\n\n                hidden = (proj_hidden, proj_cell)\n            else:\n                hidden = (hidden_state, cell_state)\n        else:\n            # For GRU and RNN\n            # If encoder has fewer layers than decoder, pad with zeros\n            if encoder_layers < decoder_layers:\n                padding = torch.zeros(\n                    decoder_layers - encoder_layers,\n                    hidden.size(1),\n                    hidden.size(2)\n                ).to(self.device)\n                hidden = torch.cat([hidden, padding], dim=0)\n            # If encoder has more layers than decoder, truncate\n            elif encoder_layers > decoder_layers:\n                hidden = hidden[:decoder_layers]\n\n            # Make sure hidden dimensions match decoder's expected dimensions\n            if hidden.size(2) != self.decoder.hidden_dim:\n                # Project hidden state to the decoder's dimension\n                batch_size = hidden.size(1)\n                proj_hidden = torch.zeros(\n                    hidden.size(0),\n                    batch_size,\n                    self.decoder.hidden_dim\n                ).to(self.device)\n\n                for layer in range(hidden.size(0)):\n                    # Simple linear projection for each layer\n                    proj_hidden[layer] = hidden[layer].clone().view(batch_size, -1)[:, :self.decoder.hidden_dim]\n\n                hidden = proj_hidden\n\n        return encoder_outputs, hidden\n\n# Character-level vocabulary builder\ndef build_vocab(tokens):\n    vocab = {'<pad>': 0, '<sos>': 1, '<eos>': 2, '<unk>': 3}\n    for token in tokens:\n        for char in token:\n            if char not in vocab:\n                vocab[char] = len(vocab)\n    return vocab\n\ndef encode_sequence(seq, vocab):\n    return [vocab.get(char, vocab['<unk>']) for char in seq]\n\nclass DakshinaDataset(Dataset):\n    def __init__(self, data_path, latin_vocab=None, devanagari_vocab=None):\n        self.latin_words = []\n        self.devanagari_words = []\n\n        # Group all transliterations by Devanagari word\n        candidates = defaultdict(list)\n\n        with open(data_path, encoding='utf-8') as f:\n            for line in f:\n                parts = line.strip().split('\\t')\n                if len(parts) != 3:\n                    continue\n                native, latin, rel = parts[0], parts[1], int(parts[2])\n                candidates[native].append((latin, rel))\n\n        # Keep only the transliteration(s) with highest score for each native word\n        for native, translits in candidates.items():\n            max_rel = max(rel for _, rel in translits)\n            for latin, rel in translits:\n                if rel == max_rel:\n                    self.latin_words.append(latin)\n                    self.devanagari_words.append(native)\n\n        print(f\"Dataset from {data_path}: {len(self.latin_words)} pairs.\")\n\n        self.latin_vocab = latin_vocab or build_vocab(self.latin_words)\n        self.devanagari_vocab = devanagari_vocab or build_vocab(self.devanagari_words)\n\n    def __len__(self):\n        return len(self.latin_words)\n\n    def __getitem__(self, idx):\n        src_seq = encode_sequence(self.latin_words[idx], self.latin_vocab)\n        trg_seq = encode_sequence(self.devanagari_words[idx], self.devanagari_vocab)\n        # add <sos> and <eos> tokens for target sequences\n        trg_seq = [self.devanagari_vocab['<sos>']] + trg_seq + [self.devanagari_vocab['<eos>']]\n        return src_seq, trg_seq\n\ndef collate_fn(batch):\n    # batch is a list of tuples (src_seq, trg_seq)\n    src_seqs, trg_seqs = zip(*batch)\n\n    # find max lengths\n    max_src_len = max(len(seq) for seq in src_seqs)\n    max_trg_len = max(len(seq) for seq in trg_seqs)\n\n    # pad sequences\n    src_padded = [seq + [0]*(max_src_len - len(seq)) for seq in src_seqs]\n    trg_padded = [seq + [0]*(max_trg_len - len(seq)) for seq in trg_seqs]\n\n    # convert to tensors\n    src_tensor = torch.tensor(src_padded, dtype=torch.long)\n    trg_tensor = torch.tensor(trg_padded, dtype=torch.long)\n\n    return src_tensor, trg_tensor\n\ndef compute_word_accuracy(preds, trg, pad_idx, sos_idx=1, eos_idx=2):\n    \"\"\"\n    Compute word-level accuracy: a word is correct only if all tokens match (excluding pad, sos, eos).\n    preds, trg: [batch_size, seq_len]\n    \"\"\"\n    batch_size = preds.size(0)\n    correct = 0\n\n    for i in range(batch_size):\n        # Get sequence for this example (exclude pad, sos, eos tokens)\n        pred_seq = [idx for idx in preds[i].tolist() if idx not in [pad_idx, sos_idx, eos_idx]]\n        trg_seq = [idx for idx in trg[i].tolist() if idx not in [pad_idx, sos_idx, eos_idx]]\n\n        # Compare full sequences (exact match)\n        if pred_seq == trg_seq:\n            correct += 1\n\n    return correct, batch_size\n\ndef beam_search(model, src_seq, src_vocab, tgt_vocab, beam_width=3, max_len=20):\n    model.eval()\n    index_to_char = {v: k for k, v in tgt_vocab.items()}\n    device = model.device\n\n    # Prepare input\n    src_indices = encode_sequence(src_seq, src_vocab)\n    src_tensor = torch.tensor([src_indices], dtype=torch.long).to(device)\n\n    # Get encoder outputs and hidden state\n    encoder_outputs, hidden = model._encode(src_tensor)\n\n    # Start with start-of-sequence token\n    beams = [([tgt_vocab['<sos>']], 0.0, hidden)]\n\n    for _ in range(max_len):\n        new_beams = []\n        for seq, score, hidden in beams:\n            last_token = torch.tensor([seq[-1]], dtype=torch.long).to(device)\n\n            # Use attention if model has it\n            if model.use_attention:\n                output, new_hidden, _ = model.decoder(last_token, hidden, encoder_outputs)\n            else:\n                output, new_hidden = model.decoder(last_token, hidden)\n\n            log_probs = torch.log_softmax(output, dim=-1)\n            topk = torch.topk(log_probs, beam_width)\n\n            for prob, idx in zip(topk.values[0], topk.indices[0]):\n                new_seq = seq + [idx.item()]\n                new_score = score + prob.item()\n                new_beams.append((new_seq, new_score, new_hidden))\n\n        beams = sorted(new_beams, key=lambda x: x[1], reverse=True)[:beam_width]\n\n        # Stop if all beams end with EOS\n        if all(seq[-1] == tgt_vocab['<eos>'] for seq, _, _ in beams):\n            break\n\n    # Pick the best beam\n    best_seq = beams[0][0]\n    # Remove special tokens for output\n    decoded = [index_to_char[i] for i in best_seq if i not in {tgt_vocab['<sos>'], tgt_vocab['<eos>'], tgt_vocab['<pad>']}]\n    return ''.join(decoded)\n\ndef train(model, dataloader, optimizer, criterion, clip=1, teacher_forcing_ratio=0.0):\n    model.train()\n    epoch_loss = 0\n    total_words = 0\n    correct_words = 0\n\n    pad_idx = 0  # Pad index in vocabulary\n    sos_idx = 1  # Start of sequence index\n    eos_idx = 2  # End of sequence index\n\n    for src, trg in dataloader:\n        src, trg = src.to(model.device), trg.to(model.device)\n        optimizer.zero_grad()\n\n        # Generate sequence\n        output, _ = model(src, trg, teacher_forcing_ratio=teacher_forcing_ratio)\n        output_dim = output.shape[-1]\n\n        # Ignore first token (<sos>) in loss calculation\n        output = output[:, 1:].contiguous().view(-1, output_dim)\n        trg_flat = trg[:, 1:].contiguous().view(-1)\n\n        loss = criterion(output, trg_flat)\n        loss.backward()\n\n        # Use gradient clipping\n        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n        optimizer.step()\n\n        epoch_loss += loss.item()\n\n        # Calculate word accuracy\n        pred_tokens = output.argmax(1).view(trg[:, 1:].shape)  # [batch_size, trg_len-1]\n        trg_trimmed = trg[:, 1:]                             # [batch_size, trg_len-1]\n\n        correct, total = compute_word_accuracy(pred_tokens, trg_trimmed, pad_idx, sos_idx, eos_idx)\n        correct_words += correct\n        total_words += total\n\n    avg_loss = epoch_loss / len(dataloader)\n    word_acc = correct_words / total_words if total_words > 0 else 0\n\n    return avg_loss, word_acc * 100\n\ndef evaluate(model, dataloader, criterion):\n    model.eval()\n    epoch_loss = 0\n    total_words = 0\n    correct_words = 0\n\n    pad_idx = 0  # Pad index in vocabulary\n    sos_idx = 1  # Start of sequence index\n    eos_idx = 2  # End of sequence index\n\n    with torch.no_grad():\n        for src, trg in dataloader:\n            src, trg = src.to(model.device), trg.to(model.device)\n\n            # Generate full sequence with no teacher forcing\n            output, _  = model(src, trg, teacher_forcing_ratio=0.0)\n            # Visualize for the first example in batch\n            # attn = torch.stack([aw[0] for aw in attention_weights]).cpu().numpy()\n            output_dim = output.shape[-1]\n\n            # Ignore first token (<sos>) in loss calculation\n            output = output[:, 1:].contiguous().view(-1, output_dim)\n            trg_flat = trg[:, 1:].contiguous().view(-1)\n\n            loss = criterion(output, trg_flat)\n            epoch_loss += loss.item()\n\n            # Calculate word accuracy\n            pred_tokens = output.argmax(1).view(trg[:, 1:].shape)\n            trg_trimmed = trg[:, 1:]\n\n            correct, total = compute_word_accuracy(pred_tokens, trg_trimmed, pad_idx, sos_idx, eos_idx)\n            correct_words += correct\n            total_words += total\n\n    avg_loss = epoch_loss / len(dataloader)\n    word_acc = correct_words / total_words if total_words > 0 else 0\n\n    return avg_loss, word_acc * 100\n\ndef predict_examples(model, dataloader, latin_index_to_token, devanagari_index_to_token, n=5):\n    \"\"\"Show a few examples of model predictions vs actual targets\"\"\"\n    model.eval()\n    pad_idx = 0\n    sos_idx = 1  # Start of sequence\n    eos_idx = 2  # End of sequence\n    count = 0\n    results = []\n\n    print(\"\\nPrediction Examples:\")\n    print(\"-\" * 60)\n\n    with torch.no_grad():\n        for src, trg in dataloader:\n            src, trg = src.to(model.device), trg.to(model.device)\n            output, _ = model(src, trg, teacher_forcing_ratio=0.0)\n            # print(len(output))\n            pred_tokens = output.argmax(-1)  # [batch_size, seq_len]\n\n            for i in range(min(src.size(0), n - count)):\n                # Decode input\n                input_indices = [idx for idx in src[i].tolist() if idx != pad_idx]\n                input_tokens = [latin_index_to_token.get(idx, '<unk>') for idx in input_indices]\n                input_text = \"\".join(input_tokens)\n\n                # Decode target\n                target_indices = [idx for idx in trg[i].tolist() if idx not in [pad_idx, sos_idx, eos_idx]]\n                target_tokens = [devanagari_index_to_token.get(idx, '<unk>') for idx in target_indices]\n                target_text = \"\".join(target_tokens)\n\n                # Decode prediction\n                pred_indices = [idx for idx in pred_tokens[i].tolist() if idx not in [pad_idx, sos_idx, eos_idx]]\n                pred_tokens_text = [devanagari_index_to_token.get(idx, '<unk>') for idx in pred_indices]\n                pred_text = \"\".join(pred_tokens_text)\n\n                result = {\n                    \"input\": input_text,\n                    \"target\": target_text,\n                    \"prediction\": pred_text,\n                    \"correct\": pred_text == target_text\n                }\n                results.append(result)\n\n                print(f\"Input:     {input_text}\")\n                print(f\"Target:    {target_text}\")\n                print(f\"Predicted: {pred_text}\")\n                print(\"-\" * 60)\n\n                count += 1\n                if count >= n:\n                    break\n            if count >= n:\n                break\n\n    return results\n    \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Define sweep configuration with improved parameters\ndef get_sweep_config():\n    sweep_config = {\n        'method': 'random',\n        'metric': {\n            'name': 'val_accuracy',\n            'goal': 'maximize'},\n            'parameters': {\n            'embed_dim': {'values': [128,256,384]},\n            'hidden_dim': {'values': [256,384,512]},\n            'rnn_type': {'values': ['lstm','gru']}, \n            'encoder_layers': {'values': [3,2]},\n            'decoder_layers': {'values': [3,2]},\n            'dropout': {'values': [0.2,0.3]},\n            'learning_rate': {'values': [0.001,1e-4]},\n            'batch_size': {'values': [128,256]},\n            'epochs': {'values': [5,10]},\n            'beam_size': {'values': [3,5]},\n            'use_attention': {'values': [True]},\n            'bidirectional': {'values': [True]},\n            'teacher_forcing_ratio': {'values': [0.0, 0.3]},\n            'weight_decay': {'values': [1e-5, 1e-6]}\n        }\n    }\n    return sweep_config\n\n# Main training function for sweep runs\ndef train_sweep():\n    # Initialize wandb with sweep configuration\n    run = wandb.init(project=\"transliteration-model\")\n\n    # Access hyperparameters from wandb.config\n    config = run.config\n\n    # Setup device\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    print(f\"Using device: {device}\")\n\n    # Define data paths (adjust for Kaggle environment)\n    data_dir = '/kaggle/input/dakshina-dataset/dakshina_dataset_v1.0/ta/lexicons/'\n    train_path = os.path.join(data_dir, 'ta.translit.sampled.train.tsv')\n    dev_path = os.path.join(data_dir, 'ta.translit.sampled.dev.tsv')\n    test_path = os.path.join(data_dir, 'ta.translit.sampled.test.tsv')\n\n    # Check if files exist\n    if not os.path.exists(train_path):\n        raise FileNotFoundError(f\"Could not find training data at {train_path}. Please check the path.\")\n\n    # Load datasets\n    print(\"Loading training dataset...\")\n    train_dataset = DakshinaDataset(train_path)\n    latin_vocab = train_dataset.latin_vocab\n    devanagari_vocab = train_dataset.devanagari_vocab\n\n    print(\"Loading validation dataset...\")\n    val_dataset = DakshinaDataset(\n        dev_path,\n        latin_vocab=latin_vocab,\n        devanagari_vocab=devanagari_vocab\n    )\n\n    print(\"Loading test dataset...\")\n    test_dataset = DakshinaDataset(\n        test_path,\n        latin_vocab=latin_vocab,\n        devanagari_vocab=devanagari_vocab\n    )\n\n    # Create DataLoaders\n    train_loader = DataLoader(\n        train_dataset,\n        batch_size=config.batch_size,\n        shuffle=True,\n        collate_fn=collate_fn\n    )\n\n    val_loader = DataLoader(\n        val_dataset,\n        batch_size=config.batch_size,\n        shuffle=False,\n        collate_fn=collate_fn\n    )\n\n    test_loader = DataLoader(\n        test_dataset,\n        batch_size=config.batch_size,\n        shuffle=False,\n        collate_fn=collate_fn\n    )\n\n    # Get vocabulary information\n    latin_vocab_size = len(latin_vocab)\n    devanagari_vocab_size = len(devanagari_vocab)\n    pad_idx = devanagari_vocab['<pad>']\n\n    # Log vocabulary sizes\n    wandb.log({\"latin_vocab_size\": latin_vocab_size, \"devanagari_vocab_size\": devanagari_vocab_size})\n\n    # Generate a model name based on hyperparameters\n    model_name = f\"{config.rnn_type}_ed{config.embed_dim}_hid{config.hidden_dim}_enc{config.encoder_layers}_dec{config.decoder_layers}_attn{config.use_attention}_drop{config.dropout}\"\n    wandb.run.name = model_name\n\n    # Create model architecture\n    encoder = Encoder(\n        input_dim=latin_vocab_size,\n        emb_dim=config.embed_dim,\n        hidden_dim=config.hidden_dim,\n        num_layers=config.encoder_layers,\n        rnn_type=config.rnn_type,\n        dropout=config.dropout,\n        bidirectional=config.bidirectional\n    )\n\n    decoder = Decoder(\n        output_dim=devanagari_vocab_size,\n        emb_dim=config.embed_dim,\n        hidden_dim=config.hidden_dim,\n        num_layers=config.decoder_layers,\n        rnn_type=config.rnn_type,\n        dropout=config.dropout,\n        attention=config.use_attention\n    )\n\n    model = Seq2Seq(\n        encoder,\n        decoder,\n        rnn_type=config.rnn_type,\n        device=device,\n        use_attention=config.use_attention\n    ).to(device)\n\n    # Count and log the number of model parameters\n    total_params = sum(p.numel() for p in model.parameters())\n    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n    wandb.log({\n        \"total_parameters\": total_params,\n        \"trainable_parameters\": trainable_params\n    })\n    print(f\"Model: {model_name}\")\n    print(f\"Total parameters: {total_params}\")\n    print(f\"Trainable parameters: {trainable_params}\")\n\n    # Setup optimizer and loss function with weight decay for regularization\n    optimizer = torch.optim.Adam(\n        model.parameters(),\n        lr=config.learning_rate,\n        weight_decay=config.weight_decay\n    )\n\n    # Learning rate scheduler\n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n        optimizer,\n        mode='max',\n        factor=0.5,\n        patience=2,\n        verbose=True\n    )\n\n    criterion = torch.nn.CrossEntropyLoss(ignore_index=pad_idx)\n\n    # Training loop with early stopping\n    best_val_loss = float('inf')\n    best_val_acc = 0\n    patience = 5  # Increased patience\n    patience_counter = 0\n\n    # Save directory for models\n    model_dir = '/kaggle/working/models'\n    os.makedirs(model_dir, exist_ok=True)\n\n    # Track metrics for each epoch\n    for epoch in range(config.epochs):\n        print(f\"\\nEpoch {epoch+1}/{config.epochs}\")\n\n        # Train\n        train_loss, train_acc = train(model, train_loader, optimizer, criterion, clip=1.0,\n                                      teacher_forcing_ratio=config.teacher_forcing_ratio)\n        print(f\"Train Loss: {train_loss:.4f}, Accuracy: {train_acc:.2f}%\")\n\n        # Validate\n        val_loss, val_acc = evaluate(model, val_loader, criterion)\n        print(f\"Val Loss: {val_loss:.4f}, Accuracy: {val_acc:.2f}%\")\n\n        # Log metrics to wandb\n        wandb.log({\n            \"epoch\": epoch + 1,\n            \"train_loss\": train_loss,\n            \"train_accuracy\": train_acc,\n            \"val_loss\": val_loss,\n            \"val_accuracy\": val_acc\n        })\n\n        # Save best model and check for early stopping\n        if val_acc > best_val_acc:\n            best_val_acc = val_acc\n            best_val_loss = val_loss\n            patience_counter = 0\n\n            # Save the best model\n            best_model_path = os.path.join(model_dir, f\"{model_name}_best.pt\")\n            torch.save(model.state_dict(), best_model_path)\n            print(f\"New best model saved with val accuracy: {val_acc:.2f}%\")\n        else:\n            patience_counter += 1\n            if patience_counter >= patience:\n                print(f\"Early stopping after {epoch+1} epochs\")\n                break\n\n    # Load the best model for testing\n    best_model_path = os.path.join(model_dir, f\"{model_name}_best.pt\")\n    try:\n        model.load_state_dict(torch.load(best_model_path))\n        print(\"Loaded best model for testing\")\n    except:\n        print(\"Using current model for testing (best model not found)\")\n\n    # Final evaluation on test set\n    test_loss, test_acc = evaluate(model, test_loader, criterion)\n    print(f\"\\nTest Results -> Loss: {test_loss:.4f}, Accuracy: {test_acc:.2f}%\")\n\n    # Log final test metrics\n    wandb.log({\n        \"test_loss\": test_loss,\n        \"test_accuracy\": test_acc\n    })\n\n    # Create index-to-token dictionaries for prediction display\n    latin_index_to_token = {idx: token for token, idx in latin_vocab.items()}\n    devanagari_index_to_token = {idx: token for token, idx in devanagari_vocab.items()}\n\n    # Generate prediction examples for visualization\n    example_results = predict_examples(\n        model,\n        test_loader,\n        latin_index_to_token,\n        devanagari_index_to_token,\n        n=5\n    )\n\n    # Log the examples as a table in wandb\n    example_table = wandb.Table(\n        columns=[\"Input\", \"Target\", \"Prediction\", \"Correct\"]\n    )\n    for result in example_results:\n        example_table.add_data(\n            result[\"input\"],\n            result[\"target\"],\n            result[\"prediction\"],\n            result[\"correct\"]\n        )\n    wandb.log({\"prediction_examples\": example_table})\n\n    # Test beam search if enabled\n    if config.beam_size > 1:\n        print(f\"\\nTesting beam search with beam width {config.beam_size}...\")\n        beam_correct = 0\n        beam_total = 0\n\n        for src, trg in test_loader:\n            src = src.to(device)\n            trg = trg.to(device)\n            for i in range(min(5, src.size(0))):  # Test beam search on a few examples\n                # Get input sequence\n                src_seq = [latin_index_to_token[idx] for idx in src[i].tolist() if idx != pad_idx]\n                src_text = ''.join(src_seq)\n\n                # Get target sequence\n                trg_indices = [idx for idx in trg[i].tolist() if idx not in [pad_idx, 1]]  # Remove <pad> and <sos>\n                trg_text = ''.join([devanagari_index_to_token.get(idx, '<unk>') for idx in trg_indices])\n\n                # Run beam search\n                beam_pred = beam_search(\n                    model,\n                    src_text,\n                    latin_vocab,\n                    devanagari_vocab,\n                    beam_width=config.beam_size,\n                    max_len=30\n                )\n\n                beam_correct += 1 if beam_pred == trg_text else 0\n                beam_total += 1\n\n                # print(f\"Input: {src_text}\")\n                # print(f\"Target: {trg_text}\")\n                # print(f\"Beam Pred: {beam_pred}\")\n                # print(\"-\" * 60)\n\n        beam_acc = beam_correct / beam_total * 100 if beam_total > 0 else 0\n        print(f\"Beam search accuracy: {beam_acc:.2f}%\")\n        wandb.log({\"beam_search_accuracy\": beam_acc})\n        \n    plot_attention_grid(model, test_loader, idx_to_src_token=latin_index_to_token, idx_to_tgt_token=devanagari_index_to_token)\n\n    return model, latin_vocab, devanagari_vocab\n\n\nimport torch\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom matplotlib import font_manager\n\ndef plot_attention_grid(model, dataloader, idx_to_src_token=None, idx_to_tgt_token=None, num_samples=9):\n    model.eval()\n    \n    # Load Tamil font (update the path as per your font location)\n    tamil_font_path = \"/kaggle/input/notations-tamil/static/NotoSansTamil-Regular.ttf\"  # Put this font file in the working dir or give full path\n    tamil_font = font_manager.FontProperties(fname=tamil_font_path)\n\n    samples_plotted = 0\n    fig, axes = plt.subplots(3, 3, figsize=(15, 15))\n    axes = axes.flatten()\n\n    with torch.no_grad():\n        for src, trg in dataloader:\n            src, trg = src.to(model.device), trg.to(model.device)\n\n            # Forward pass with no teacher forcing to get attention\n            output, attention_weights = model(src, trg, teacher_forcing_ratio=0.0)\n\n            if not attention_weights:\n                print(\"No attention weights returned by the model.\")\n                return\n            \n            # Shape: [tgt_len-1, batch_size, src_len]\n            attn_tensor = torch.stack(attention_weights)\n\n            batch_size = src.size(0)\n\n            for i in range(batch_size):\n                if samples_plotted >= num_samples:\n                    break\n\n                # Tokens\n                if idx_to_src_token and idx_to_tgt_token:\n                    src_tokens = [idx_to_src_token[idx.item()] for idx in src[i]]\n                    tgt_tokens = [idx_to_tgt_token[idx.item()] for idx in trg[i][1:]]  # skip <sos>\n                else:\n                    src_tokens = None\n                    tgt_tokens = None\n\n                # Remove <pad> tokens and track lengths\n                src_tokens_clean = [tok for tok in src_tokens if tok != \"<pad>\"]\n                tgt_tokens_clean = [tok for tok in tgt_tokens if tok != \"<pad>\"]\n                src_trim_len = len(src_tokens_clean)\n                tgt_trim_len = len(tgt_tokens_clean)\n\n                # Trim attention to match cleaned tokens\n                attn_for_sample = attn_tensor[:, i, :].cpu().numpy()  # [tgt_len-1, src_len]\n                attn_for_sample = attn_for_sample[:tgt_trim_len, :src_trim_len]\n\n                # Plot\n                ax = axes[samples_plotted]\n                sns.heatmap(attn_for_sample, cmap=\"viridis\",\n                            xticklabels=src_tokens_clean,\n                            yticklabels=tgt_tokens_clean,\n                            ax=ax)\n                ax.set_xlabel(\"Source Tokens\", fontproperties=tamil_font)\n                ax.set_ylabel(\"Target Tokens\", fontproperties=tamil_font)\n                ax.set_title(f\"Sample {samples_plotted + 1}\", fontproperties=tamil_font)\n                ax.tick_params(axis='x', rotation=45)\n                ax.tick_params(axis='y', rotation=0)\n\n                # Apply Tamil font to tick labels\n                for label in ax.get_xticklabels() + ax.get_yticklabels():\n                    label.set_fontproperties(tamil_font)\n\n                samples_plotted += 1\n\n            if samples_plotted >= num_samples:\n                break\n    plt.savefig(\"heatmap.png\", dpi = 300)\n    # wandb.log(\"heatmap.png\")\n    wandb.log({\"attention_heatmap\": wandb.Image(\"heatmap.png\")})\n    plt.tight_layout()\n    plt.show()\n\n\n# Entry point - runs a wandb sweep\ndef run_wandb_sweep():\n    sweep_config = get_sweep_config()\n    sweep_id = wandb.sweep(sweep_config, project=\"transliteration-model-tam1\")\n    wandb.agent(sweep_id, train_sweep, count=1)\n\n\n# Main execution block for Kaggle\nif __name__ == \"__main__\":\n    # Run the wandb sweep\n    run_wandb_sweep()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T06:44:14.440594Z","iopub.execute_input":"2025-05-20T06:44:14.441293Z","iopub.status.idle":"2025-05-20T06:46:29.175598Z","shell.execute_reply.started":"2025-05-20T06:44:14.441259Z","shell.execute_reply":"2025-05-20T06:46:29.174529Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mma23m007\u001b[0m (\u001b[33mma23m007-iit-madras\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"name":"stdout","text":"Create sweep with ID: u2rroxuh\nSweep URL: https://wandb.ai/ma23m007-iit-madras/transliteration-model/sweeps/u2rroxuh\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: i6wco6sc with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_size: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbidirectional: True\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdecoder_layers: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembed_dim: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \tencoder_layers: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dim: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n\u001b[34m\u001b[1mwandb\u001b[0m: \trnn_type: lstm\n\u001b[34m\u001b[1mwandb\u001b[0m: \tteacher_forcing_ratio: 0\n\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_attention: True\n\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 1e-06\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Ignoring project 'transliteration-model' when running a sweep."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250520_064436-i6wco6sc</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/ma23m007-iit-madras/transliteration-model/runs/i6wco6sc' target=\"_blank\">fast-sweep-1</a></strong> to <a href='https://wandb.ai/ma23m007-iit-madras/transliteration-model' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/ma23m007-iit-madras/transliteration-model/sweeps/u2rroxuh' target=\"_blank\">https://wandb.ai/ma23m007-iit-madras/transliteration-model/sweeps/u2rroxuh</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/ma23m007-iit-madras/transliteration-model' target=\"_blank\">https://wandb.ai/ma23m007-iit-madras/transliteration-model</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/ma23m007-iit-madras/transliteration-model/sweeps/u2rroxuh' target=\"_blank\">https://wandb.ai/ma23m007-iit-madras/transliteration-model/sweeps/u2rroxuh</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/ma23m007-iit-madras/transliteration-model/runs/i6wco6sc' target=\"_blank\">https://wandb.ai/ma23m007-iit-madras/transliteration-model/runs/i6wco6sc</a>"},"metadata":{}},{"name":"stdout","text":"Using device: cuda\nLoading training dataset...\nDataset from /kaggle/input/dakshina-dataset/dakshina_dataset_v1.0/ta/lexicons/ta.translit.sampled.train.tsv: 38716 pairs.\nLoading validation dataset...\nDataset from /kaggle/input/dakshina-dataset/dakshina_dataset_v1.0/ta/lexicons/ta.translit.sampled.dev.tsv: 4054 pairs.\nLoading test dataset...\nDataset from /kaggle/input/dakshina-dataset/dakshina_dataset_v1.0/ta/lexicons/ta.translit.sampled.test.tsv: 3938 pairs.\nModel: lstm_ed128_hid256_enc3_dec3_attnTrue_drop0.2\nTotal parameters: 5940530\nTrainable parameters: 5940530\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 1/3\nTrain Loss: 2.5324, Accuracy: 0.02%\nVal Loss: 2.0287, Accuracy: 0.07%\nNew best model saved with val accuracy: 0.07%\n\nEpoch 2/3\nTrain Loss: 1.5166, Accuracy: 4.12%\nVal Loss: 1.0786, Accuracy: 14.04%\nNew best model saved with val accuracy: 14.04%\n\nEpoch 3/3\nTrain Loss: 0.7838, Accuracy: 22.28%\nVal Loss: 0.6618, Accuracy: 18.60%\nNew best model saved with val accuracy: 18.60%\nLoaded best model for testing\n\nTest Results -> Loss: 0.6707, Accuracy: 17.72%\n\nPrediction Examples:\n------------------------------------------------------------\nInput:     farm\nTarget:    à®ƒà®ªà®¾à®°à¯à®®à¯\nPredicted: à®ªà®¾à®°à¯à¯\n------------------------------------------------------------\nInput:     face\nTarget:    à®ƒà®ªà¯‡à®¸à¯\nPredicted: à®ªà¯‡à®šà¯à¯à¯à¯à¯à¯à¯\n------------------------------------------------------------\nInput:     aeathimuka\nTarget:    à®…à®‡à®…à®¤à®¿à®®à¯à®•\nPredicted: à®à®¤à®¿à®®à¯à®•\n------------------------------------------------------------\nInput:     aiathimuka\nTarget:    à®…à®‡à®…à®¤à®¿à®®à¯à®•\nPredicted: à®†à®¤à®¿à®®à¯à®•\n------------------------------------------------------------\nInput:     ayiathimuka\nTarget:    à®…à®‡à®…à®¤à®¿à®®à¯à®•\nPredicted: à®…à®¯à®¿à®¤à®¿à®¤à¯à®•\n------------------------------------------------------------\n\nTesting beam search with beam width 3...\nBeam search accuracy: 0.00%\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2947 (\\N{TAMIL SIGN VISARGA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Matplotlib currently does not support Tamil natively.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2986 (\\N{TAMIL LETTER PA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3006 (\\N{TAMIL VOWEL SIGN AA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2992 (\\N{TAMIL LETTER RA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3021 (\\N{TAMIL SIGN VIRAMA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2990 (\\N{TAMIL LETTER MA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3015 (\\N{TAMIL VOWEL SIGN EE}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Matplotlib currently does not support Tamil natively.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3000 (\\N{TAMIL LETTER SA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2947 (\\N{TAMIL SIGN VISARGA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2986 (\\N{TAMIL LETTER PA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3021 (\\N{TAMIL SIGN VIRAMA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2949 (\\N{TAMIL LETTER A}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Matplotlib currently does not support Tamil natively.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2951 (\\N{TAMIL LETTER I}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2980 (\\N{TAMIL LETTER TA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3007 (\\N{TAMIL VOWEL SIGN I}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3009 (\\N{TAMIL VOWEL SIGN U}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2965 (\\N{TAMIL LETTER KA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2990 (\\N{TAMIL LETTER MA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2949 (\\N{TAMIL LETTER A}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Matplotlib currently does not support Tamil natively.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2951 (\\N{TAMIL LETTER I}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2980 (\\N{TAMIL LETTER TA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3007 (\\N{TAMIL VOWEL SIGN I}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2990 (\\N{TAMIL LETTER MA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3009 (\\N{TAMIL VOWEL SIGN U}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2965 (\\N{TAMIL LETTER KA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2949 (\\N{TAMIL LETTER A}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Matplotlib currently does not support Tamil natively.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2951 (\\N{TAMIL LETTER I}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2980 (\\N{TAMIL LETTER TA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3007 (\\N{TAMIL VOWEL SIGN I}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2990 (\\N{TAMIL LETTER MA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3009 (\\N{TAMIL VOWEL SIGN U}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2965 (\\N{TAMIL LETTER KA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2949 (\\N{TAMIL LETTER A}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Matplotlib currently does not support Tamil natively.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2965 (\\N{TAMIL LETTER KA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2980 (\\N{TAMIL LETTER TA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3021 (\\N{TAMIL SIGN VIRAMA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3007 (\\N{TAMIL VOWEL SIGN I}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3008 (\\N{TAMIL VOWEL SIGN II}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Matplotlib currently does not support Tamil natively.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3016 (\\N{TAMIL VOWEL SIGN AI}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2949 (\\N{TAMIL LETTER A}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2965 (\\N{TAMIL LETTER KA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2980 (\\N{TAMIL LETTER TA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3021 (\\N{TAMIL SIGN VIRAMA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3007 (\\N{TAMIL VOWEL SIGN I}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2992 (\\N{TAMIL LETTER RA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2949 (\\N{TAMIL LETTER A}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Matplotlib currently does not support Tamil natively.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2965 (\\N{TAMIL LETTER KA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2990 (\\N{TAMIL LETTER MA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2980 (\\N{TAMIL LETTER TA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3006 (\\N{TAMIL VOWEL SIGN AA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2986 (\\N{TAMIL LETTER PA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3021 (\\N{TAMIL SIGN VIRAMA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2994 (\\N{TAMIL LETTER LA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Matplotlib currently does not support Tamil natively.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2949 (\\N{TAMIL LETTER A}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2965 (\\N{TAMIL LETTER KA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2990 (\\N{TAMIL LETTER MA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2980 (\\N{TAMIL LETTER TA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3006 (\\N{TAMIL VOWEL SIGN AA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2986 (\\N{TAMIL LETTER PA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3021 (\\N{TAMIL SIGN VIRAMA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3007 (\\N{TAMIL VOWEL SIGN I}) missing from current font.\n  fig.canvas.draw()\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>beam_search_accuracy</td><td>â–</td></tr><tr><td>devanagari_vocab_size</td><td>â–</td></tr><tr><td>epoch</td><td>â–â–…â–ˆ</td></tr><tr><td>latin_vocab_size</td><td>â–</td></tr><tr><td>test_accuracy</td><td>â–</td></tr><tr><td>test_loss</td><td>â–</td></tr><tr><td>total_parameters</td><td>â–</td></tr><tr><td>train_accuracy</td><td>â–â–‚â–ˆ</td></tr><tr><td>train_loss</td><td>â–ˆâ–„â–</td></tr><tr><td>trainable_parameters</td><td>â–</td></tr><tr><td>val_accuracy</td><td>â–â–†â–ˆ</td></tr><tr><td>val_loss</td><td>â–ˆâ–ƒâ–</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>beam_search_accuracy</td><td>0</td></tr><tr><td>devanagari_vocab_size</td><td>50</td></tr><tr><td>epoch</td><td>3</td></tr><tr><td>latin_vocab_size</td><td>30</td></tr><tr><td>test_accuracy</td><td>17.72473</td></tr><tr><td>test_loss</td><td>0.67065</td></tr><tr><td>total_parameters</td><td>5940530</td></tr><tr><td>train_accuracy</td><td>22.28019</td></tr><tr><td>train_loss</td><td>0.78376</td></tr><tr><td>trainable_parameters</td><td>5940530</td></tr><tr><td>val_accuracy</td><td>18.59891</td></tr><tr><td>val_loss</td><td>0.66183</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">lstm_ed128_hid256_enc3_dec3_attnTrue_drop0.2</strong> at: <a href='https://wandb.ai/ma23m007-iit-madras/transliteration-model/runs/i6wco6sc' target=\"_blank\">https://wandb.ai/ma23m007-iit-madras/transliteration-model/runs/i6wco6sc</a><br> View project at: <a href='https://wandb.ai/ma23m007-iit-madras/transliteration-model' target=\"_blank\">https://wandb.ai/ma23m007-iit-madras/transliteration-model</a><br>Synced 5 W&B file(s), 1 media file(s), 4 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250520_064436-i6wco6sc/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run i6wco6sc errored:\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_35/1630552953.py\", line 869, in train_sweep\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     plot_attention_grid(model, test_loader, idx_to_src_token=latin_index_to_token, idx_to_tgt_token=devanagari_index_to_token)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_35/1630552953.py\", line 949, in plot_attention_grid\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     wandb.log(\"heatmap.png\")\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/wandb_run.py\", line 387, in wrapper\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return func(self, *args, **kwargs)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/wandb_run.py\", line 483, in wrapper\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return func(self, *args, **kwargs)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/wandb_run.py\", line 435, in wrapper_fn\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return func(self, *args, **kwargs)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/wandb_run.py\", line 425, in wrapper\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return func(self, *args, **kwargs)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/wandb_run.py\", line 1946, in log\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._log(data=data, step=step, commit=commit)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/wandb_run.py\", line 1654, in _log\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     raise ValueError(\"wandb.log must be passed a dictionary\")\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m ValueError: wandb.log must be passed a dictionary\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 1500x1500 with 18 Axes>","image/png":"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\n"},"metadata":{}}],"execution_count":1}]}