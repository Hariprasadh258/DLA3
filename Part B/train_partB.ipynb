{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-05-18T14:51:19.658770Z",
     "iopub.status.busy": "2025-05-18T14:51:19.658064Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mma23m007\u001b[0m (\u001b[33mma23m007-iit-madras\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: 8uep1oif\n",
      "Sweep URL: https://wandb.ai/ma23m007-iit-madras/transliteration-model-tamil-attn/sweeps/8uep1oif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: a6dpe137 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbidirectional: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecoder_layers: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tembed_dim: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tencoder_layers: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dim: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trnn_type: lstm\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tteacher_forcing_ratio: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_attention: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 1e-05\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'transliteration-model' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250518_145141-a6dpe137</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ma23m007-iit-madras/transliteration-model-tamil-attn/runs/a6dpe137' target=\"_blank\">toasty-sweep-1</a></strong> to <a href='https://wandb.ai/ma23m007-iit-madras/transliteration-model-tamil-attn' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/ma23m007-iit-madras/transliteration-model-tamil-attn/sweeps/8uep1oif' target=\"_blank\">https://wandb.ai/ma23m007-iit-madras/transliteration-model-tamil-attn/sweeps/8uep1oif</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ma23m007-iit-madras/transliteration-model-tamil-attn' target=\"_blank\">https://wandb.ai/ma23m007-iit-madras/transliteration-model-tamil-attn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/ma23m007-iit-madras/transliteration-model-tamil-attn/sweeps/8uep1oif' target=\"_blank\">https://wandb.ai/ma23m007-iit-madras/transliteration-model-tamil-attn/sweeps/8uep1oif</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ma23m007-iit-madras/transliteration-model-tamil-attn/runs/a6dpe137' target=\"_blank\">https://wandb.ai/ma23m007-iit-madras/transliteration-model-tamil-attn/runs/a6dpe137</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading training dataset...\n",
      "Dataset from /kaggle/input/dakshina/dakshina_dataset_v1.0/ta/lexicons/ta.translit.sampled.train.tsv: 38716 pairs.\n",
      "Loading validation dataset...\n",
      "Dataset from /kaggle/input/dakshina/dakshina_dataset_v1.0/ta/lexicons/ta.translit.sampled.dev.tsv: 4054 pairs.\n",
      "Loading test dataset...\n",
      "Dataset from /kaggle/input/dakshina/dakshina_dataset_v1.0/ta/lexicons/ta.translit.sampled.test.tsv: 3938 pairs.\n",
      "Model: lstm_ed128_hid256_enc3_dec3_attnTrue_drop0.2\n",
      "Total parameters: 3312690\n",
      "Trainable parameters: 3312690\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/3\n",
      "Train Loss: 2.7614, Accuracy: 0.00%\n",
      "Val Loss: 2.6072, Accuracy: 0.00%\n",
      "\n",
      "Epoch 2/3\n",
      "Train Loss: 2.1822, Accuracy: 0.19%\n",
      "Val Loss: 1.5927, Accuracy: 1.95%\n",
      "New best model saved with val accuracy: 1.95%\n",
      "\n",
      "Epoch 3/3\n",
      "Train Loss: 1.1682, Accuracy: 8.93%\n",
      "Val Loss: 0.9148, Accuracy: 17.54%\n",
      "New best model saved with val accuracy: 17.54%\n",
      "Loaded best model for testing\n",
      "\n",
      "Test Results -> Loss: 0.9276, Accuracy: 17.57%\n",
      "\n",
      "Prediction Examples:\n",
      "------------------------------------------------------------\n",
      "Input:     farm\n",
      "Target:    ஃபார்ம்\n",
      "Predicted: பார்\n",
      "------------------------------------------------------------\n",
      "Input:     face\n",
      "Target:    ஃபேஸ்\n",
      "Predicted: பாச்\n",
      "------------------------------------------------------------\n",
      "Input:     aeathimuka\n",
      "Target:    அஇஅதிமுக\n",
      "Predicted: ஏதிகுக\n",
      "------------------------------------------------------------\n",
      "Input:     aiathimuka\n",
      "Target:    அஇஅதிமுக\n",
      "Predicted: இததுகு\n",
      "------------------------------------------------------------\n",
      "Input:     ayiathimuka\n",
      "Target:    அஇஅதிமுக\n",
      "Predicted: அயைதிதுக\n",
      "------------------------------------------------------------\n",
      "\n",
      "Testing beam search with beam width 3...\n",
      "Beam search accuracy: 0.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2947 (\\N{TAMIL SIGN VISARGA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Matplotlib currently does not support Tamil natively.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2986 (\\N{TAMIL LETTER PA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3006 (\\N{TAMIL VOWEL SIGN AA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2992 (\\N{TAMIL LETTER RA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3021 (\\N{TAMIL SIGN VIRAMA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2990 (\\N{TAMIL LETTER MA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3015 (\\N{TAMIL VOWEL SIGN EE}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Matplotlib currently does not support Tamil natively.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3000 (\\N{TAMIL LETTER SA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2947 (\\N{TAMIL SIGN VISARGA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2986 (\\N{TAMIL LETTER PA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3021 (\\N{TAMIL SIGN VIRAMA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2949 (\\N{TAMIL LETTER A}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Matplotlib currently does not support Tamil natively.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2951 (\\N{TAMIL LETTER I}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2980 (\\N{TAMIL LETTER TA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3007 (\\N{TAMIL VOWEL SIGN I}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3009 (\\N{TAMIL VOWEL SIGN U}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2965 (\\N{TAMIL LETTER KA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2990 (\\N{TAMIL LETTER MA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2949 (\\N{TAMIL LETTER A}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Matplotlib currently does not support Tamil natively.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2951 (\\N{TAMIL LETTER I}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2980 (\\N{TAMIL LETTER TA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3007 (\\N{TAMIL VOWEL SIGN I}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2990 (\\N{TAMIL LETTER MA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3009 (\\N{TAMIL VOWEL SIGN U}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2965 (\\N{TAMIL LETTER KA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2949 (\\N{TAMIL LETTER A}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Matplotlib currently does not support Tamil natively.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2951 (\\N{TAMIL LETTER I}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2980 (\\N{TAMIL LETTER TA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3007 (\\N{TAMIL VOWEL SIGN I}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2990 (\\N{TAMIL LETTER MA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3009 (\\N{TAMIL VOWEL SIGN U}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2965 (\\N{TAMIL LETTER KA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2949 (\\N{TAMIL LETTER A}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Matplotlib currently does not support Tamil natively.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2965 (\\N{TAMIL LETTER KA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2980 (\\N{TAMIL LETTER TA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3021 (\\N{TAMIL SIGN VIRAMA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3007 (\\N{TAMIL VOWEL SIGN I}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3008 (\\N{TAMIL VOWEL SIGN II}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Matplotlib currently does not support Tamil natively.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3016 (\\N{TAMIL VOWEL SIGN AI}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2949 (\\N{TAMIL LETTER A}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2965 (\\N{TAMIL LETTER KA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2980 (\\N{TAMIL LETTER TA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3021 (\\N{TAMIL SIGN VIRAMA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3007 (\\N{TAMIL VOWEL SIGN I}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2992 (\\N{TAMIL LETTER RA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2949 (\\N{TAMIL LETTER A}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Matplotlib currently does not support Tamil natively.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2965 (\\N{TAMIL LETTER KA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2990 (\\N{TAMIL LETTER MA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2980 (\\N{TAMIL LETTER TA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3006 (\\N{TAMIL VOWEL SIGN AA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2986 (\\N{TAMIL LETTER PA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3021 (\\N{TAMIL SIGN VIRAMA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2994 (\\N{TAMIL LETTER LA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Matplotlib currently does not support Tamil natively.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2949 (\\N{TAMIL LETTER A}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2965 (\\N{TAMIL LETTER KA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2990 (\\N{TAMIL LETTER MA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2980 (\\N{TAMIL LETTER TA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3006 (\\N{TAMIL VOWEL SIGN AA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2986 (\\N{TAMIL LETTER PA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3021 (\\N{TAMIL SIGN VIRAMA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3007 (\\N{TAMIL VOWEL SIGN I}) missing from current font.\n",
      "  fig.canvas.draw()\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import wandb\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import defaultdict\n",
    "\n",
    "# Initialize wandb - will use WANDB_API_KEY environment variable if available\n",
    "try:\n",
    "    # Fix wandb login - only call once\n",
    "    wandb.login(key=\"999fe4f321204bd8f10135f3e40de296c23050f9\")\n",
    "except:\n",
    "    print(\"WandB login failed - results will not be logged. Set WANDB_API_KEY in your environment.\")\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hidden_dim, num_layers, rnn_type, dropout=0.5, bidirectional=False):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        self.rnn_type = rnn_type\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.bidirectional = bidirectional\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.directions = 2 if bidirectional else 1\n",
    "\n",
    "        if rnn_type == 'lstm':\n",
    "            self.rnn = nn.LSTM(emb_dim, hidden_dim, num_layers, batch_first=True,\n",
    "                              dropout=dropout if num_layers > 1 else 0,\n",
    "                              bidirectional=bidirectional)\n",
    "        elif rnn_type == 'gru':\n",
    "            self.rnn = nn.GRU(emb_dim, hidden_dim, num_layers, batch_first=True,\n",
    "                             dropout=dropout if num_layers > 1 else 0,\n",
    "                             bidirectional=bidirectional)\n",
    "        else:  # rnn\n",
    "            self.rnn = nn.RNN(emb_dim, hidden_dim, num_layers, batch_first=True,\n",
    "                             dropout=dropout if num_layers > 1 else 0,\n",
    "                             bidirectional=bidirectional)\n",
    "\n",
    "        # Projection layer to reduce bidirectional output to the expected dimension\n",
    "        if bidirectional:\n",
    "            self.projection = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "\n",
    "    def forward(self, src):\n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "\n",
    "        if self.rnn_type == 'lstm':\n",
    "            outputs, (hidden, cell) = self.rnn(embedded)\n",
    "\n",
    "            # Process bidirectional states if needed\n",
    "            if self.bidirectional:\n",
    "                # Process hidden states\n",
    "                hidden = hidden.view(self.num_layers, self.directions, -1, self.hidden_dim)\n",
    "                # Concat forward and backward directions\n",
    "                hidden = torch.cat([hidden[:, 0], hidden[:, 1]], dim=2)\n",
    "                # Project to the correct dimension\n",
    "                hidden = self.projection(hidden)\n",
    "\n",
    "                # Process cell states\n",
    "                cell = cell.view(self.num_layers, self.directions, -1, self.hidden_dim)\n",
    "                cell = torch.cat([cell[:, 0], cell[:, 1]], dim=2)\n",
    "                cell = self.projection(cell)\n",
    "\n",
    "                # If outputs is bidirectional, we need to process it too\n",
    "                batch_size = outputs.size(0)\n",
    "                seq_len = outputs.size(1)\n",
    "                # Reshape and project encoder outputs\n",
    "                outputs = outputs.contiguous().view(batch_size, seq_len, self.hidden_dim * 2)\n",
    "                outputs = self.projection(outputs)\n",
    "\n",
    "                return outputs, (hidden, cell)\n",
    "            return outputs, (hidden, cell)\n",
    "        else:\n",
    "            outputs, hidden = self.rnn(embedded)\n",
    "\n",
    "            # Process bidirectional states if needed\n",
    "            if self.bidirectional:\n",
    "                hidden = hidden.view(self.num_layers, self.directions, -1, self.hidden_dim)\n",
    "                hidden = torch.cat([hidden[:, 0], hidden[:, 1]], dim=2)\n",
    "                hidden = self.projection(hidden)\n",
    "\n",
    "                # Process outputs if bidirectional\n",
    "                batch_size = outputs.size(0)\n",
    "                seq_len = outputs.size(1)\n",
    "                # Reshape and project encoder outputs\n",
    "                outputs = outputs.contiguous().view(batch_size, seq_len, self.hidden_dim * 2)\n",
    "                outputs = self.projection(outputs)\n",
    "\n",
    "            return outputs, hidden\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hidden_dim, num_layers, rnn_type, dropout=0.5, attention=False):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        self.rnn_type = rnn_type\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.attention = attention\n",
    "\n",
    "        # Increase input size if using attention\n",
    "        rnn_input_size = emb_dim + hidden_dim if attention else emb_dim\n",
    "\n",
    "        if rnn_type == 'lstm':\n",
    "            self.rnn = nn.LSTM(rnn_input_size, hidden_dim, num_layers, batch_first=True,\n",
    "                              dropout=dropout if num_layers > 1 else 0)\n",
    "        elif rnn_type == 'gru':\n",
    "            self.rnn = nn.GRU(rnn_input_size, hidden_dim, num_layers, batch_first=True,\n",
    "                             dropout=dropout if num_layers > 1 else 0)\n",
    "        else:  # rnn\n",
    "            self.rnn = nn.RNN(rnn_input_size, hidden_dim, num_layers, batch_first=True,\n",
    "                             dropout=dropout if num_layers > 1 else 0)\n",
    "\n",
    "        # Attention mechanism\n",
    "        if attention:\n",
    "            # Attention layers - simplified approach that works with any encoder output dimensionality\n",
    "            self.attn = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "            self.v = nn.Linear(hidden_dim, 1, bias=False)\n",
    "\n",
    "        self.fc_out = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, input_char, hidden, encoder_outputs=None):\n",
    "        input_char = input_char.unsqueeze(1)  # [batch_size, 1]\n",
    "        embedded = self.dropout(self.embedding(input_char))  # [batch_size, 1, emb_dim]\n",
    "\n",
    "        # Apply attention if enabled and encoder_outputs are provided\n",
    "        if self.attention and encoder_outputs is not None:\n",
    "            # Make sure we're extracting the hidden state correctly based on RNN type\n",
    "            if self.rnn_type == 'lstm':\n",
    "                query = hidden[0][-1].unsqueeze(1)  # [batch_size, 1, hidden_dim]\n",
    "            else:\n",
    "                query = hidden[-1].unsqueeze(1)  # [batch_size, 1, hidden_dim]\n",
    "\n",
    "            # Get dimensions\n",
    "            batch_size = encoder_outputs.size(0)\n",
    "            src_len = encoder_outputs.size(1)\n",
    "\n",
    "            # Create energy by combining query with encoder outputs\n",
    "            query = query.repeat(1, src_len, 1)  # [batch_size, src_len, hidden_dim]\n",
    "\n",
    "            # Concatenate query with encoder outputs\n",
    "            energy_input = torch.cat((query, encoder_outputs), dim=2)  # [batch_size, src_len, 2*hidden_dim]\n",
    "\n",
    "            # Calculate attention scores\n",
    "            energy = torch.tanh(self.attn(energy_input))  # [batch_size, src_len, hidden_dim]\n",
    "            attention = self.v(energy).squeeze(2)  # [batch_size, src_len]\n",
    "\n",
    "            # Apply softmax to get attention weights\n",
    "            attention_weights = torch.softmax(attention, dim=1).unsqueeze(1)  # [batch_size, 1, src_len]\n",
    "\n",
    "            # Create context vector by applying attention weights to encoder outputs\n",
    "            context = torch.bmm(attention_weights, encoder_outputs)  # [batch_size, 1, hidden_dim]\n",
    "\n",
    "            # Combine with embedding\n",
    "            rnn_input = torch.cat((embedded, context), dim=2)  # [batch_size, 1, emb_dim+hidden_dim]\n",
    "        else:\n",
    "            rnn_input = embedded\n",
    "\n",
    "        # Forward pass through RNN\n",
    "        if self.rnn_type == 'lstm':\n",
    "            output, (hidden, cell) = self.rnn(rnn_input, hidden)\n",
    "            hidden_state = (hidden, cell)\n",
    "        else:\n",
    "            output, hidden = self.rnn(rnn_input, hidden)\n",
    "            hidden_state = hidden\n",
    "\n",
    "        # Generate prediction\n",
    "        prediction = self.fc_out(output.squeeze(1))  # [batch_size, output_dim]\n",
    "\n",
    "        # Return attention_weights if available\n",
    "        if self.attention and encoder_outputs is not None:\n",
    "            return prediction, hidden_state, attention_weights.squeeze(1)  # [batch_size, src_len]\n",
    "        else:\n",
    "            return prediction, hidden_state, None\n",
    "\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, rnn_type, device, use_attention=False):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.rnn_type = rnn_type\n",
    "        self.device = device\n",
    "        self.use_attention = use_attention\n",
    "\n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.0):\n",
    "        batch_size = src.size(0)\n",
    "        trg_len = trg.size(1)\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "\n",
    "        outputs = torch.zeros(batch_size, trg_len, trg_vocab_size).to(self.device)\n",
    "\n",
    "        # For collecting attention weights\n",
    "        all_attention_weights = [] if self.use_attention else None\n",
    "\n",
    "        # Encode the source sequence\n",
    "        encoder_outputs, hidden = self._encode(src)\n",
    "\n",
    "        # Use the first token as input to start decoding\n",
    "        input_char = trg[:, 0]  # <sos> token\n",
    "\n",
    "        for t in range(1, trg_len):\n",
    "            # Generate output from decoder\n",
    "            if self.use_attention:\n",
    "                output, hidden, attn_weights  = self.decoder(input_char, hidden, encoder_outputs)\n",
    "                all_attention_weights.append(attn_weights)  # <-- ADD THIS LINE\n",
    "            else:\n",
    "                output, hidden, _ = self.decoder(input_char, hidden)\n",
    "\n",
    "            outputs[:, t] = output\n",
    "\n",
    "            # Teacher forcing: use real target or predicted token\n",
    "            teacher_force = torch.rand(1).item() < teacher_forcing_ratio\n",
    "            top1 = output.argmax(1)\n",
    "            input_char = trg[:, t] if teacher_force else top1\n",
    "\n",
    "        return outputs, all_attention_weights\n",
    "\n",
    "    def _encode(self, src):\n",
    "        # Get encoder outputs and final hidden state\n",
    "        encoder_outputs, hidden = self.encoder(src)\n",
    "\n",
    "        # Adjust hidden state dimensions if encoder and decoder have different layers\n",
    "        encoder_layers = self.encoder.num_layers\n",
    "        decoder_layers = self.decoder.num_layers\n",
    "\n",
    "        if self.rnn_type == 'lstm':\n",
    "            hidden_state, cell_state = hidden\n",
    "\n",
    "            # If encoder has fewer layers than decoder, pad with zeros\n",
    "            if encoder_layers < decoder_layers:\n",
    "                padding = torch.zeros(\n",
    "                    decoder_layers - encoder_layers,\n",
    "                    hidden_state.size(1),\n",
    "                    hidden_state.size(2)\n",
    "                ).to(self.device)\n",
    "                hidden_state = torch.cat([hidden_state, padding], dim=0)\n",
    "                cell_state = torch.cat([cell_state, padding], dim=0)\n",
    "\n",
    "            # If encoder has more layers than decoder, truncate\n",
    "            elif encoder_layers > decoder_layers:\n",
    "                hidden_state = hidden_state[:decoder_layers]\n",
    "                cell_state = cell_state[:decoder_layers]\n",
    "\n",
    "            # Make sure hidden dimensions match decoder's expected dimensions\n",
    "            if hidden_state.size(2) != self.decoder.hidden_dim:\n",
    "                # Project hidden state to the decoder's dimension using a linear projection\n",
    "                batch_size = hidden_state.size(1)\n",
    "                proj_hidden = torch.zeros(\n",
    "                    hidden_state.size(0),\n",
    "                    batch_size,\n",
    "                    self.decoder.hidden_dim\n",
    "                ).to(self.device)\n",
    "\n",
    "                for layer in range(hidden_state.size(0)):\n",
    "                    # Simple linear projection for each layer\n",
    "                    proj_hidden[layer] = hidden_state[layer].clone().view(batch_size, -1)[:, :self.decoder.hidden_dim]\n",
    "\n",
    "                # Apply the same projection to cell state\n",
    "                proj_cell = torch.zeros_like(proj_hidden)\n",
    "                for layer in range(cell_state.size(0)):\n",
    "                    proj_cell[layer] = cell_state[layer].clone().view(batch_size, -1)[:, :self.decoder.hidden_dim]\n",
    "\n",
    "                hidden = (proj_hidden, proj_cell)\n",
    "            else:\n",
    "                hidden = (hidden_state, cell_state)\n",
    "        else:\n",
    "            # For GRU and RNN\n",
    "            # If encoder has fewer layers than decoder, pad with zeros\n",
    "            if encoder_layers < decoder_layers:\n",
    "                padding = torch.zeros(\n",
    "                    decoder_layers - encoder_layers,\n",
    "                    hidden.size(1),\n",
    "                    hidden.size(2)\n",
    "                ).to(self.device)\n",
    "                hidden = torch.cat([hidden, padding], dim=0)\n",
    "            # If encoder has more layers than decoder, truncate\n",
    "            elif encoder_layers > decoder_layers:\n",
    "                hidden = hidden[:decoder_layers]\n",
    "\n",
    "            # Make sure hidden dimensions match decoder's expected dimensions\n",
    "            if hidden.size(2) != self.decoder.hidden_dim:\n",
    "                # Project hidden state to the decoder's dimension\n",
    "                batch_size = hidden.size(1)\n",
    "                proj_hidden = torch.zeros(\n",
    "                    hidden.size(0),\n",
    "                    batch_size,\n",
    "                    self.decoder.hidden_dim\n",
    "                ).to(self.device)\n",
    "\n",
    "                for layer in range(hidden.size(0)):\n",
    "                    # Simple linear projection for each layer\n",
    "                    proj_hidden[layer] = hidden[layer].clone().view(batch_size, -1)[:, :self.decoder.hidden_dim]\n",
    "\n",
    "                hidden = proj_hidden\n",
    "\n",
    "        return encoder_outputs, hidden\n",
    "\n",
    "# Character-level vocabulary builder\n",
    "def build_vocab(tokens):\n",
    "    vocab = {'<pad>': 0, '<sos>': 1, '<eos>': 2, '<unk>': 3}\n",
    "    for token in tokens:\n",
    "        for char in token:\n",
    "            if char not in vocab:\n",
    "                vocab[char] = len(vocab)\n",
    "    return vocab\n",
    "\n",
    "def encode_sequence(seq, vocab):\n",
    "    return [vocab.get(char, vocab['<unk>']) for char in seq]\n",
    "\n",
    "class DakshinaDataset(Dataset):\n",
    "    def __init__(self, data_path, latin_vocab=None, devanagari_vocab=None):\n",
    "        self.latin_words = []\n",
    "        self.devanagari_words = []\n",
    "\n",
    "        # Group all transliterations by Devanagari word\n",
    "        candidates = defaultdict(list)\n",
    "\n",
    "        with open(data_path, encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split('\\t')\n",
    "                if len(parts) != 3:\n",
    "                    continue\n",
    "                native, latin, rel = parts[0], parts[1], int(parts[2])\n",
    "                candidates[native].append((latin, rel))\n",
    "\n",
    "        # Keep only the transliteration(s) with highest score for each native word\n",
    "        for native, translits in candidates.items():\n",
    "            max_rel = max(rel for _, rel in translits)\n",
    "            for latin, rel in translits:\n",
    "                if rel == max_rel:\n",
    "                    self.latin_words.append(latin)\n",
    "                    self.devanagari_words.append(native)\n",
    "\n",
    "        print(f\"Dataset from {data_path}: {len(self.latin_words)} pairs.\")\n",
    "\n",
    "        self.latin_vocab = latin_vocab or build_vocab(self.latin_words)\n",
    "        self.devanagari_vocab = devanagari_vocab or build_vocab(self.devanagari_words)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.latin_words)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src_seq = encode_sequence(self.latin_words[idx], self.latin_vocab)\n",
    "        trg_seq = encode_sequence(self.devanagari_words[idx], self.devanagari_vocab)\n",
    "        # add <sos> and <eos> tokens for target sequences\n",
    "        trg_seq = [self.devanagari_vocab['<sos>']] + trg_seq + [self.devanagari_vocab['<eos>']]\n",
    "        return src_seq, trg_seq\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # batch is a list of tuples (src_seq, trg_seq)\n",
    "    src_seqs, trg_seqs = zip(*batch)\n",
    "\n",
    "    # find max lengths\n",
    "    max_src_len = max(len(seq) for seq in src_seqs)\n",
    "    max_trg_len = max(len(seq) for seq in trg_seqs)\n",
    "\n",
    "    # pad sequences\n",
    "    src_padded = [seq + [0]*(max_src_len - len(seq)) for seq in src_seqs]\n",
    "    trg_padded = [seq + [0]*(max_trg_len - len(seq)) for seq in trg_seqs]\n",
    "\n",
    "    # convert to tensors\n",
    "    src_tensor = torch.tensor(src_padded, dtype=torch.long)\n",
    "    trg_tensor = torch.tensor(trg_padded, dtype=torch.long)\n",
    "\n",
    "    return src_tensor, trg_tensor\n",
    "\n",
    "def compute_word_accuracy(preds, trg, pad_idx, sos_idx=1, eos_idx=2):\n",
    "    \"\"\"\n",
    "    Compute word-level accuracy: a word is correct only if all tokens match (excluding pad, sos, eos).\n",
    "    preds, trg: [batch_size, seq_len]\n",
    "    \"\"\"\n",
    "    batch_size = preds.size(0)\n",
    "    correct = 0\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        # Get sequence for this example (exclude pad, sos, eos tokens)\n",
    "        pred_seq = [idx for idx in preds[i].tolist() if idx not in [pad_idx, sos_idx, eos_idx]]\n",
    "        trg_seq = [idx for idx in trg[i].tolist() if idx not in [pad_idx, sos_idx, eos_idx]]\n",
    "\n",
    "        # Compare full sequences (exact match)\n",
    "        if pred_seq == trg_seq:\n",
    "            correct += 1\n",
    "\n",
    "    return correct, batch_size\n",
    "\n",
    "def beam_search(model, src_seq, src_vocab, tgt_vocab, beam_width=3, max_len=20):\n",
    "    model.eval()\n",
    "    index_to_char = {v: k for k, v in tgt_vocab.items()}\n",
    "    device = model.device\n",
    "\n",
    "    # Prepare input\n",
    "    src_indices = encode_sequence(src_seq, src_vocab)\n",
    "    src_tensor = torch.tensor([src_indices], dtype=torch.long).to(device)\n",
    "\n",
    "    # Get encoder outputs and hidden state\n",
    "    encoder_outputs, hidden = model._encode(src_tensor)\n",
    "\n",
    "    # Start with start-of-sequence token\n",
    "    beams = [([tgt_vocab['<sos>']], 0.0, hidden)]\n",
    "\n",
    "    for _ in range(max_len):\n",
    "        new_beams = []\n",
    "        for seq, score, hidden in beams:\n",
    "            last_token = torch.tensor([seq[-1]], dtype=torch.long).to(device)\n",
    "\n",
    "            # Use attention if model has it\n",
    "            if model.use_attention:\n",
    "                output, new_hidden, _ = model.decoder(last_token, hidden, encoder_outputs)\n",
    "            else:\n",
    "                output, new_hidden = model.decoder(last_token, hidden)\n",
    "\n",
    "            log_probs = torch.log_softmax(output, dim=-1)\n",
    "            topk = torch.topk(log_probs, beam_width)\n",
    "\n",
    "            for prob, idx in zip(topk.values[0], topk.indices[0]):\n",
    "                new_seq = seq + [idx.item()]\n",
    "                new_score = score + prob.item()\n",
    "                new_beams.append((new_seq, new_score, new_hidden))\n",
    "\n",
    "        beams = sorted(new_beams, key=lambda x: x[1], reverse=True)[:beam_width]\n",
    "\n",
    "        # Stop if all beams end with EOS\n",
    "        if all(seq[-1] == tgt_vocab['<eos>'] for seq, _, _ in beams):\n",
    "            break\n",
    "\n",
    "    # Pick the best beam\n",
    "    best_seq = beams[0][0]\n",
    "    # Remove special tokens for output\n",
    "    decoded = [index_to_char[i] for i in best_seq if i not in {tgt_vocab['<sos>'], tgt_vocab['<eos>'], tgt_vocab['<pad>']}]\n",
    "    return ''.join(decoded)\n",
    "\n",
    "def train(model, dataloader, optimizer, criterion, clip=1, teacher_forcing_ratio=0.0):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    total_words = 0\n",
    "    correct_words = 0\n",
    "\n",
    "    pad_idx = 0  # Pad index in vocabulary\n",
    "    sos_idx = 1  # Start of sequence index\n",
    "    eos_idx = 2  # End of sequence index\n",
    "\n",
    "    for src, trg in dataloader:\n",
    "        src, trg = src.to(model.device), trg.to(model.device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Generate sequence\n",
    "        output, _ = model(src, trg, teacher_forcing_ratio=teacher_forcing_ratio)\n",
    "        output_dim = output.shape[-1]\n",
    "\n",
    "        # Ignore first token (<sos>) in loss calculation\n",
    "        output = output[:, 1:].contiguous().view(-1, output_dim)\n",
    "        trg_flat = trg[:, 1:].contiguous().view(-1)\n",
    "\n",
    "        loss = criterion(output, trg_flat)\n",
    "        loss.backward()\n",
    "\n",
    "        # Use gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        # Calculate word accuracy\n",
    "        pred_tokens = output.argmax(1).view(trg[:, 1:].shape)  # [batch_size, trg_len-1]\n",
    "        trg_trimmed = trg[:, 1:]                             # [batch_size, trg_len-1]\n",
    "\n",
    "        correct, total = compute_word_accuracy(pred_tokens, trg_trimmed, pad_idx, sos_idx, eos_idx)\n",
    "        correct_words += correct\n",
    "        total_words += total\n",
    "\n",
    "    avg_loss = epoch_loss / len(dataloader)\n",
    "    word_acc = correct_words / total_words if total_words > 0 else 0\n",
    "\n",
    "    return avg_loss, word_acc * 100\n",
    "\n",
    "def evaluate(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    total_words = 0\n",
    "    correct_words = 0\n",
    "\n",
    "    pad_idx = 0  # Pad index in vocabulary\n",
    "    sos_idx = 1  # Start of sequence index\n",
    "    eos_idx = 2  # End of sequence index\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for src, trg in dataloader:\n",
    "            src, trg = src.to(model.device), trg.to(model.device)\n",
    "\n",
    "            # Generate full sequence with no teacher forcing\n",
    "            output, _  = model(src, trg, teacher_forcing_ratio=0.0)\n",
    "            # Visualize for the first example in batch\n",
    "            # attn = torch.stack([aw[0] for aw in attention_weights]).cpu().numpy()\n",
    "            output_dim = output.shape[-1]\n",
    "\n",
    "            # Ignore first token (<sos>) in loss calculation\n",
    "            output = output[:, 1:].contiguous().view(-1, output_dim)\n",
    "            trg_flat = trg[:, 1:].contiguous().view(-1)\n",
    "\n",
    "            loss = criterion(output, trg_flat)\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            # Calculate word accuracy\n",
    "            pred_tokens = output.argmax(1).view(trg[:, 1:].shape)\n",
    "            trg_trimmed = trg[:, 1:]\n",
    "\n",
    "            correct, total = compute_word_accuracy(pred_tokens, trg_trimmed, pad_idx, sos_idx, eos_idx)\n",
    "            correct_words += correct\n",
    "            total_words += total\n",
    "\n",
    "    avg_loss = epoch_loss / len(dataloader)\n",
    "    word_acc = correct_words / total_words if total_words > 0 else 0\n",
    "\n",
    "    return avg_loss, word_acc * 100\n",
    "\n",
    "def predict_examples(model, dataloader, latin_index_to_token, devanagari_index_to_token, n=5):\n",
    "    \"\"\"Show a few examples of model predictions vs actual targets\"\"\"\n",
    "    model.eval()\n",
    "    pad_idx = 0\n",
    "    sos_idx = 1  # Start of sequence\n",
    "    eos_idx = 2  # End of sequence\n",
    "    count = 0\n",
    "    results = []\n",
    "\n",
    "    print(\"\\nPrediction Examples:\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for src, trg in dataloader:\n",
    "            src, trg = src.to(model.device), trg.to(model.device)\n",
    "            output, _ = model(src, trg, teacher_forcing_ratio=0.0)\n",
    "            # print(len(output))\n",
    "            pred_tokens = output.argmax(-1)  # [batch_size, seq_len]\n",
    "\n",
    "            for i in range(min(src.size(0), n - count)):\n",
    "                # Decode input\n",
    "                input_indices = [idx for idx in src[i].tolist() if idx != pad_idx]\n",
    "                input_tokens = [latin_index_to_token.get(idx, '<unk>') for idx in input_indices]\n",
    "                input_text = \"\".join(input_tokens)\n",
    "\n",
    "                # Decode target\n",
    "                target_indices = [idx for idx in trg[i].tolist() if idx not in [pad_idx, sos_idx, eos_idx]]\n",
    "                target_tokens = [devanagari_index_to_token.get(idx, '<unk>') for idx in target_indices]\n",
    "                target_text = \"\".join(target_tokens)\n",
    "\n",
    "                # Decode prediction\n",
    "                pred_indices = [idx for idx in pred_tokens[i].tolist() if idx not in [pad_idx, sos_idx, eos_idx]]\n",
    "                pred_tokens_text = [devanagari_index_to_token.get(idx, '<unk>') for idx in pred_indices]\n",
    "                pred_text = \"\".join(pred_tokens_text)\n",
    "\n",
    "                result = {\n",
    "                    \"input\": input_text,\n",
    "                    \"target\": target_text,\n",
    "                    \"prediction\": pred_text,\n",
    "                    \"correct\": pred_text == target_text\n",
    "                }\n",
    "                results.append(result)\n",
    "\n",
    "                print(f\"Input:     {input_text}\")\n",
    "                print(f\"Target:    {target_text}\")\n",
    "                print(f\"Predicted: {pred_text}\")\n",
    "                print(\"-\" * 60)\n",
    "\n",
    "                count += 1\n",
    "                if count >= n:\n",
    "                    break\n",
    "            if count >= n:\n",
    "                break\n",
    "\n",
    "    return results\n",
    "    \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Define sweep configuration with improved parameters\n",
    "def get_sweep_config():\n",
    "    sweep_config = {\n",
    "        'method': 'random',\n",
    "        'metric': {\n",
    "            'name': 'val_accuracy',\n",
    "            'goal': 'maximize'\n",
    "        },\n",
    "        # 'parameters': {\n",
    "        #     'embed_dim': {'values': [128, 256, 384]},\n",
    "        #     'hidden_dim': {'values': [256, 384, 512]},\n",
    "        #     'rnn_type': {'values': ['lstm', 'gru','rnn']},  # Removed basic RNN\n",
    "        #     'encoder_layers': {'values': [1, 2,3]},\n",
    "        #     'decoder_layers': {'values': [1,2, 3]},\n",
    "        #     'dropout': {'values': [0.2, 0.3, 0.4]},\n",
    "        #     'learning_rate': {'values': [0.001, 0.0005,0.1,0.001]},\n",
    "        #     'batch_size': {'values': [64, 128]},\n",
    "        #     'epochs': {'values': [10, 15, 20]},\n",
    "        #     'beam_size': {'values': [3, 5]},\n",
    "        #     'use_attention': {'values': [True, False]},\n",
    "        #     'bidirectional': {'values': [True,False]},\n",
    "        #     'teacher_forcing_ratio': {'values': [0.0, 0.3]},\n",
    "        #     'weight_decay': {'values': [1e-5, 1e-6]}\n",
    "        # }\n",
    "        'parameters': {\n",
    "            'embed_dim': {'values': [128]},\n",
    "            'hidden_dim': {'values': [256]},\n",
    "            'rnn_type': {'values': ['lstm']},  # Removed basic RNN\n",
    "            'encoder_layers': {'values': [3]},\n",
    "            'decoder_layers': {'values': [3]},\n",
    "            'dropout': {'values': [0.2]},\n",
    "            'learning_rate': {'values': [0.001]},\n",
    "            'batch_size': {'values': [128]},\n",
    "            'epochs': {'values': [3]},\n",
    "            'beam_size': {'values': [3]},\n",
    "            'use_attention': {'values': [True]},\n",
    "            'bidirectional': {'values': [False]},\n",
    "            'teacher_forcing_ratio': {'values': [0.0, 0.3]},\n",
    "            'weight_decay': {'values': [1e-5, 1e-6]}\n",
    "        }\n",
    "    }\n",
    "    return sweep_config\n",
    "\n",
    "# Main training function for sweep runs\n",
    "def train_sweep():\n",
    "    # Initialize wandb with sweep configuration\n",
    "    run = wandb.init(project=\"transliteration-model\")\n",
    "\n",
    "    # Access hyperparameters from wandb.config\n",
    "    config = run.config\n",
    "\n",
    "    # Setup device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Define data paths (adjust for Kaggle environment)\n",
    "    data_dir = '/kaggle/input/dakshina/dakshina_dataset_v1.0/ta/lexicons/'\n",
    "    train_path = os.path.join(data_dir, 'ta.translit.sampled.train.tsv')\n",
    "    dev_path = os.path.join(data_dir, 'ta.translit.sampled.dev.tsv')\n",
    "    test_path = os.path.join(data_dir, 'ta.translit.sampled.test.tsv')\n",
    "\n",
    "    # Check if files exist\n",
    "    if not os.path.exists(train_path):\n",
    "        raise FileNotFoundError(f\"Could not find training data at {train_path}. Please check the path.\")\n",
    "\n",
    "    # Load datasets\n",
    "    print(\"Loading training dataset...\")\n",
    "    train_dataset = DakshinaDataset(train_path)\n",
    "    latin_vocab = train_dataset.latin_vocab\n",
    "    devanagari_vocab = train_dataset.devanagari_vocab\n",
    "\n",
    "    print(\"Loading validation dataset...\")\n",
    "    val_dataset = DakshinaDataset(\n",
    "        dev_path,\n",
    "        latin_vocab=latin_vocab,\n",
    "        devanagari_vocab=devanagari_vocab\n",
    "    )\n",
    "\n",
    "    print(\"Loading test dataset...\")\n",
    "    test_dataset = DakshinaDataset(\n",
    "        test_path,\n",
    "        latin_vocab=latin_vocab,\n",
    "        devanagari_vocab=devanagari_vocab\n",
    "    )\n",
    "\n",
    "    # Create DataLoaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=config.batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=config.batch_size,\n",
    "        shuffle=False,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=config.batch_size,\n",
    "        shuffle=False,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "\n",
    "    # Get vocabulary information\n",
    "    latin_vocab_size = len(latin_vocab)\n",
    "    devanagari_vocab_size = len(devanagari_vocab)\n",
    "    pad_idx = devanagari_vocab['<pad>']\n",
    "\n",
    "    # Log vocabulary sizes\n",
    "    wandb.log({\"latin_vocab_size\": latin_vocab_size, \"devanagari_vocab_size\": devanagari_vocab_size})\n",
    "\n",
    "    # Generate a model name based on hyperparameters\n",
    "    model_name = f\"{config.rnn_type}_ed{config.embed_dim}_hid{config.hidden_dim}_enc{config.encoder_layers}_dec{config.decoder_layers}_attn{config.use_attention}_drop{config.dropout}\"\n",
    "    wandb.run.name = model_name\n",
    "\n",
    "    # Create model architecture\n",
    "    encoder = Encoder(\n",
    "        input_dim=latin_vocab_size,\n",
    "        emb_dim=config.embed_dim,\n",
    "        hidden_dim=config.hidden_dim,\n",
    "        num_layers=config.encoder_layers,\n",
    "        rnn_type=config.rnn_type,\n",
    "        dropout=config.dropout,\n",
    "        bidirectional=config.bidirectional\n",
    "    )\n",
    "\n",
    "    decoder = Decoder(\n",
    "        output_dim=devanagari_vocab_size,\n",
    "        emb_dim=config.embed_dim,\n",
    "        hidden_dim=config.hidden_dim,\n",
    "        num_layers=config.decoder_layers,\n",
    "        rnn_type=config.rnn_type,\n",
    "        dropout=config.dropout,\n",
    "        attention=config.use_attention\n",
    "    )\n",
    "\n",
    "    model = Seq2Seq(\n",
    "        encoder,\n",
    "        decoder,\n",
    "        rnn_type=config.rnn_type,\n",
    "        device=device,\n",
    "        use_attention=config.use_attention\n",
    "    ).to(device)\n",
    "\n",
    "    # Count and log the number of model parameters\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    wandb.log({\n",
    "        \"total_parameters\": total_params,\n",
    "        \"trainable_parameters\": trainable_params\n",
    "    })\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(f\"Total parameters: {total_params}\")\n",
    "    print(f\"Trainable parameters: {trainable_params}\")\n",
    "\n",
    "    # Setup optimizer and loss function with weight decay for regularization\n",
    "    optimizer = torch.optim.Adam(\n",
    "        model.parameters(),\n",
    "        lr=config.learning_rate,\n",
    "        weight_decay=config.weight_decay\n",
    "    )\n",
    "\n",
    "    # Learning rate scheduler\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer,\n",
    "        mode='max',\n",
    "        factor=0.5,\n",
    "        patience=2,\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "    criterion = torch.nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
    "\n",
    "    # Training loop with early stopping\n",
    "    best_val_loss = float('inf')\n",
    "    best_val_acc = 0\n",
    "    patience = 5  # Increased patience\n",
    "    patience_counter = 0\n",
    "\n",
    "    # Save directory for models\n",
    "    model_dir = '/kaggle/working/models'\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "    # Track metrics for each epoch\n",
    "    for epoch in range(config.epochs):\n",
    "        print(f\"\\nEpoch {epoch+1}/{config.epochs}\")\n",
    "\n",
    "        # Train\n",
    "        train_loss, train_acc = train(model, train_loader, optimizer, criterion, clip=1.0,\n",
    "                                      teacher_forcing_ratio=config.teacher_forcing_ratio)\n",
    "        print(f\"Train Loss: {train_loss:.4f}, Accuracy: {train_acc:.2f}%\")\n",
    "\n",
    "        # Validate\n",
    "        val_loss, val_acc = evaluate(model, val_loader, criterion)\n",
    "        print(f\"Val Loss: {val_loss:.4f}, Accuracy: {val_acc:.2f}%\")\n",
    "\n",
    "        # Log metrics to wandb\n",
    "        wandb.log({\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"train_loss\": train_loss,\n",
    "            \"train_accuracy\": train_acc,\n",
    "            \"val_loss\": val_loss,\n",
    "            \"val_accuracy\": val_acc\n",
    "        })\n",
    "\n",
    "        # Save best model and check for early stopping\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "\n",
    "            # Save the best model\n",
    "            best_model_path = os.path.join(model_dir, f\"{model_name}_best.pt\")\n",
    "            torch.save(model.state_dict(), best_model_path)\n",
    "            print(f\"New best model saved with val accuracy: {val_acc:.2f}%\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping after {epoch+1} epochs\")\n",
    "                break\n",
    "\n",
    "    # Load the best model for testing\n",
    "    best_model_path = os.path.join(model_dir, f\"{model_name}_best.pt\")\n",
    "    try:\n",
    "        model.load_state_dict(torch.load(best_model_path))\n",
    "        print(\"Loaded best model for testing\")\n",
    "    except:\n",
    "        print(\"Using current model for testing (best model not found)\")\n",
    "\n",
    "    # Final evaluation on test set\n",
    "    test_loss, test_acc = evaluate(model, test_loader, criterion)\n",
    "    print(f\"\\nTest Results -> Loss: {test_loss:.4f}, Accuracy: {test_acc:.2f}%\")\n",
    "\n",
    "    # Log final test metrics\n",
    "    wandb.log({\n",
    "        \"test_loss\": test_loss,\n",
    "        \"test_accuracy\": test_acc\n",
    "    })\n",
    "\n",
    "    # Create index-to-token dictionaries for prediction display\n",
    "    latin_index_to_token = {idx: token for token, idx in latin_vocab.items()}\n",
    "    devanagari_index_to_token = {idx: token for token, idx in devanagari_vocab.items()}\n",
    "\n",
    "    # Generate prediction examples for visualization\n",
    "    example_results = predict_examples(\n",
    "        model,\n",
    "        test_loader,\n",
    "        latin_index_to_token,\n",
    "        devanagari_index_to_token,\n",
    "        n=5\n",
    "    )\n",
    "\n",
    "    # Log the examples as a table in wandb\n",
    "    example_table = wandb.Table(\n",
    "        columns=[\"Input\", \"Target\", \"Prediction\", \"Correct\"]\n",
    "    )\n",
    "    for result in example_results:\n",
    "        example_table.add_data(\n",
    "            result[\"input\"],\n",
    "            result[\"target\"],\n",
    "            result[\"prediction\"],\n",
    "            result[\"correct\"]\n",
    "        )\n",
    "    wandb.log({\"prediction_examples\": example_table})\n",
    "\n",
    "    # Test beam search if enabled\n",
    "    if config.beam_size > 1:\n",
    "        print(f\"\\nTesting beam search with beam width {config.beam_size}...\")\n",
    "        beam_correct = 0\n",
    "        beam_total = 0\n",
    "\n",
    "        for src, trg in test_loader:\n",
    "            src = src.to(device)\n",
    "            trg = trg.to(device)\n",
    "            for i in range(min(5, src.size(0))):  # Test beam search on a few examples\n",
    "                # Get input sequence\n",
    "                src_seq = [latin_index_to_token[idx] for idx in src[i].tolist() if idx != pad_idx]\n",
    "                src_text = ''.join(src_seq)\n",
    "\n",
    "                # Get target sequence\n",
    "                trg_indices = [idx for idx in trg[i].tolist() if idx not in [pad_idx, 1]]  # Remove <pad> and <sos>\n",
    "                trg_text = ''.join([devanagari_index_to_token.get(idx, '<unk>') for idx in trg_indices])\n",
    "\n",
    "                # Run beam search\n",
    "                beam_pred = beam_search(\n",
    "                    model,\n",
    "                    src_text,\n",
    "                    latin_vocab,\n",
    "                    devanagari_vocab,\n",
    "                    beam_width=config.beam_size,\n",
    "                    max_len=30\n",
    "                )\n",
    "\n",
    "                beam_correct += 1 if beam_pred == trg_text else 0\n",
    "                beam_total += 1\n",
    "\n",
    "                # print(f\"Input: {src_text}\")\n",
    "                # print(f\"Target: {trg_text}\")\n",
    "                # print(f\"Beam Pred: {beam_pred}\")\n",
    "                # print(\"-\" * 60)\n",
    "\n",
    "        beam_acc = beam_correct / beam_total * 100 if beam_total > 0 else 0\n",
    "        print(f\"Beam search accuracy: {beam_acc:.2f}%\")\n",
    "        wandb.log({\"beam_search_accuracy\": beam_acc})\n",
    "        \n",
    "    # plot_attention_grid(model, test_loader, idx_to_src_token=latin_index_to_token, idx_to_tgt_token=devanagari_index_to_token)\n",
    "\n",
    "    return model, latin_vocab, devanagari_vocab\n",
    "\n",
    "# def plot_attention_grid(model, dataloader, idx_to_src_token=None, idx_to_tgt_token=None, num_samples=9):\n",
    "#     model.eval()\n",
    "    \n",
    "#     samples_plotted = 0\n",
    "    \n",
    "#     fig, axes = plt.subplots(3, 3, figsize=(15, 15))\n",
    "#     axes = axes.flatten()\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for src, trg in dataloader:\n",
    "#             src, trg = src.to(model.device), trg.to(model.device)\n",
    "            \n",
    "#             # Forward pass with no teacher forcing to get attention\n",
    "#             output, attention_weights = model(src, trg, teacher_forcing_ratio=0.0)\n",
    "\n",
    "#             # attention_weights shape: list with length trg_len-1, each [batch_size, src_len]\n",
    "#             # Stack into tensor: [tgt_len-1, batch_size, src_len]\n",
    "#             if not attention_weights:\n",
    "#                 print(\"No attention weights returned by the model.\")\n",
    "#                 return  # or break\n",
    "#             attn_tensor = torch.stack(attention_weights)  # [tgt_len-1, batch_size, src_len]\n",
    "#             print(\"Attention tensor shape:\", attn_tensor.shape)\n",
    "#             print(\"Attention min/max:\", attn_tensor.min().item(), attn_tensor.max().item())\n",
    "\n",
    "#             batch_size = src.size(0)\n",
    "#             tgt_len = trg.size(1)\n",
    "#             src_len = src.size(1)\n",
    "            \n",
    "#             for i in range(batch_size):\n",
    "#                 if samples_plotted >= num_samples:\n",
    "#                     break\n",
    "                \n",
    "#                 attn_for_sample = attn_tensor[:, i, :].cpu().numpy()  # [tgt_len-1, src_len]\n",
    "\n",
    "#                 if idx_to_src_token and idx_to_tgt_token:\n",
    "#                     src_tokens = [idx_to_src_token[idx.item()] for idx in src[i]]\n",
    "#                     tgt_tokens = [idx_to_tgt_token[idx.item()] for idx in trg[i][1:]]  # skip <sos>\n",
    "#                 else:\n",
    "#                     src_tokens = None\n",
    "#                     tgt_tokens = None\n",
    "                \n",
    "#                 # Plot on subplot\n",
    "#                 ax = axes[samples_plotted]\n",
    "#                 # Remove <pad> tokens from source and target\n",
    "#                 src_tokens = [tok for tok in src_tokens if tok != \"<pad>\"]\n",
    "#                 tgt_tokens = [tok for tok in tgt_tokens if tok != \"<pad>\"]\n",
    "#                 sns.heatmap(attn_for_sample, cmap=\"viridis\", xticklabels=src_tokens, yticklabels=tgt_tokens, ax=ax)\n",
    "#                 ax.set_xlabel(\"Source Tokens\")\n",
    "#                 ax.set_ylabel(\"Target Tokens\")\n",
    "#                 ax.set_title(f\"Sample {samples_plotted + 1}\")\n",
    "#                 ax.tick_params(axis='x', rotation=45)\n",
    "#                 ax.tick_params(axis='y', rotation=0)\n",
    "\n",
    "#                 samples_plotted += 1\n",
    "            \n",
    "#             if samples_plotted >= num_samples:\n",
    "#                 break\n",
    "\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib import font_manager\n",
    "\n",
    "def plot_attention_grid(model, dataloader, idx_to_src_token=None, idx_to_tgt_token=None, num_samples=9):\n",
    "    model.eval()\n",
    "    \n",
    "    # Load Tamil font (update the path as per your font location)\n",
    "    tamil_font_path = \"/kaggle/input/notations-tamil/static/NotoSansTamil-Regular.ttf\"  # Put this font file in the working dir or give full path\n",
    "    tamil_font = font_manager.FontProperties(fname=tamil_font_path)\n",
    "\n",
    "    samples_plotted = 0\n",
    "    fig, axes = plt.subplots(3, 3, figsize=(15, 15))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for src, trg in dataloader:\n",
    "            src, trg = src.to(model.device), trg.to(model.device)\n",
    "\n",
    "            # Forward pass with no teacher forcing to get attention\n",
    "            output, attention_weights = model(src, trg, teacher_forcing_ratio=0.0)\n",
    "\n",
    "            if not attention_weights:\n",
    "                print(\"No attention weights returned by the model.\")\n",
    "                return\n",
    "            \n",
    "            # Shape: [tgt_len-1, batch_size, src_len]\n",
    "            attn_tensor = torch.stack(attention_weights)\n",
    "\n",
    "            batch_size = src.size(0)\n",
    "\n",
    "            for i in range(batch_size):\n",
    "                if samples_plotted >= num_samples:\n",
    "                    break\n",
    "\n",
    "                # Tokens\n",
    "                if idx_to_src_token and idx_to_tgt_token:\n",
    "                    src_tokens = [idx_to_src_token[idx.item()] for idx in src[i]]\n",
    "                    tgt_tokens = [idx_to_tgt_token[idx.item()] for idx in trg[i][1:]]  # skip <sos>\n",
    "                else:\n",
    "                    src_tokens = None\n",
    "                    tgt_tokens = None\n",
    "\n",
    "                # Remove <pad> tokens and track lengths\n",
    "                src_tokens_clean = [tok for tok in src_tokens if tok != \"<pad>\"]\n",
    "                tgt_tokens_clean = [tok for tok in tgt_tokens if tok != \"<pad>\"]\n",
    "                src_trim_len = len(src_tokens_clean)\n",
    "                tgt_trim_len = len(tgt_tokens_clean)\n",
    "\n",
    "                # Trim attention to match cleaned tokens\n",
    "                attn_for_sample = attn_tensor[:, i, :].cpu().numpy()  # [tgt_len-1, src_len]\n",
    "                attn_for_sample = attn_for_sample[:tgt_trim_len, :src_trim_len]\n",
    "\n",
    "                # Plot\n",
    "                ax = axes[samples_plotted]\n",
    "                sns.heatmap(attn_for_sample, cmap=\"viridis\",\n",
    "                            xticklabels=src_tokens_clean,\n",
    "                            yticklabels=tgt_tokens_clean,\n",
    "                            ax=ax)\n",
    "                ax.set_xlabel(\"Source Tokens\", fontproperties=tamil_font)\n",
    "                ax.set_ylabel(\"Target Tokens\", fontproperties=tamil_font)\n",
    "                ax.set_title(f\"Sample {samples_plotted + 1}\", fontproperties=tamil_font)\n",
    "                ax.tick_params(axis='x', rotation=45)\n",
    "                ax.tick_params(axis='y', rotation=0)\n",
    "\n",
    "                # Apply Tamil font to tick labels\n",
    "                for label in ax.get_xticklabels() + ax.get_yticklabels():\n",
    "                    label.set_fontproperties(tamil_font)\n",
    "\n",
    "                samples_plotted += 1\n",
    "\n",
    "            if samples_plotted >= num_samples:\n",
    "                break\n",
    "    plt.savefig(\"heatmap.png\", dpi = 300)\n",
    "    wandb.log(\"heatmap.png\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Entry point - runs a wandb sweep\n",
    "def run_wandb_sweep():\n",
    "    sweep_config = get_sweep_config()\n",
    "    sweep_id = wandb.sweep(sweep_config, project=\"transliteration-model-tamil-attn\")\n",
    "    wandb.agent(sweep_id, train_sweep, count=1)\n",
    "\n",
    "\n",
    "# Main execution block for Kaggle\n",
    "if __name__ == \"__main__\":\n",
    "    # Run the wandb sweep\n",
    "    run_wandb_sweep()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7236744,
     "sourceId": 11539512,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7452617,
     "sourceId": 11860234,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
